{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73db0601",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a374da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69d7f8",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    #T.ToPILImage(),\n",
    "    T.Resize((48, 48)),\n",
    "    #T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec43c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RandomPatchMask:\n",
    "    \"\"\"This class creates a random patch mask for an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_ratio=0.15, patch_size=16):\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # img is a Tensor [C, H, W]\n",
    "        C, H, W = img.shape\n",
    "        # number of patches horizontally and vertically\n",
    "        num_patches_h = H // self.patch_size\n",
    "        num_patches_w = W // self.patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        # how many to mask\n",
    "        num_masked = int(total_patches * self.mask_ratio)\n",
    "        \n",
    "        # choose random patches\n",
    "        patch_indices = list(range(total_patches))\n",
    "        random.shuffle(patch_indices)\n",
    "        mask_indices = patch_indices[:num_masked]\n",
    "        \n",
    "        # create a copy to mask\n",
    "        masked_img = img.clone()\n",
    "        \n",
    "        for idx in mask_indices:\n",
    "            row = idx // num_patches_w\n",
    "            col = idx % num_patches_w\n",
    "            y_start = row * self.patch_size\n",
    "            x_start = col * self.patch_size\n",
    "            # set patch to 0\n",
    "            masked_img[:, y_start:y_start+self.patch_size, x_start:x_start+self.patch_size] = 0\n",
    "        \n",
    "        return masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dbf2d",
   "metadata": {},
   "source": [
    "### Unlabeled faces: MultiTaskDataset; FER2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4525fd8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_random_permutations(num_patches=9, num_permutations=30):\n",
    "    permutations = set()\n",
    "    while len(permutations) < num_permutations:\n",
    "        perm = tuple(random.sample(range(num_patches), num_patches))\n",
    "        permutations.add(perm)\n",
    "    return [list(p) for p in permutations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc270d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A single dataset that returns a dictionary with keys:\n",
    "       {\n",
    "         'denoise':  (denoise_input, denoise_target),\n",
    "         'rotation': (rotation_input, rotation_label),\n",
    "         'jigsaw':   (jigsaw_input,   jigsaw_label),\n",
    "         'mask':     (mask_input,     mask_target)\n",
    "       }\n",
    "\n",
    "    Each item in the batch corresponds to one original image, from which\n",
    "    we generate the different task inputs/targets on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, base_transforms, image_size=224, \n",
    "                 jigsaw_permutations=None,  # e.g. a precomputed list of permutations\n",
    "                 mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Basic transforms (resize + to tensor + normalization)\n",
    "        # self.base_transform = T.Compose([\n",
    "        #     T.Resize((image_size, image_size)),\n",
    "        #     T.ToTensor(),\n",
    "        #     T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                 std=[0.229, 0.224, 0.225])\n",
    "        # ])\n",
    "        self.base_transform = base_transforms\n",
    "        \n",
    "        # If you have a fixed set of jigsaw permutations, store them here\n",
    "        # For example: jigsaw_permutations = [[0,1,2,3,4,5,6,7,8], [3,0,1,4,2,5,7,8,6], ...]\n",
    "        self.jigsaw_permutations = jigsaw_permutations\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load the image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')  # or 'L' for grayscale\n",
    "        img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        #print('Current image: ', img_path, end='\\n')\n",
    "        \n",
    "        # Convert to tensor but *don't* normalize yet, because for denoising\n",
    "        # we might want the original pixel scale in [0,1].\n",
    "        # We'll do basic conversion:\n",
    "        img_tensor = T.ToTensor()(img)  # shape: [3, H, W], range ~[0,1]\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (a) Denoise: Add noise -> (noisy_input, clean_target)\n",
    "        # ---------------------------------------\n",
    "        noisy_input = self.add_gaussian_noise(img_tensor)\n",
    "        # The target is the *clean* version (we can still do normal transforms after)\n",
    "        denoise_input = self.base_transform(T.functional.to_pil_image(noisy_input))\n",
    "        denoise_target = self.base_transform(img)  \n",
    "        # (B, 3, H, W) after transform\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (b) Rotation: rotate the image randomly among {0, 90, 180, 270}\n",
    "        # ---------------------------------------\n",
    "        angles = [0, 90, 180, 270]\n",
    "        angle = random.choice(angles)\n",
    "        rotation_label = angles.index(angle)  # e.g. 0->0°, 1->90°, ...\n",
    "        rotated_img = img.rotate(angle)\n",
    "        rotation_input = self.base_transform(rotated_img)\n",
    "        # rotation_label is just an integer 0..3\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (c) Jigsaw puzzle: shuffle patches, produce a class label\n",
    "        # ---------------------------------------\n",
    "        # We'll do a simplistic approach: pick a random permutation from jigsaw_permutations\n",
    "        # and rearrange the patches accordingly. The \"class label\" is the index of that permutation.\n",
    "        # In a real scenario, you might just feed the shuffled patches as input\n",
    "        # and the label is the permutation index.\n",
    "        if self.jigsaw_permutations is not None:\n",
    "            perm_index = random.randint(0, len(self.jigsaw_permutations) - 1)\n",
    "            perm = self.jigsaw_permutations[perm_index]  # e.g. [3,0,1,2,4,5,6,7,8]\n",
    "            jigsaw_img = self.create_jigsaw(img, perm)\n",
    "            jigsaw_input = self.base_transform(jigsaw_img)\n",
    "            jigsaw_label = torch.tensor(perm_index, dtype=torch.long)\n",
    "        else:\n",
    "            # If you haven't defined permutations, fallback\n",
    "            jigsaw_input = self.base_transform(img)\n",
    "            jigsaw_label = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (d) Masked Patch: randomly mask a fraction of patches\n",
    "        # ---------------------------------------\n",
    "        # We'll do a naive approach: zero out a random fraction of pixels. \n",
    "        # The \"target\" is the original unmasked image. \n",
    "        mask_input, mask_target = self.create_masked_input(img_tensor, self.mask_ratio)\n",
    "        mask_input = self.base_transform(T.functional.to_pil_image(mask_input))\n",
    "        mask_target = self.base_transform(T.functional.to_pil_image(mask_target))\n",
    "\n",
    "        # Return a dictionary of tasks\n",
    "        return {\n",
    "            'denoise':  (denoise_input,  denoise_target),\n",
    "            'rotation': (rotation_input, torch.tensor(rotation_label, dtype=torch.long)),\n",
    "            'jigsaw':   (jigsaw_input,   jigsaw_label),\n",
    "            'mask':     (mask_input,     mask_target),\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Helper functions\n",
    "    # -----------------------------\n",
    "    def add_gaussian_noise(self, img_tensor, std=0.1):\n",
    "        \"\"\"\n",
    "        Add random Gaussian noise to a [C, H, W] tensor, range ~[0,1].\n",
    "        std controls the noise level.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(img_tensor) * std\n",
    "        noisy_img = img_tensor + noise\n",
    "        return torch.clamp(noisy_img, 0.0, 1.0)\n",
    "\n",
    "    def create_jigsaw(self, img_pil, permutation, grid_size=3):\n",
    "        \"\"\"\n",
    "        Slices the image into grid_size x grid_size patches, rearranges them\n",
    "        according to 'permutation' (list of length 9 if grid_size=3),\n",
    "        then reassembles them into a single PIL image.\n",
    "        \"\"\"\n",
    "        w, h = img_pil.size\n",
    "        patch_w = w // grid_size\n",
    "        patch_h = h // grid_size\n",
    "        patches = []\n",
    "        \n",
    "        # Cut into patches\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                left = col * patch_w\n",
    "                upper = row * patch_h\n",
    "                box = (left, upper, left+patch_w, upper+patch_h)\n",
    "                patch = img_pil.crop(box)\n",
    "                patches.append(patch)\n",
    "        \n",
    "        # Rearrange using permutation\n",
    "        shuffled_patches = [patches[i] for i in permutation]\n",
    "        \n",
    "        # Reassemble\n",
    "        new_img = Image.new('RGB', (w, h))\n",
    "        idx = 0\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                new_img.paste(shuffled_patches[idx], (col*patch_w, row*patch_h))\n",
    "                idx += 1\n",
    "        \n",
    "        return new_img\n",
    "\n",
    "    def create_masked_input(self, img_tensor, mask_ratio=0.75):\n",
    "        \"\"\"\n",
    "        Masks a random fraction (mask_ratio) of the image pixels by setting them to 0.\n",
    "        Returns (masked_img, original_img) as Tensors in [0,1].\n",
    "        \"\"\"\n",
    "        c, h, w = img_tensor.shape\n",
    "        num_pixels = h * w\n",
    "        num_mask = int(num_pixels * mask_ratio)\n",
    "\n",
    "        # Flatten the image, choose which pixels to mask\n",
    "        flat_img = img_tensor.view(c, -1).clone()  # shape [3, H*W]\n",
    "        mask_indices = random.sample(range(num_pixels), num_mask)\n",
    "        # zero out those pixels in all channels\n",
    "        for mi in mask_indices:\n",
    "            flat_img[:, mi] = 0.0\n",
    "        \n",
    "        masked_img = flat_img.view(c, h, w)\n",
    "        # The target is the original unmasked image\n",
    "        target_img = img_tensor.clone()\n",
    "        return masked_img, target_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc6a75",
   "metadata": {},
   "source": [
    "### Unlabeled faces DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df5c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fer_2013_dir_train = Path(os.getcwd(), 'datasets', 'fer2013', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jigsaw_permutations = generate_random_permutations(num_patches=9, num_permutations=30)\n",
    "# fer2013_train_files = [p for p in Path(fer_2013_dir_train).rglob('*') if p.is_file()]\n",
    "\n",
    "# dataset = MultiTaskDataset(fer2013_train_files, train_transforms, image_size=48,\n",
    "#                            jigsaw_permutations=jigsaw_permutations,\n",
    "#                            mask_ratio=0.75)\n",
    "\n",
    "# ssl_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714510ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledFacesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        # load, detect face, align, etc.\n",
    "        face_img = cv2.imread(img_path).astype('float32')\n",
    "        \n",
    "        if face_img is None:\n",
    "            # handle missing face or skip\n",
    "            # for simplicity, just return a dummy or raise an error\n",
    "            raise RuntimeError(f\"No face found in {img_path}\")\n",
    "        \n",
    "        if self.transform:\n",
    "            face_img = self.transform(face_img)\n",
    "        \n",
    "        return face_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9059a",
   "metadata": {},
   "source": [
    "### Labeled faces: CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledFERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_paths, labels, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        face_img = cv2.imread(img_path).astype('float32')\n",
    "        if face_img is None:\n",
    "            raise RuntimeError(f\"No face found in {img_path}\")\n",
    "        \n",
    "        if self.transform:\n",
    "            face_img = self.transform(face_img)\n",
    "        \n",
    "        return face_img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3085cb",
   "metadata": {},
   "source": [
    "### FewShot Image Dataset: Manga faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01528619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotFERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, class_folders, transform=None, n_way=5, k_shot=1, k_query=5):\n",
    "        # class_folders: e.g. {class_name: [list_of_image_paths]}\n",
    "        self.class_folders = class_folders\n",
    "        self.transform = transform\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "    \n",
    "    def __len__(self):\n",
    "        # The \"length\" might be the number of episodes you want\n",
    "        return 1000  # or some large number for meta-training\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Sample n_way classes\n",
    "        sampled_classes = random.sample(list(self.class_folders.keys()), self.n_way)\n",
    "        \n",
    "        support_imgs = []\n",
    "        support_labels = []\n",
    "        query_imgs = []\n",
    "        query_labels = []\n",
    "        \n",
    "        label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "        for cls_name in sampled_classes:\n",
    "            paths = self.class_folders[cls_name]\n",
    "            selected_paths = random.sample(paths, self.k_shot + self.k_query)\n",
    "            s_paths = selected_paths[:self.k_shot]\n",
    "            q_paths = selected_paths[self.k_shot:]\n",
    "            \n",
    "            for sp in s_paths:\n",
    "                img = cv2.imread(sp).astype('float32')\n",
    "                if img is not None and self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_imgs.append(img)\n",
    "                support_labels.append(label_map[cls_name])\n",
    "            \n",
    "            for qp in q_paths:\n",
    "                img = cv2.imread(qp).astype('float32')\n",
    "                if img is not None and self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_imgs.append(img)\n",
    "                query_labels.append(label_map[cls_name])\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels)  # [n_way*k_shot]\n",
    "        query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels) # [n_way*k_query]\n",
    "        \n",
    "        return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c9d60",
   "metadata": {},
   "source": [
    "# Self-supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5bf2d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_SelfSupervised(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 embedding_dim=768,\n",
    "                 num_rotation_classes=4,\n",
    "                 num_jigsaw_classes=30):\n",
    "        \"\"\"\n",
    "        image_size: input image resolution (assumed square)\n",
    "        patch_size: size of each non-overlapping patch\n",
    "        in_chans: number of input channels (e.g., 3 for RGB)\n",
    "        embedding_dim: dimension of patch embeddings from ViT\n",
    "        num_rotation_classes: number of rotation categories (0°, 90°, 180°, 270°)\n",
    "        num_jigsaw_classes: number of predefined jigsaw permutation orders\n",
    "        \"\"\"\n",
    "        super(ViT_SelfSupervised, self).__init__()\n",
    "        \n",
    "        # Calculate the number of patches (ViT divides the image into non-overlapping patches)\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # --- ViT Encoder ---\n",
    "        # Instantiate the ViT model from TorchVision.\n",
    "        # We set weights=False to train from scratch.\n",
    "        self.encoder = vit_b_16(weights=None, image_size=image_size, num_classes=7)\n",
    "        # Remove its default classification head by replacing with Identity.\n",
    "        # IMPORTANT: In this example we assume that the modified encoder returns the CLS token and the patch tokens.\n",
    "        self.encoder.heads = Identity()\n",
    "\n",
    "        # --- Pretext Task Heads ---\n",
    "\n",
    "        # (a) Denoising & Reconstruction head.\n",
    "        # For each patch token, we want to predict the original patch pixels.\n",
    "        # Here we use a simple linear projection: output dim = patch_size x patch_size x in_chans.\n",
    "        self.denoise_decoder = nn.Linear(embedding_dim, patch_size * patch_size * in_chans)\n",
    "\n",
    "        # (b) Rotation Prediction head.\n",
    "        # Uses the CLS token from the encoder.\n",
    "        self.rotation_head = nn.Linear(embedding_dim, num_rotation_classes)\n",
    "\n",
    "        # (c) Jigsaw Puzzle head.\n",
    "        # For the jigsaw task, we assume that the image (with patches shuffled) is processed by the encoder.\n",
    "        # We then flatten the patch tokens (excluding the CLS token) and feed them to an MLP that predicts one of\n",
    "        # num_jigsaw_classes possible permutation orders.\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * self.num_patches, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_jigsaw_classes)\n",
    "        )\n",
    "\n",
    "        # (d) Masked Patch Prediction head.\n",
    "        # Similar to the denoising decoder, but applied only to the masked patches.\n",
    "        self.mask_decoder = nn.Linear(embedding_dim, patch_size * patch_size * in_chans)\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the ViT encoder.\n",
    "        We assume that the encoder returns a tensor of shape:\n",
    "           (B, num_tokens, embedding_dim)\n",
    "        where token 0 is the CLS token and tokens 1: are the patch tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: [B, C, H, W]\n",
    "        # Use conv_proj to get patch embeddings.\n",
    "        # This produces an output of shape [B, embed_dim, H_p, W_p],\n",
    "        # where H_p * W_p equals the total number of patches.\n",
    "        x = self.encoder.conv_proj(x)  # shape: [B, embedding_dim, H_p, W_p]\n",
    "\n",
    "        B, C, H_p, W_p = x.shape\n",
    "        # Flatten the spatial dimensions and transpose to get shape [B, num_patches, embedding_dim]\n",
    "        x = x.flatten(2).transpose(1, 2)  # shape: [B, H_p*W_p, embedding_dim]\n",
    "\n",
    "        # Expand the CLS token to batch size.\n",
    "        #cls_tokens = self.encoder.cls_token.expand(B, -1, -1)  # shape: [B, 1, embedding_dim]\n",
    "        cls_tokens = self.encoder.class_token.expand(B, -1, -1)\n",
    "\n",
    "\n",
    "        # Concatenate the CLS token with the patch tokens.\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # shape: [B, 1 + num_patches, embedding_dim]\n",
    "        \n",
    "        ### option 1\n",
    "        # Retrieve positional embeddings from state_dict\n",
    "        # pos_embed = self.encoder.state_dict()['pos_embed']  # shape: [1, 1+num_patches, embedding_dim]\n",
    "        \n",
    "        # #pos_embed = self.encoder.pos_embed\n",
    "        # x = x + pos_embed.to(x.device)\n",
    "        \n",
    "        # x = self.encoder.dropout(x)\n",
    "\n",
    "        # # Pass through transformer encoder blocks.\n",
    "        # for blk in self.encoder.encoder.blocks:\n",
    "        #     x = blk(x)\n",
    "        # # Apply final normalization.\n",
    "        # x = self.encoder.encoder.norm(x)\n",
    "        \n",
    "        \n",
    "        ### option 2\n",
    "        # Check where the correct dropout layer is located\n",
    "        if hasattr(self.encoder, 'dropout') and callable(self.encoder.dropout):\n",
    "            x = self.encoder.dropout(x)  # Apply dropout if it exists\n",
    "        elif hasattr(self.encoder.encoder, 'dropout') and callable(self.encoder.encoder.dropout):\n",
    "            x = self.encoder.encoder.dropout(x)  # Apply encoder-level dropout\n",
    "            \n",
    "        for blk in self.encoder.encoder.layers:\n",
    "            x = blk(x)\n",
    "        x = self.encoder.encoder.ln(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    # def forward_denoise(self, x):\n",
    "    #     \"\"\"\n",
    "    #     Image denoising and reconstruction task.\n",
    "    #     x: input noisy images.\n",
    "    #     Returns: reconstructed patches for all patch tokens.\n",
    "    #     \"\"\"\n",
    "    #     tokens = self.forward_encoder(x)  # shape: (B, 1 + num_patches, D)\n",
    "    #     # Exclude the CLS token (first token)\n",
    "    #     patch_tokens = tokens[:, 1:, :]  # shape: (B, num_patches, D)\n",
    "    #     # Reconstruct each patch.\n",
    "    #     # The output will be of shape (B, num_patches, patch_size*patch_size*in_chans)\n",
    "    #     recon_patches = self.denoise_decoder(patch_tokens)\n",
    "    #     return recon_patches\n",
    "    def forward_denoise(self, x):\n",
    "        \"\"\"Denoising task with proper reshaping\"\"\"\n",
    "        B = x.shape[0]\n",
    "        tokens = self.forward_encoder(x)\n",
    "        patch_tokens = tokens[:, 1:, :]  # [B, num_patches, D]\n",
    "        \n",
    "        # Reconstruct patches\n",
    "        recon_patches = self.denoise_decoder(patch_tokens)  # [B, 9, 768]\n",
    "        \n",
    "        # Reshape to original image dimensions [B, 3, 48, 48]\n",
    "        recon_image = self.patches_to_image(recon_patches, B)\n",
    "        return recon_image\n",
    "\n",
    "    def forward_masked(self, x):\n",
    "        \"\"\"Masked patch prediction with proper reshaping\"\"\"\n",
    "        B = x.shape[0]\n",
    "        tokens = self.forward_encoder(x)\n",
    "        patch_tokens = tokens[:, 1:, :]  # [B, 9, D]\n",
    "        \n",
    "        # Reconstruct patches\n",
    "        mask_predictions = self.mask_decoder(patch_tokens)  # [B, 9, 768]\n",
    "        \n",
    "        # Reshape to original image dimensions [B, 3, 48, 48]\n",
    "        recon_image = self.patches_to_image(mask_predictions, B)\n",
    "        return recon_image\n",
    "\n",
    "    def patches_to_image(self, patches, batch_size):\n",
    "        \"\"\"Convert patch sequence to image tensor\"\"\"\n",
    "        # patches shape: [B, num_patches, patch_size^2 * 3]\n",
    "        patch_size = self.patch_size\n",
    "        num_patches = self.num_patches\n",
    "        channels = self.in_chans\n",
    "        \n",
    "        # Reshape to [B, num_patches, C, patch_size, patch_size]\n",
    "        patches = patches.view(\n",
    "            batch_size, \n",
    "            num_patches, \n",
    "            channels, \n",
    "            patch_size, \n",
    "            patch_size\n",
    "        )\n",
    "        \n",
    "        # Reshape to image grid\n",
    "        grid_size = int(num_patches ** 0.5)  # 3 for 9 patches\n",
    "        image = patches.permute(0, 2, 1, 3, 4)  # [B, C, num_patches, p, p]\n",
    "        image = image.contiguous().view(\n",
    "            batch_size, \n",
    "            channels, \n",
    "            grid_size * patch_size, \n",
    "            grid_size * patch_size\n",
    "        )\n",
    "        return image\n",
    "\n",
    "    def forward_rotation(self, x):\n",
    "        \"\"\"\n",
    "        Image rotation prediction task.\n",
    "        x: input rotated images.\n",
    "        Returns: rotation logits predicted from the CLS token.\n",
    "        \"\"\"\n",
    "        tokens = self.forward_encoder(x)  # shape: (B, 1 + num_patches, D)\n",
    "        cls_token = tokens[:, 0, :]         # shape: (B, D)\n",
    "        rotation_logits = self.rotation_head(cls_token)\n",
    "        return rotation_logits\n",
    "\n",
    "    def forward_jigsaw(self, x):\n",
    "        \"\"\"\n",
    "        Jigsaw puzzle task.\n",
    "        x: input images with shuffled patches.\n",
    "        Returns: logits for predicting the permutation order (classification over num_jigsaw_classes).\n",
    "        \"\"\"\n",
    "        tokens = self.forward_encoder(x)      # shape: (B, 1 + num_patches, D)\n",
    "        patch_tokens = tokens[:, 1:, :]         # remove CLS token, shape: (B, num_patches, D)\n",
    "        # Flatten patch tokens for each image: (B, num_patches * D)\n",
    "        flat_tokens = patch_tokens.reshape(x.size(0), -1)\n",
    "        jigsaw_logits = self.jigsaw_head(flat_tokens)\n",
    "        return jigsaw_logits\n",
    "\n",
    "    # def forward_masked(self, x):\n",
    "    #     \"\"\"\n",
    "    #     Masked patch prediction task.\n",
    "    #     x: input images with some patches masked out.\n",
    "    #        (The masking operation should be performed in the data pre-processing or transform.)\n",
    "    #     Returns: reconstructed predictions for the (unobserved) masked patches.\n",
    "    #     For simplicity, here we process the entire set of patch tokens, and later you would compare\n",
    "    #     the output for masked locations with the ground truth.\n",
    "    #     \"\"\"\n",
    "    #     tokens = self.forward_encoder(x)      # shape: (B, 1 + num_patches, D)\n",
    "    #     patch_tokens = tokens[:, 1:, :]         # shape: (B, num_patches, D)\n",
    "    #     # Predict patch pixel values for each token.\n",
    "    #     mask_predictions = self.mask_decoder(patch_tokens)\n",
    "    #     return mask_predictions\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        \"\"\"\n",
    "        A unified forward method that selects the appropriate pretext task.\n",
    "        task: a string specifying the task type: 'denoise', 'rotation', 'jigsaw', or 'mask'.\n",
    "        \"\"\"\n",
    "        if task == 'denoise':\n",
    "            return self.forward_denoise(x)\n",
    "        elif task == 'rotation':\n",
    "            return self.forward_rotation(x)\n",
    "        elif task == 'jigsaw':\n",
    "            return self.forward_jigsaw(x)\n",
    "        elif task == 'mask':\n",
    "            return self.forward_masked(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task specified. Choose from 'denoise', 'rotation', 'jigsaw', or 'mask'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d64fbc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the log-variances (or deltas) as learnable parameters.\n",
    "        # We'll store them in log-space for numerical stability, e.g. log(delta^2).\n",
    "        self.log_var_den = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_rot = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_puz = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_msk = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, L_den, L_rot, L_puz, L_msk):\n",
    "        # Convert log_var_* to delta = exp(log_var / 2)\n",
    "        # But the formula in the paper effectively uses 1/(2*delta^2), so we can do:\n",
    "        # Weighted losses:\n",
    "        w_den = 1.0 / (2.0 * torch.exp(self.log_var_den))\n",
    "        w_rot = 1.0 / (2.0 * torch.exp(self.log_var_rot))\n",
    "        w_puz = 1.0 / (2.0 * torch.exp(self.log_var_puz))\n",
    "        w_msk = 1.0 / (2.0 * torch.exp(self.log_var_msk))\n",
    "\n",
    "        # Combined loss\n",
    "        loss = (w_den * L_den \n",
    "                + w_rot * L_rot\n",
    "                + w_puz * L_puz\n",
    "                + w_msk * L_msk\n",
    "                + (self.log_var_den + self.log_var_rot \n",
    "                   + self.log_var_puz + self.log_var_msk) * 0.5)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a54d2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "multi_task_loss_fn = MultiTaskLossWrapper().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b1ec2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_ssl_model(model, dataloader, num_epochs=10, device='cuda', learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This is a simplified training loop that assumes:\n",
    "      - The dataloader returns a dictionary containing inputs for each task.\n",
    "        For example, each batch might be a dict with keys: 'denoise', 'rotation', 'jigsaw', 'mask'\n",
    "      - Each key maps to a tuple: (input_tensor, target) where target is the ground truth for that task.\n",
    "    In practice, you may need separate dataloaders or combine losses with appropriate weighting.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "    \n",
    "    # For simplicity, we use fixed weights for each task’s loss.\n",
    "    weight_denoise = 1.0\n",
    "    weight_rotation = 1.0\n",
    "    weight_jigsaw = 1.0\n",
    "    weight_mask = 1.0\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 3  # Stop training if no improvement after this many epochs\n",
    "    \n",
    "    metrics_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(epoch + 1) + \"/\" + str(num_epochs))\n",
    "        #print('DATALOADER LEN \\n', len(dataloader))\n",
    "        \n",
    "        for t, batch in enumerate(tk):\n",
    "            optimizer.zero_grad()\n",
    "            #loss = 0.0\n",
    "            L_den = 0.0\n",
    "            L_rot = 0.0\n",
    "            L_jig = 0.0\n",
    "            L_mask = 0.0\n",
    "            #print('BATCH IS \\n', batch)\n",
    "            #break\n",
    "\n",
    "            # --- Denoising & Reconstruction Task ---\n",
    "            if 'denoise' in batch:\n",
    "                inputs_denoise, targets_denoise = batch['denoise']  # targets: original patches or full image patches\n",
    "                inputs_denoise = inputs_denoise.to(device)\n",
    "                targets_denoise = targets_denoise.to(device)\n",
    "                recon_patches = model(inputs_denoise, task='denoise')\n",
    "                # Reshape targets to match the output if necessary.\n",
    "                loss_denoise = F.mse_loss(recon_patches, targets_denoise)\n",
    "                L_den += weight_denoise * loss_denoise\n",
    "\n",
    "            # --- Rotation Prediction Task ---\n",
    "            if 'rotation' in batch:\n",
    "                inputs_rotation, targets_rotation = batch['rotation']\n",
    "                inputs_rotation = inputs_rotation.to(device)\n",
    "                targets_rotation = targets_rotation.to(device)\n",
    "                rotation_logits = model(inputs_rotation, task='rotation')\n",
    "                loss_rotation = F.cross_entropy(rotation_logits, targets_rotation)\n",
    "                L_rot += weight_rotation * loss_rotation\n",
    "\n",
    "            # --- Jigsaw Puzzle Task ---\n",
    "            if 'jigsaw' in batch:\n",
    "                inputs_jigsaw, targets_jigsaw = batch['jigsaw']  # targets: permutation labels (integer class index)\n",
    "                inputs_jigsaw = inputs_jigsaw.to(device)\n",
    "                targets_jigsaw = targets_jigsaw.to(device)\n",
    "                jigsaw_logits = model(inputs_jigsaw, task='jigsaw')\n",
    "                loss_jigsaw = F.cross_entropy(jigsaw_logits, targets_jigsaw)\n",
    "                L_jig += weight_jigsaw * loss_jigsaw\n",
    "\n",
    "            # --- Masked Patch Prediction Task ---\n",
    "            if 'mask' in batch:\n",
    "                inputs_mask, targets_mask = batch['mask']\n",
    "                inputs_mask = inputs_mask.to(device)\n",
    "                targets_mask = targets_mask.to(device)\n",
    "                mask_predictions = model(inputs_mask, task='mask')\n",
    "                loss_mask = F.mse_loss(mask_predictions, targets_mask)\n",
    "                L_mask += weight_mask * loss_mask\n",
    "                \n",
    "            # Combine them with learned weights:\n",
    "            L_ssl = multi_task_loss_fn(L_den, L_rot, L_jig, L_mask)\n",
    "\n",
    "            L_ssl.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += L_ssl.item()\n",
    "            tk.set_postfix({'loss': '%6f' % float(L_ssl / (t + 1))})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        metrics_loss.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            # Save the pretrained encoder weights (and optionally heads) for later fine-tuning.\n",
    "            torch.save(model.state_dict(), \"vit_ssl_pretrained.pth\")\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "\n",
    "    print(\"\")\n",
    "    return metrics_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879e86c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Example usage:\n",
    "# Assume we have a combined dataloader that yields a dict with keys: 'denoise', 'rotation', 'jigsaw', 'mask'.\n",
    "# Each entry is a tuple: (input_tensor, target_tensor)\n",
    "# In practice, you need to implement or combine datasets that perform the corresponding data augmentation.\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model instance \n",
    "    model_ssl = ViT_SelfSupervised(image_size=48, patch_size=16, in_chans=3,\n",
    "                                   embedding_dim=768, num_rotation_classes=4,\n",
    "                                   num_jigsaw_classes=30)\n",
    "    #print(dir(model_ssl))\n",
    "    # Here, \"ssl_dataloader\" should be defined by you.\n",
    "    # For demonstration purposes, assume it's provided.\n",
    "    # ssl_dataloader = ...\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # train_ssl_model(model_ssl, ssl_dataloader, num_epochs=10, device=device)\n",
    "    \n",
    "    jigsaw_permutations = generate_random_permutations(num_patches=9, num_permutations=30)\n",
    "    fer2013_train_files = [p for p in Path(fer_2013_dir_train).rglob('*') if p.is_file()]\n",
    "\n",
    "    dataset = MultiTaskDataset(fer2013_train_files, train_transforms, image_size=48,\n",
    "                            jigsaw_permutations=jigsaw_permutations,\n",
    "                            mask_ratio=0.75)\n",
    "\n",
    "    ssl_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    pretrained_metrics_loss = train_ssl_model(model_ssl, ssl_dataloader, num_epochs=10, device=device, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2bbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e253eda5",
   "metadata": {},
   "source": [
    "# ViT encoder fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467975ac",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),  # Upsample images to 224x224.\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "ckplus_dir_train = Path(os.getcwd(), 'datasets', 'fer2013')\n",
    "\n",
    "train_dataset = ImageFolder(root=ckplus_dir_train.joinpath('train'), transform=train_transforms)\n",
    "val_dataset   = ImageFolder(root=ckplus_dir_train.joinpath('test'), transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb417c",
   "metadata": {},
   "source": [
    "### FineTuningModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningModel(nn.Module):\n",
    "    def __init__(self, num_classes=7, image_size=224, use_pretrained_ssl=True, pretrained_path=\"vit_ssl_pretrained.pth\"):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "        \n",
    "        # Load the ViT model from torchvision.\n",
    "        self.encoder = vit_b_16(weights=None, image_size=image_size)\n",
    "        # Remove the default classification head.\n",
    "        self.encoder.heads = nn.Identity()\n",
    "        \n",
    "        # Optionally load the pretrained weights from SSL stage.\n",
    "        if use_pretrained_ssl:\n",
    "            state_dict = torch.load(pretrained_path, map_location='cuda')\n",
    "            self.encoder.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # Define a new classification head.\n",
    "        embedding_dim = 768  # This is the standard for vit_b_16.\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Assume the encoder outputs a tensor of shape [B, 1 + num_patches, D].\n",
    "        tokens = self.encoder(x)\n",
    "        # We use the CLS token (first token) for classification.\n",
    "        #cls_token = tokens[:, 0, :]\n",
    "        #logits = self.classifier(cls_token)\n",
    "        logits = self.classifier(tokens)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8a14",
   "metadata": {},
   "source": [
    "### Create Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Train one epoch of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The  model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities from model\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for logging; Total loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # TODO: Print/log training loss for this epoch\n",
    "        tk.set_postfix({'loss': '%6f' % float(epoch_loss / (t + 1))})\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Test one epoch of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "        \n",
    "        running_acc (float): Returns running accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[VALID]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for t, data in enumerate(tk):          \n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute log probabilities from model\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)            \n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate loss for logging; Total loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            running_acc = correct / total * 100\n",
    "\n",
    "            tk.set_postfix({'loss': '%6f' % float(epoch_loss / (t + 1)), 'acc': '%2f%%' %float(running_acc),})\n",
    "\n",
    "    return epoch_loss / len(dataloader), running_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, training_dataloader, testing_dataloader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 5\n",
    "\n",
    "    # Dictionary to store loss values over epochs\n",
    "    metrics_loss = {\n",
    "        'training_loss': [],\n",
    "        'validation_loss': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss = train_one_epoch(model, training_dataloader, criterion, optimizer, device, epoch, epochs)\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "\n",
    "        metrics_loss['training_loss'].append(train_loss)\n",
    "        metrics_loss['validation_loss'].append(valid_loss)\n",
    "\n",
    "        # Update the learning rate based on validation loss and print\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), 'fine_tuned_with_fer2013_NO_PRETRAIN.pt')\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "            \n",
    "        \n",
    "    print(\"\")\n",
    "    #return model\n",
    "    return metrics_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1b19",
   "metadata": {},
   "source": [
    "### Create model instance and call train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instance:\n",
    "model_fer = FineTuningModel(num_classes=7, image_size=48, use_pretrained_ssl=False)\n",
    "\n",
    "# Train the model:\n",
    "finetuning_losses = train_and_validate_model(model_fer, train_loader, val_loader, epochs=20, learning_rate=0.01, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c183a06",
   "metadata": {},
   "source": [
    "# FSL Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556c60f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
