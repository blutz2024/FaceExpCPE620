{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73db0601",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a374da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69d7f8",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5689ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# train_transforms = T.Compose([\n",
    "#     #T.ToPILImage(),\n",
    "#     T.Resize((48, 48)),\n",
    "#     #T.RandomHorizontalFlip(p=0.5),\n",
    "#     T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean, std)\n",
    "# ])\n",
    "\n",
    "# val_transforms = T.Compose([\n",
    "#     T.ToPILImage(),\n",
    "#     T.Resize((48, 48)),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean, std)\n",
    "# ])\n",
    "\n",
    "\n",
    "mean = [0.485]  # Single channel\n",
    "std = [0.229]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dec43c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RandomPatchMask:\n",
    "    \"\"\"This class creates a random patch mask for an image.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_ratio=0.15, patch_size=16):\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # img is a Tensor [C, H, W]\n",
    "        C, H, W = img.shape\n",
    "        # number of patches horizontally and vertically\n",
    "        num_patches_h = H // self.patch_size\n",
    "        num_patches_w = W // self.patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "        \n",
    "        # how many to mask\n",
    "        num_masked = int(total_patches * self.mask_ratio)\n",
    "        \n",
    "        # choose random patches\n",
    "        patch_indices = list(range(total_patches))\n",
    "        random.shuffle(patch_indices)\n",
    "        mask_indices = patch_indices[:num_masked]\n",
    "        \n",
    "        # create a copy to mask\n",
    "        masked_img = img.clone()\n",
    "        \n",
    "        for idx in mask_indices:\n",
    "            row = idx // num_patches_w\n",
    "            col = idx % num_patches_w\n",
    "            y_start = row * self.patch_size\n",
    "            x_start = col * self.patch_size\n",
    "            # set patch to 0\n",
    "            masked_img[:, y_start:y_start+self.patch_size, x_start:x_start+self.patch_size] = 0\n",
    "        \n",
    "        return masked_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dbf2d",
   "metadata": {},
   "source": [
    "### Unlabeled faces: MultiTaskDataset; FER2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4525fd8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_random_permutations(num_patches=9, num_permutations=30):\n",
    "    permutations = set()\n",
    "    while len(permutations) < num_permutations:\n",
    "        perm = tuple(random.sample(range(num_patches), num_patches))\n",
    "        permutations.add(perm)\n",
    "    return [list(p) for p in permutations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfdc270d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiTaskDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A single dataset that returns a dictionary with keys:\n",
    "       {\n",
    "         'denoise':  (denoise_input, denoise_target),\n",
    "         'rotation': (rotation_input, rotation_label),\n",
    "         'jigsaw':   (jigsaw_input,   jigsaw_label),\n",
    "         'mask':     (mask_input,     mask_target)\n",
    "       }\n",
    "\n",
    "    Each item in the batch corresponds to one original image, from which\n",
    "    we generate the different task inputs/targets on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths, base_transforms, image_size=224, \n",
    "                 jigsaw_permutations=None,  # e.g. a precomputed list of permutations\n",
    "                 mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.image_paths = image_paths\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Basic transforms (resize + to tensor + normalization)\n",
    "        # self.base_transform = T.Compose([\n",
    "        #     T.Resize((image_size, image_size)),\n",
    "        #     T.ToTensor(),\n",
    "        #     T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                 std=[0.229, 0.224, 0.225])\n",
    "        # ])\n",
    "        self.base_transform = base_transforms\n",
    "        \n",
    "        # If you have a fixed set of jigsaw permutations, store them here\n",
    "        # For example: jigsaw_permutations = [[0,1,2,3,4,5,6,7,8], [3,0,1,4,2,5,7,8,6], ...]\n",
    "        self.jigsaw_permutations = jigsaw_permutations\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Load the image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')  # or 'L' for grayscale\n",
    "        img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        #print('Current image: ', img_path, end='\\n')\n",
    "        \n",
    "        # Convert to tensor but *don't* normalize yet, because for denoising\n",
    "        # we might want the original pixel scale in [0,1].\n",
    "        # We'll do basic conversion:\n",
    "        img_tensor = T.ToTensor()(img)  # shape: [3, H, W], range ~[0,1]\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (a) Denoise: Add noise -> (noisy_input, clean_target)\n",
    "        # ---------------------------------------\n",
    "        noisy_input = self.add_gaussian_noise(img_tensor)\n",
    "        # The target is the *clean* version (we can still do normal transforms after)\n",
    "        denoise_input = self.base_transform(T.functional.to_pil_image(noisy_input))\n",
    "        denoise_target = self.base_transform(img)  \n",
    "        # (B, 3, H, W) after transform\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (b) Rotation: rotate the image randomly among {0, 90, 180, 270}\n",
    "        # ---------------------------------------\n",
    "        angles = [0, 90, 180, 270]\n",
    "        angle = random.choice(angles)\n",
    "        rotation_label = angles.index(angle)  # e.g. 0->0°, 1->90°, ...\n",
    "        rotated_img = img.rotate(angle)\n",
    "        rotation_input = self.base_transform(rotated_img)\n",
    "        # rotation_label is just an integer 0..3\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (c) Jigsaw puzzle: shuffle patches, produce a class label\n",
    "        # ---------------------------------------\n",
    "        # We'll do a simplistic approach: pick a random permutation from jigsaw_permutations\n",
    "        # and rearrange the patches accordingly. The \"class label\" is the index of that permutation.\n",
    "        # In a real scenario, you might just feed the shuffled patches as input\n",
    "        # and the label is the permutation index.\n",
    "        if self.jigsaw_permutations is not None:\n",
    "            perm_index = random.randint(0, len(self.jigsaw_permutations) - 1)\n",
    "            perm = self.jigsaw_permutations[perm_index]  # e.g. [3,0,1,2,4,5,6,7,8]\n",
    "            jigsaw_img = self.create_jigsaw(img, perm)\n",
    "            jigsaw_input = self.base_transform(jigsaw_img)\n",
    "            jigsaw_label = torch.tensor(perm_index, dtype=torch.long)\n",
    "        else:\n",
    "            # If you haven't defined permutations, fallback\n",
    "            jigsaw_input = self.base_transform(img)\n",
    "            jigsaw_label = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # (d) Masked Patch: randomly mask a fraction of patches\n",
    "        # ---------------------------------------\n",
    "        # We'll do a naive approach: zero out a random fraction of pixels. \n",
    "        # The \"target\" is the original unmasked image. \n",
    "        mask_input, mask_target = self.create_masked_input(img_tensor, self.mask_ratio)\n",
    "        mask_input = self.base_transform(T.functional.to_pil_image(mask_input))\n",
    "        mask_target = self.base_transform(T.functional.to_pil_image(mask_target))\n",
    "\n",
    "        # Return a dictionary of tasks\n",
    "        return {\n",
    "            'denoise':  (denoise_input,  denoise_target),\n",
    "            'rotation': (rotation_input, torch.tensor(rotation_label, dtype=torch.long)),\n",
    "            'jigsaw':   (jigsaw_input,   jigsaw_label),\n",
    "            'mask':     (mask_input,     mask_target),\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Helper functions\n",
    "    # -----------------------------\n",
    "    def add_gaussian_noise(self, img_tensor, std=0.1):\n",
    "        \"\"\"\n",
    "        Add random Gaussian noise to a [C, H, W] tensor, range ~[0,1].\n",
    "        std controls the noise level.\n",
    "        \"\"\"\n",
    "        noise = torch.randn_like(img_tensor) * std\n",
    "        noisy_img = img_tensor + noise\n",
    "        return torch.clamp(noisy_img, 0.0, 1.0)\n",
    "\n",
    "    def create_jigsaw(self, img_pil, permutation, grid_size=3):\n",
    "        \"\"\"\n",
    "        Slices the image into grid_size x grid_size patches, rearranges them\n",
    "        according to 'permutation' (list of length 9 if grid_size=3),\n",
    "        then reassembles them into a single PIL image.\n",
    "        \"\"\"\n",
    "        w, h = img_pil.size\n",
    "        patch_w = w // grid_size\n",
    "        patch_h = h // grid_size\n",
    "        patches = []\n",
    "        \n",
    "        # Cut into patches\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                left = col * patch_w\n",
    "                upper = row * patch_h\n",
    "                box = (left, upper, left+patch_w, upper+patch_h)\n",
    "                patch = img_pil.crop(box)\n",
    "                patches.append(patch)\n",
    "        \n",
    "        # Rearrange using permutation\n",
    "        shuffled_patches = [patches[i] for i in permutation]\n",
    "        \n",
    "        # Reassemble\n",
    "        new_img = Image.new('RGB', (w, h))\n",
    "        idx = 0\n",
    "        for row in range(grid_size):\n",
    "            for col in range(grid_size):\n",
    "                new_img.paste(shuffled_patches[idx], (col*patch_w, row*patch_h))\n",
    "                idx += 1\n",
    "        \n",
    "        return new_img\n",
    "\n",
    "    def create_masked_input(self, img_tensor, mask_ratio=0.75):\n",
    "        \"\"\"\n",
    "        Masks a random fraction (mask_ratio) of the image pixels by setting them to 0.\n",
    "        Returns (masked_img, original_img) as Tensors in [0,1].\n",
    "        \"\"\"\n",
    "        c, h, w = img_tensor.shape\n",
    "        num_pixels = h * w\n",
    "        num_mask = int(num_pixels * mask_ratio)\n",
    "\n",
    "        # Flatten the image, choose which pixels to mask\n",
    "        flat_img = img_tensor.view(c, -1).clone()  # shape [3, H*W]\n",
    "        mask_indices = random.sample(range(num_pixels), num_mask)\n",
    "        # zero out those pixels in all channels\n",
    "        for mi in mask_indices:\n",
    "            flat_img[:, mi] = 0.0\n",
    "        \n",
    "        masked_img = flat_img.view(c, h, w)\n",
    "        # The target is the original unmasked image\n",
    "        target_img = img_tensor.clone()\n",
    "        return masked_img, target_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc6a75",
   "metadata": {},
   "source": [
    "### Unlabeled faces DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04df5c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fer_2013_dir_train = Path(os.getcwd(), 'datasets', 'fer2013', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jigsaw_permutations = generate_random_permutations(num_patches=9, num_permutations=30)\n",
    "# fer2013_train_files = [p for p in Path(fer_2013_dir_train).rglob('*') if p.is_file()]\n",
    "\n",
    "# dataset = MultiTaskDataset(fer2013_train_files, train_transforms, image_size=48,\n",
    "#                            jigsaw_permutations=jigsaw_permutations,\n",
    "#                            mask_ratio=0.75)\n",
    "\n",
    "# ssl_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714510ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UnlabeledFacesDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, img_paths, transform=None):\n",
    "#         self.img_paths = img_paths\n",
    "#         self.transform = transform\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.img_paths[idx]\n",
    "#         # load, detect face, align, etc.\n",
    "#         face_img = cv2.imread(img_path).astype('float32')\n",
    "        \n",
    "#         if face_img is None:\n",
    "#             # handle missing face or skip\n",
    "#             # for simplicity, just return a dummy or raise an error\n",
    "#             raise RuntimeError(f\"No face found in {img_path}\")\n",
    "        \n",
    "#         if self.transform:\n",
    "#             face_img = self.transform(face_img)\n",
    "        \n",
    "#         return face_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9059a",
   "metadata": {},
   "source": [
    "### Labeled faces: CK+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LabeledFERDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, img_paths, labels, transform=None):\n",
    "#         self.img_paths = img_paths\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_paths)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.img_paths[idx]\n",
    "#         label = self.labels[idx]\n",
    "        \n",
    "#         face_img = cv2.imread(img_path).astype('float32')\n",
    "#         if face_img is None:\n",
    "#             raise RuntimeError(f\"No face found in {img_path}\")\n",
    "        \n",
    "#         if self.transform:\n",
    "#             face_img = self.transform(face_img)\n",
    "        \n",
    "#         return face_img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3085cb",
   "metadata": {},
   "source": [
    "### FewShot Image Dataset: Manga faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01528619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FewShotFERDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, class_folders, transform=None, n_way=5, k_shot=1, k_query=5):\n",
    "#         # class_folders: e.g. {class_name: [list_of_image_paths]}\n",
    "#         self.class_folders = class_folders\n",
    "#         self.transform = transform\n",
    "#         self.n_way = n_way\n",
    "#         self.k_shot = k_shot\n",
    "#         self.k_query = k_query\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         # The \"length\" might be the number of episodes you want\n",
    "#         return 1000  # or some large number for meta-training\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Sample n_way classes\n",
    "#         sampled_classes = random.sample(list(self.class_folders.keys()), self.n_way)\n",
    "        \n",
    "#         support_imgs = []\n",
    "#         support_labels = []\n",
    "#         query_imgs = []\n",
    "#         query_labels = []\n",
    "        \n",
    "#         label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "#         for cls_name in sampled_classes:\n",
    "#             paths = self.class_folders[cls_name]\n",
    "#             selected_paths = random.sample(paths, self.k_shot + self.k_query)\n",
    "#             s_paths = selected_paths[:self.k_shot]\n",
    "#             q_paths = selected_paths[self.k_shot:]\n",
    "            \n",
    "#             for sp in s_paths:\n",
    "#                 img = cv2.imread(sp).astype('float32')\n",
    "#                 if img is not None and self.transform:\n",
    "#                     img = self.transform(img)\n",
    "#                 support_imgs.append(img)\n",
    "#                 support_labels.append(label_map[cls_name])\n",
    "            \n",
    "#             for qp in q_paths:\n",
    "#                 img = cv2.imread(qp).astype('float32')\n",
    "#                 if img is not None and self.transform:\n",
    "#                     img = self.transform(img)\n",
    "#                 query_imgs.append(img)\n",
    "#                 query_labels.append(label_map[cls_name])\n",
    "        \n",
    "#         # Convert lists to tensors\n",
    "#         support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "#         support_labels = torch.tensor(support_labels)  # [n_way*k_shot]\n",
    "#         query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "#         query_labels = torch.tensor(query_labels) # [n_way*k_query]\n",
    "        \n",
    "#         return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c9d60",
   "metadata": {},
   "source": [
    "# Self-supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb5bf2d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf238442",
   "metadata": {},
   "source": [
    "### ViT_SelfSupervised class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9220dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_SelfSupervised(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=224,\n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 embedding_dim=768,\n",
    "                 num_rotation_classes=4,\n",
    "                 num_jigsaw_classes=30):\n",
    "        \"\"\"\n",
    "        image_size: input image resolution (assumed square)\n",
    "        patch_size: size of each non-overlapping patch\n",
    "        in_chans: number of input channels (e.g., 3 for RGB)\n",
    "        embedding_dim: dimension of patch embeddings from ViT\n",
    "        num_rotation_classes: number of rotation categories (0°, 90°, 180°, 270°)\n",
    "        num_jigsaw_classes: number of predefined jigsaw permutation orders\n",
    "        \"\"\"\n",
    "        super(ViT_SelfSupervised, self).__init__()\n",
    "        \n",
    "        # Calculate the number of patches (ViT divides the image into non-overlapping patches)\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # --- ViT Encoder ---\n",
    "        # Instantiate the ViT model from TorchVision.\n",
    "        # We set weights=False to train from scratch.\n",
    "        self.encoder = vit_b_16(weights=None, image_size=image_size, num_classes=7)\n",
    "        # Remove its default classification head by replacing with Identity.\n",
    "        # IMPORTANT: In this example we assume that the modified encoder returns the CLS token and the patch tokens.\n",
    "        self.encoder.heads = nn.Identity()\n",
    "\n",
    "        # --- Pretext Task Heads ---\n",
    "\n",
    "        # (a) Denoising & Reconstruction head.\n",
    "        # For each patch token, we want to predict the original patch pixels.\n",
    "        # Here we use a simple linear projection: output dim = patch_size x patch_size x in_chans.\n",
    "        self.denoise_decoder = nn.Linear(embedding_dim, patch_size * patch_size * in_chans)\n",
    "\n",
    "        # (b) Rotation Prediction head.\n",
    "        # Uses the CLS token from the encoder.\n",
    "        self.rotation_head = nn.Linear(embedding_dim, num_rotation_classes)\n",
    "\n",
    "        # (c) Jigsaw Puzzle head.\n",
    "        # For the jigsaw task, we assume that the image (with patches shuffled) is processed by the encoder.\n",
    "        # We then flatten the patch tokens (excluding the CLS token) and feed them to an MLP that predicts one of\n",
    "        # num_jigsaw_classes possible permutation orders.\n",
    "        self.jigsaw_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * self.num_patches, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_jigsaw_classes)\n",
    "        )\n",
    "\n",
    "        # (d) Masked Patch Prediction head.\n",
    "        # Similar to the denoising decoder, but applied only to the masked patches.\n",
    "        self.mask_decoder = nn.Linear(embedding_dim, patch_size * patch_size * in_chans)\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the ViT encoder.\n",
    "        We assume that the encoder returns a tensor of shape:\n",
    "           (B, num_tokens, embedding_dim)\n",
    "        where token 0 is the CLS token and tokens 1: are the patch tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        # x: [B, C, H, W]\n",
    "        # Use conv_proj to get patch embeddings.\n",
    "        # This produces an output of shape [B, embed_dim, H_p, W_p],\n",
    "        # where H_p * W_p equals the total number of patches.\n",
    "        x = self.encoder.conv_proj(x)  # shape: [B, embedding_dim, H_p, W_p]\n",
    "\n",
    "        B, C, H_p, W_p = x.shape\n",
    "        # Flatten the spatial dimensions and transpose to get shape [B, num_patches, embedding_dim]\n",
    "        x = x.flatten(2).transpose(1, 2)  # shape: [B, H_p*W_p, embedding_dim]\n",
    "\n",
    "        # Expand the CLS token to batch size.\n",
    "        #cls_tokens = self.encoder.cls_token.expand(B, -1, -1)  # shape: [B, 1, embedding_dim]\n",
    "        cls_tokens = self.encoder.class_token.expand(B, -1, -1)\n",
    "\n",
    "\n",
    "        # Concatenate the CLS token with the patch tokens.\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # shape: [B, 1 + num_patches, embedding_dim]\n",
    "        \n",
    "        ### option 1\n",
    "        # Retrieve positional embeddings from state_dict\n",
    "        # pos_embed = self.encoder.state_dict()['pos_embed']  # shape: [1, 1+num_patches, embedding_dim]\n",
    "        \n",
    "        # #pos_embed = self.encoder.pos_embed\n",
    "        # x = x + pos_embed.to(x.device)\n",
    "        \n",
    "        # x = self.encoder.dropout(x)\n",
    "\n",
    "        # # Pass through transformer encoder blocks.\n",
    "        # for blk in self.encoder.encoder.blocks:\n",
    "        #     x = blk(x)\n",
    "        # # Apply final normalization.\n",
    "        # x = self.encoder.encoder.norm(x)\n",
    "        \n",
    "        \n",
    "        ### option 2\n",
    "        # Check where the correct dropout layer is located\n",
    "        if hasattr(self.encoder, 'dropout') and callable(self.encoder.dropout):\n",
    "            x = self.encoder.dropout(x)  # Apply dropout if it exists\n",
    "        elif hasattr(self.encoder.encoder, 'dropout') and callable(self.encoder.encoder.dropout):\n",
    "            x = self.encoder.encoder.dropout(x)  # Apply encoder-level dropout\n",
    "            \n",
    "        for blk in self.encoder.encoder.layers:\n",
    "            x = blk(x)\n",
    "        x = self.encoder.encoder.ln(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    # def forward_denoise(self, x):\n",
    "    #     \"\"\"\n",
    "    #     Image denoising and reconstruction task.\n",
    "    #     x: input noisy images.\n",
    "    #     Returns: reconstructed patches for all patch tokens.\n",
    "    #     \"\"\"\n",
    "    #     tokens = self.forward_encoder(x)  # shape: (B, 1 + num_patches, D)\n",
    "    #     # Exclude the CLS token (first token)\n",
    "    #     patch_tokens = tokens[:, 1:, :]  # shape: (B, num_patches, D)\n",
    "    #     # Reconstruct each patch.\n",
    "    #     # The output will be of shape (B, num_patches, patch_size*patch_size*in_chans)\n",
    "    #     recon_patches = self.denoise_decoder(patch_tokens)\n",
    "    #     return recon_patches\n",
    "    def forward_denoise(self, x):\n",
    "        \"\"\"Denoising task with proper reshaping\"\"\"\n",
    "        B = x.shape[0]\n",
    "        tokens = self.forward_encoder(x)\n",
    "        patch_tokens = tokens[:, 1:, :]  # [B, num_patches, D]\n",
    "        \n",
    "        # Reconstruct patches\n",
    "        recon_patches = self.denoise_decoder(patch_tokens)  # [B, 9, 768]\n",
    "        \n",
    "        # Reshape to original image dimensions [B, 3, 48, 48]\n",
    "        recon_image = self.patches_to_image(recon_patches, B)\n",
    "        return recon_image\n",
    "\n",
    "    def forward_masked(self, x):\n",
    "        \"\"\"Masked patch prediction with proper reshaping\"\"\"\n",
    "        B = x.shape[0]\n",
    "        tokens = self.forward_encoder(x)\n",
    "        patch_tokens = tokens[:, 1:, :]  # [B, 9, D]\n",
    "        \n",
    "        # Reconstruct patches\n",
    "        mask_predictions = self.mask_decoder(patch_tokens)  # [B, 9, 768]\n",
    "        \n",
    "        # Reshape to original image dimensions [B, 3, 48, 48]\n",
    "        recon_image = self.patches_to_image(mask_predictions, B)\n",
    "        return recon_image\n",
    "\n",
    "    def patches_to_image(self, patches, batch_size):\n",
    "        \"\"\"Convert patch sequence to image tensor\"\"\"\n",
    "        # patches shape: [B, num_patches, patch_size^2 * 3]\n",
    "        patch_size = self.patch_size\n",
    "        num_patches = self.num_patches\n",
    "        channels = self.in_chans\n",
    "        \n",
    "        # Reshape to [B, num_patches, C, patch_size, patch_size]\n",
    "        patches = patches.view(\n",
    "            batch_size, \n",
    "            num_patches, \n",
    "            channels, \n",
    "            patch_size, \n",
    "            patch_size\n",
    "        )\n",
    "        \n",
    "        # Reshape to image grid\n",
    "        grid_size = int(num_patches ** 0.5)  # 3 for 9 patches\n",
    "        image = patches.permute(0, 2, 1, 3, 4)  # [B, C, num_patches, p, p]\n",
    "        image = image.contiguous().view(\n",
    "            batch_size, \n",
    "            channels, \n",
    "            grid_size * patch_size, \n",
    "            grid_size * patch_size\n",
    "        )\n",
    "        return image\n",
    "\n",
    "    def forward_rotation(self, x):\n",
    "        \"\"\"\n",
    "        Image rotation prediction task.\n",
    "        x: input rotated images.\n",
    "        Returns: rotation logits predicted from the CLS token.\n",
    "        \"\"\"\n",
    "        tokens = self.forward_encoder(x)  # shape: (B, 1 + num_patches, D)\n",
    "        cls_token = tokens[:, 0, :]         # shape: (B, D)\n",
    "        rotation_logits = self.rotation_head(cls_token)\n",
    "        return rotation_logits\n",
    "\n",
    "    def forward_jigsaw(self, x):\n",
    "        \"\"\"\n",
    "        Jigsaw puzzle task.\n",
    "        x: input images with shuffled patches.\n",
    "        Returns: logits for predicting the permutation order (classification over num_jigsaw_classes).\n",
    "        \"\"\"\n",
    "        tokens = self.forward_encoder(x)      # shape: (B, 1 + num_patches, D)\n",
    "        patch_tokens = tokens[:, 1:, :]         # remove CLS token, shape: (B, num_patches, D)\n",
    "        # Flatten patch tokens for each image: (B, num_patches * D)\n",
    "        flat_tokens = patch_tokens.reshape(x.size(0), -1)\n",
    "        jigsaw_logits = self.jigsaw_head(flat_tokens)\n",
    "        return jigsaw_logits\n",
    "\n",
    "    # def forward_masked(self, x):\n",
    "    #     \"\"\"\n",
    "    #     Masked patch prediction task.\n",
    "    #     x: input images with some patches masked out.\n",
    "    #        (The masking operation should be performed in the data pre-processing or transform.)\n",
    "    #     Returns: reconstructed predictions for the (unobserved) masked patches.\n",
    "    #     For simplicity, here we process the entire set of patch tokens, and later you would compare\n",
    "    #     the output for masked locations with the ground truth.\n",
    "    #     \"\"\"\n",
    "    #     tokens = self.forward_encoder(x)      # shape: (B, 1 + num_patches, D)\n",
    "    #     patch_tokens = tokens[:, 1:, :]         # shape: (B, num_patches, D)\n",
    "    #     # Predict patch pixel values for each token.\n",
    "    #     mask_predictions = self.mask_decoder(patch_tokens)\n",
    "    #     return mask_predictions\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        \"\"\"\n",
    "        A unified forward method that selects the appropriate pretext task.\n",
    "        task: a string specifying the task type: 'denoise', 'rotation', 'jigsaw', or 'mask'.\n",
    "        \"\"\"\n",
    "        if task == 'denoise':\n",
    "            return self.forward_denoise(x)\n",
    "        elif task == 'rotation':\n",
    "            return self.forward_rotation(x)\n",
    "        elif task == 'jigsaw':\n",
    "            return self.forward_jigsaw(x)\n",
    "        elif task == 'mask':\n",
    "            return self.forward_masked(x)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task specified. Choose from 'denoise', 'rotation', 'jigsaw', or 'mask'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c243fd9",
   "metadata": {},
   "source": [
    "### MultiTaskLossWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d64fbc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the log-variances (or deltas) as learnable parameters.\n",
    "        # We'll store them in log-space for numerical stability, e.g. log(delta^2).\n",
    "        self.log_var_den = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_rot = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_puz = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_msk = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, L_den, L_rot, L_puz, L_msk):\n",
    "        # Convert log_var_* to delta = exp(log_var / 2)\n",
    "        # But the formula in the paper effectively uses 1/(2*delta^2), so we can do:\n",
    "        # Weighted losses:\n",
    "        w_den = 1.0 / (2.0 * torch.exp(self.log_var_den))\n",
    "        w_rot = 1.0 / (2.0 * torch.exp(self.log_var_rot))\n",
    "        w_puz = 1.0 / (2.0 * torch.exp(self.log_var_puz))\n",
    "        w_msk = 1.0 / (2.0 * torch.exp(self.log_var_msk))\n",
    "\n",
    "        # Combined loss\n",
    "        loss = (w_den * L_den \n",
    "                + w_rot * L_rot\n",
    "                + w_puz * L_puz\n",
    "                + w_msk * L_msk\n",
    "                + (self.log_var_den + self.log_var_rot \n",
    "                   + self.log_var_puz + self.log_var_msk) * 0.5)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9a54d2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "multi_task_loss_fn = MultiTaskLossWrapper().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1ec8c",
   "metadata": {},
   "source": [
    "### Train SSL Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e4b1ec2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_ssl_model(model, dataloader, num_epochs=10, device='cuda', learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    This is a simplified training loop that assumes:\n",
    "      - The dataloader returns a dictionary containing inputs for each task.\n",
    "        For example, each batch might be a dict with keys: 'denoise', 'rotation', 'jigsaw', 'mask'\n",
    "      - Each key maps to a tuple: (input_tensor, target) where target is the ground truth for that task.\n",
    "    In practice, you may need separate dataloaders or combine losses with appropriate weighting.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "    \n",
    "    # For simplicity, we use fixed weights for each task’s loss.\n",
    "    weight_denoise = 1.0\n",
    "    weight_rotation = 1.0\n",
    "    weight_jigsaw = 1.0\n",
    "    weight_mask = 1.0\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 3  # Stop training if no improvement after this many epochs\n",
    "    \n",
    "    metrics_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(epoch + 1) + \"/\" + str(num_epochs))\n",
    "        #print('DATALOADER LEN \\n', len(dataloader))\n",
    "        \n",
    "        for t, batch in enumerate(tk):\n",
    "            optimizer.zero_grad()\n",
    "            #loss = 0.0\n",
    "            L_den = 0.0\n",
    "            L_rot = 0.0\n",
    "            L_jig = 0.0\n",
    "            L_mask = 0.0\n",
    "            #print('BATCH IS \\n', batch)\n",
    "            #break\n",
    "\n",
    "            # --- Denoising & Reconstruction Task ---\n",
    "            if 'denoise' in batch:\n",
    "                inputs_denoise, targets_denoise = batch['denoise']  # targets: original patches or full image patches\n",
    "                inputs_denoise = inputs_denoise.to(device)\n",
    "                targets_denoise = targets_denoise.to(device)\n",
    "                recon_patches = model(inputs_denoise, task='denoise')\n",
    "                # Reshape targets to match the output if necessary.\n",
    "                loss_denoise = F.mse_loss(recon_patches, targets_denoise)\n",
    "                L_den += weight_denoise * loss_denoise\n",
    "\n",
    "            # --- Rotation Prediction Task ---\n",
    "            if 'rotation' in batch:\n",
    "                inputs_rotation, targets_rotation = batch['rotation']\n",
    "                inputs_rotation = inputs_rotation.to(device)\n",
    "                targets_rotation = targets_rotation.to(device)\n",
    "                rotation_logits = model(inputs_rotation, task='rotation')\n",
    "                loss_rotation = F.cross_entropy(rotation_logits, targets_rotation)\n",
    "                L_rot += weight_rotation * loss_rotation\n",
    "\n",
    "            # --- Jigsaw Puzzle Task ---\n",
    "            if 'jigsaw' in batch:\n",
    "                inputs_jigsaw, targets_jigsaw = batch['jigsaw']  # targets: permutation labels (integer class index)\n",
    "                inputs_jigsaw = inputs_jigsaw.to(device)\n",
    "                targets_jigsaw = targets_jigsaw.to(device)\n",
    "                jigsaw_logits = model(inputs_jigsaw, task='jigsaw')\n",
    "                loss_jigsaw = F.cross_entropy(jigsaw_logits, targets_jigsaw)\n",
    "                L_jig += weight_jigsaw * loss_jigsaw\n",
    "\n",
    "            # --- Masked Patch Prediction Task ---\n",
    "            if 'mask' in batch:\n",
    "                inputs_mask, targets_mask = batch['mask']\n",
    "                inputs_mask = inputs_mask.to(device)\n",
    "                targets_mask = targets_mask.to(device)\n",
    "                mask_predictions = model(inputs_mask, task='mask')\n",
    "                loss_mask = F.mse_loss(mask_predictions, targets_mask)\n",
    "                L_mask += weight_mask * loss_mask\n",
    "                \n",
    "            # Combine them with learned weights:\n",
    "            L_ssl = multi_task_loss_fn(L_den, L_rot, L_jig, L_mask)\n",
    "\n",
    "            L_ssl.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += L_ssl.item()\n",
    "            tk.set_postfix({'loss': '%6f' % float(L_ssl / (t + 1))})\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        metrics_loss.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            # Save the pretrained encoder weights (and optionally heads) for later fine-tuning.\n",
    "            torch.save(model.state_dict(), \"weights/vit_ssl_pretrained.pth\")\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "\n",
    "    print(\"\")\n",
    "    return metrics_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879e86c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Example usage:\n",
    "# Assume we have a combined dataloader that yields a dict with keys: 'denoise', 'rotation', 'jigsaw', 'mask'.\n",
    "# Each entry is a tuple: (input_tensor, target_tensor)\n",
    "# In practice, you need to implement or combine datasets that perform the corresponding data augmentation.\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model instance \n",
    "    model_ssl = ViT_SelfSupervised(image_size=48, patch_size=16, in_chans=3,\n",
    "                                   embedding_dim=768, num_rotation_classes=4,\n",
    "                                   num_jigsaw_classes=30)\n",
    "    #print(dir(model_ssl))\n",
    "    # Here, \"ssl_dataloader\" should be defined by you.\n",
    "    # For demonstration purposes, assume it's provided.\n",
    "    # ssl_dataloader = ...\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # train_ssl_model(model_ssl, ssl_dataloader, num_epochs=10, device=device)\n",
    "    \n",
    "    jigsaw_permutations = generate_random_permutations(num_patches=9, num_permutations=30)\n",
    "    fer2013_train_files = [p for p in Path(fer_2013_dir_train).rglob('*') if p.is_file()]\n",
    "\n",
    "    dataset = MultiTaskDataset(fer2013_train_files, train_transforms, image_size=48,\n",
    "                            jigsaw_permutations=jigsaw_permutations,\n",
    "                            mask_ratio=0.75)\n",
    "\n",
    "    ssl_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    \n",
    "    pretrained_metrics_loss = train_ssl_model(model_ssl, ssl_dataloader, num_epochs=10, device=device, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2bbee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e253eda5",
   "metadata": {},
   "source": [
    "# ViT encoder fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467975ac",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c05a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485]  # Single channel\n",
    "std = [0.229]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "fer2013_dir_train = Path(os.getcwd(), 'datasets', 'fer2013')\n",
    "\n",
    "train_dataset = ImageFolder(root=fer2013_dir_train.joinpath('train'), transform=train_transforms)\n",
    "val_dataset   = ImageFolder(root=fer2013_dir_train.joinpath('test'), transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb417c",
   "metadata": {},
   "source": [
    "### FineTuningModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f3dc73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningModel(nn.Module):\n",
    "    def __init__(self, num_classes=7, image_size=224, use_pretrained_ssl=True, pretrained_path=\"vit_ssl_pretrained.pth\"):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "        \n",
    "        # Load the ViT model from torchvision.\n",
    "        self.encoder = vit_b_16(weights=None, image_size=image_size)\n",
    "        # Remove the default classification head.\n",
    "        self.encoder.heads = nn.Identity()\n",
    "        \n",
    "        # Optionally load the pretrained weights from SSL stage.\n",
    "        if use_pretrained_ssl:\n",
    "            state_dict = torch.load(pretrained_path, map_location='cuda')\n",
    "            self.encoder.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        # Define a new classification head.\n",
    "        embedding_dim = 768  # This is the standard for vit_b_16.\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        # Assume the encoder outputs a tensor of shape [B, 1 + num_patches, D].\n",
    "        tokens = self.encoder(x)\n",
    "        # We use the CLS token (first token) for classification.\n",
    "        # Handle different dimensional outputs\n",
    "        if tokens.dim() == 3:\n",
    "            # Standard ViT output: [B, num_tokens, D]\n",
    "            cls_token = tokens[:, 0, :]  # Extract CLS token\n",
    "        else:\n",
    "            # Direct feature output: [B, D]\n",
    "            cls_token = tokens\n",
    "        \n",
    "        # Get final logits\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        return (logits, cls_token) if return_embeddings else logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8a14",
   "metadata": {},
   "source": [
    "### Create Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de0701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred,y_true):\n",
    "    top_p,top_class = y_pred.topk(1, dim = 1)\n",
    "    equals = top_class == y_true.view(*top_class.shape)\n",
    "    return torch.mean(equals.type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e3b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Train one epoch of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The  model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities from model\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for logging; Total loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "\n",
    "        # Print/log training loss and accuracy for this epoch\n",
    "        tk.set_postfix({\n",
    "            'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "            'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "        })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7824849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Test one epoch of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "        \n",
    "        running_acc (float): Returns running accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[VALID]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for t, data in enumerate(tk):          \n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute log probabilities from model\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)            \n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate loss for logging; Total loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "            \n",
    "            #running_acc = correct / total * 100\n",
    "\n",
    "            #tk.set_postfix({'loss': '%6f' % float(epoch_loss / (t + 1)), 'acc': '%2f%%' %float(running_acc),})\n",
    "            tk.set_postfix({\n",
    "                'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "                'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "            })\n",
    "\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, training_dataloader, testing_dataloader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Train and Test the model using loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 5\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'training_loss': [],\n",
    "        'training_accuracy': [],\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss, train_accuracy = train_one_epoch(model, training_dataloader, criterion, optimizer, device, epoch, epochs)\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "\n",
    "        history_metrics['training_loss'].append(train_loss)\n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['training_accuracy'].append(train_accuracy)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "\n",
    "        # Update the learning rate based on validation loss and print\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), 'weights/vit_fine_tuned_with_fer2013_NO_PRETRAIN.pt')\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "            \n",
    "        \n",
    "    print(\"\")\n",
    "    #return model\n",
    "    return history_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1b19",
   "metadata": {},
   "source": [
    "### Create model instance and call train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b888288",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8162a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model instance:\n",
    "model_fer = FineTuningModel(num_classes=7, image_size=48, use_pretrained_ssl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/20: 100%|██████████| 898/898 [01:28<00:00, 10.17it/s, loss=1.797473, acc=0.251246]\n",
      "EPOCH[VALID]1/20: 100%|██████████| 225/225 [00:29<00:00,  7.66it/s, loss=1.780289, acc=0.262944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 2, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/20: 100%|██████████| 898/898 [01:05<00:00, 13.65it/s, loss=1.769565, acc=0.266815]\n",
      "EPOCH[VALID]2/20: 100%|██████████| 225/225 [00:17<00:00, 13.19it/s, loss=1.779265, acc=0.257472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 3, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/20: 100%|██████████| 898/898 [01:05<00:00, 13.73it/s, loss=1.758142, acc=0.278410]\n",
      "EPOCH[VALID]3/20: 100%|██████████| 225/225 [00:17<00:00, 13.13it/s, loss=1.733075, acc=0.293611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 4, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/20: 100%|██████████| 898/898 [01:06<00:00, 13.56it/s, loss=1.748842, acc=0.284222]\n",
      "EPOCH[VALID]4/20: 100%|██████████| 225/225 [00:16<00:00, 13.26it/s, loss=1.720345, acc=0.298028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 5, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/20: 100%|██████████| 898/898 [01:07<00:00, 13.27it/s, loss=1.742977, acc=0.284883]\n",
      "EPOCH[VALID]5/20: 100%|██████████| 225/225 [00:16<00:00, 13.32it/s, loss=1.727594, acc=0.308472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 6, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/20: 100%|██████████| 898/898 [01:06<00:00, 13.41it/s, loss=1.732255, acc=0.295476]\n",
      "EPOCH[VALID]6/20: 100%|██████████| 225/225 [00:30<00:00,  7.38it/s, loss=1.738679, acc=0.274861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 7, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/20: 100%|██████████| 898/898 [01:08<00:00, 13.08it/s, loss=1.731515, acc=0.294119]\n",
      "EPOCH[VALID]7/20: 100%|██████████| 225/225 [00:17<00:00, 13.04it/s, loss=1.699793, acc=0.319444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 8, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/20: 100%|██████████| 898/898 [01:05<00:00, 13.70it/s, loss=1.721614, acc=0.302332]\n",
      "EPOCH[VALID]8/20: 100%|██████████| 225/225 [00:17<00:00, 12.86it/s, loss=1.691885, acc=0.311361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 9, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/20: 100%|██████████| 898/898 [01:05<00:00, 13.73it/s, loss=1.721803, acc=0.301051]\n",
      "EPOCH[VALID]9/20: 100%|██████████| 225/225 [00:16<00:00, 13.41it/s, loss=1.699319, acc=0.317611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 10, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/20: 100%|██████████| 898/898 [01:06<00:00, 13.54it/s, loss=1.711502, acc=0.309069]\n",
      "EPOCH[VALID]10/20: 100%|██████████| 225/225 [00:16<00:00, 13.40it/s, loss=1.736935, acc=0.304028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 11, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]11/20: 100%|██████████| 898/898 [01:04<00:00, 14.02it/s, loss=1.707864, acc=0.309312]\n",
      "EPOCH[VALID]11/20: 100%|██████████| 225/225 [00:16<00:00, 13.27it/s, loss=1.659956, acc=0.336250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 12, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]12/20: 100%|██████████| 898/898 [01:03<00:00, 14.05it/s, loss=1.707028, acc=0.310962]\n",
      "EPOCH[VALID]12/20: 100%|██████████| 225/225 [00:17<00:00, 12.72it/s, loss=1.669878, acc=0.327361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 13, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]13/20: 100%|██████████| 898/898 [01:04<00:00, 14.02it/s, loss=1.703111, acc=0.314129]\n",
      "EPOCH[VALID]13/20: 100%|██████████| 225/225 [00:16<00:00, 13.49it/s, loss=1.646916, acc=0.346806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 14, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]14/20: 100%|██████████| 898/898 [01:03<00:00, 14.13it/s, loss=1.699556, acc=0.312368]\n",
      "EPOCH[VALID]14/20: 100%|██████████| 225/225 [00:16<00:00, 13.57it/s, loss=1.648841, acc=0.346722]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 15, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]15/20: 100%|██████████| 898/898 [01:03<00:00, 14.18it/s, loss=1.695200, acc=0.319265]\n",
      "EPOCH[VALID]15/20: 100%|██████████| 225/225 [00:16<00:00, 13.46it/s, loss=1.674029, acc=0.324056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 16, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]16/20: 100%|██████████| 898/898 [01:04<00:00, 13.82it/s, loss=1.689902, acc=0.321123]\n",
      "EPOCH[VALID]16/20: 100%|██████████| 225/225 [00:17<00:00, 12.61it/s, loss=1.634338, acc=0.349028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 17, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]17/20: 100%|██████████| 898/898 [01:04<00:00, 13.89it/s, loss=1.684782, acc=0.327304]\n",
      "EPOCH[VALID]17/20: 100%|██████████| 225/225 [00:16<00:00, 13.39it/s, loss=1.631200, acc=0.354583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 18, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]18/20: 100%|██████████| 898/898 [01:04<00:00, 13.82it/s, loss=1.685150, acc=0.325425]\n",
      "EPOCH[VALID]18/20: 100%|██████████| 225/225 [00:16<00:00, 13.52it/s, loss=1.680742, acc=0.322694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 19, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]19/20: 100%|██████████| 898/898 [01:04<00:00, 13.89it/s, loss=1.678458, acc=0.327408]\n",
      "EPOCH[VALID]19/20: 100%|██████████| 225/225 [00:16<00:00, 13.29it/s, loss=1.645004, acc=0.337528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 20, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]20/20: 100%|██████████| 898/898 [01:04<00:00, 13.88it/s, loss=1.676877, acc=0.327666]\n",
      "EPOCH[VALID]20/20: 100%|██████████| 225/225 [00:17<00:00, 13.20it/s, loss=1.627575, acc=0.352111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "finetuning_losses = train_and_validate_model(model_fer, train_loader, val_loader, epochs=20, learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80331968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time:  0:28:24.946426\n"
     ]
    }
   ],
   "source": [
    "time2 = datetime.now()\n",
    "print(\"Total training time: \", time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "734640e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(finetuning_losses['training_loss']) + 1)),\n",
    "    \"Training Loss\": finetuning_losses['training_loss'],\n",
    "    \"Validation Loss\": finetuning_losses['validation_loss'],\n",
    "    \"Training Accuracy\": [acc.cpu().item() for acc in finetuning_losses['training_accuracy']],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in finetuning_losses['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/vit_model_stats_001_50_epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac478898",
   "metadata": {},
   "source": [
    "# Test Model Accuracy on Out of Distribution Data set (Manga Faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01049cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out_of_distribution(model, testing_dataloader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "        \n",
    "    Returns:\n",
    "        history_metrics (dict): Dictionary containing validation loss and accuracy over epochs.\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "        \n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "                \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7bf31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "        \"initial_filters\": 8,    \n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_classes\": 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e00cfbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_model_2 = model_fer.to(device)\n",
    "vit_model_2.load_state_dict(torch.load('weights/vit_fine_tuned_with_fer2013_NO_PRETRAIN.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef10d47",
   "metadata": {},
   "source": [
    "## Import MangaFaces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d406047",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae7d7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "manga_faces_train_dir = Path(os.getcwd(), 'datasets', 'manga', 'train')\n",
    "manga_faces_train_images = ImageFolder(root=manga_faces_train_dir, transform=train_transforms)\n",
    "manga_faces_train_images_loader = DataLoader(manga_faces_train_images, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test Set\n",
    "manga_faces_test_dir = Path(os.getcwd(), 'datasets', 'manga', 'test')\n",
    "manga_faces_test_images = ImageFolder(root=manga_faces_test_dir, transform=val_transforms)\n",
    "manga_faces_test_images_loader = DataLoader(manga_faces_test_images, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743f8a8",
   "metadata": {},
   "source": [
    "## Run 'test_out_of_distribution' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e035bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/5: 100%|██████████| 7/7 [00:02<00:00,  3.32it/s, loss=2.658994, acc=0.095536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/5: 100%|██████████| 7/7 [00:00<00:00,  8.82it/s, loss=2.675275, acc=0.113393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/5: 100%|██████████| 7/7 [00:00<00:00,  8.70it/s, loss=2.626171, acc=0.093750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/5: 100%|██████████| 7/7 [00:00<00:00,  9.59it/s, loss=2.660418, acc=0.116071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/5: 100%|██████████| 7/7 [00:00<00:00, 11.18it/s, loss=2.655772, acc=0.108036]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_metrics = test_out_of_distribution(vit_model_2, manga_faces_train_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99abed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Store the metrics from when the model was tested on the out-of-distribution dataset\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(test_out_of_distribution_metrics['validation_loss']) + 1)),\n",
    "    \"Validation Loss\": test_out_of_distribution_metrics['validation_loss'],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in test_out_of_distribution_metrics['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/vit_model_out_of_distribution_stats_001_5_epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be05ecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/5: 100%|██████████| 5/5 [00:01<00:00,  2.85it/s, loss=2.480916, acc=0.218750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/5: 100%|██████████| 5/5 [00:00<00:00, 13.60it/s, loss=2.793143, acc=0.087500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/5: 100%|██████████| 5/5 [00:00<00:00, 19.59it/s, loss=2.590372, acc=0.175000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/5: 100%|██████████| 5/5 [00:00<00:00, 20.10it/s, loss=2.689698, acc=0.087500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/5: 100%|██████████| 5/5 [00:00<00:00, 19.24it/s, loss=2.762226, acc=0.087500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_metrics = test_out_of_distribution(vit_model_2, manga_faces_test_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c183a06",
   "metadata": {},
   "source": [
    "# FSL Domain Adaptation Prototypical Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556c60f",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15233c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotFERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for few-shot FER, where images are organized by class in folders.\n",
    "    This dataset generates episodes (tasks) on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, n_way=5, k_shot=1, k_query=5, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Root folder containing one folder per class.\n",
    "        n_way: number of classes per episode.\n",
    "        k_shot: number of support examples per class.\n",
    "        k_query: number of query examples per class.\n",
    "        transform: transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Build a mapping: class -> list of image paths.\n",
    "        self.class_to_imgs = {}\n",
    "        for cls_name in os.listdir(root_dir):\n",
    "            cls_folder = Path.joinpath(root_dir, cls_name)\n",
    "            if Path.is_dir(cls_folder):\n",
    "                self.class_to_imgs[cls_name] = [Path.joinpath(cls_folder, img)                                                 \n",
    "                                                 for img in Path(cls_folder).rglob('*')\n",
    "                                                 if str(img).endswith('.jpg') or str(img).endswith('.png')]        \n",
    "        self.classes = list(self.class_to_imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Define the number of episodes arbitrarily.\n",
    "        return 1000  # or any number representing episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly sample n_way classes for this episode.\n",
    "        sampled_classes = random.sample(self.classes, self.n_way)\n",
    "        support_imgs, support_labels = [], []\n",
    "        query_imgs, query_labels = [], []\n",
    "        \n",
    "        label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "        for cls_name in sampled_classes:\n",
    "            imgs = self.class_to_imgs[cls_name]\n",
    "            # Ensure there are enough examples in this class.\n",
    "            selected_imgs = random.sample(imgs, self.k_shot + self.k_query)\n",
    "            support_paths = selected_imgs[:self.k_shot]\n",
    "            query_paths = selected_imgs[self.k_shot:]\n",
    "            \n",
    "            for sp in support_paths:\n",
    "                img = Image.open(sp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_imgs.append(img)\n",
    "                support_labels.append(label_map[cls_name])\n",
    "            \n",
    "            for qp in query_paths:\n",
    "                img = Image.open(qp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_imgs.append(img)\n",
    "                query_labels.append(label_map[cls_name])\n",
    "        \n",
    "        # Convert lists to tensors.\n",
    "        support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels, dtype=torch.long)\n",
    "        query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels, dtype=torch.long)\n",
    "        \n",
    "        return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adc3d5",
   "metadata": {},
   "source": [
    "### Constructing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b699e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (should match what the encoder expects)\n",
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Root folder with classes as subfolders.\n",
    "few_shot_dataset = FewShotFERDataset(root_dir=manga_faces_train_dir, n_way=4, k_shot=10, k_query=22, transform=transform)\n",
    "few_shot_loader = DataLoader(few_shot_dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3e480",
   "metadata": {},
   "source": [
    "### Prototypical Network Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "584fdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, support_imgs, support_labels, query_imgs, query_labels, device):\n",
    "    \"\"\"\n",
    "    model: the fine-tuned FER model, used as feature extractor.\n",
    "    support_imgs: [n_way*k_shot, C, H, W]\n",
    "    query_imgs: [n_way*k_query, C, H, W]\n",
    "    support_labels: [n_way*k_shot]\n",
    "    query_labels: [n_way*k_query]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        \n",
    "        # Extract features using the pretrained encoder.\n",
    "        # Adjusted to handle both 2D (CNN) and 3D (ViT) feature outputs\n",
    "        def get_cls_features(x):\n",
    "            features = model.encoder(x)\n",
    "            # If features are 3D (ViT), extract CLS token; else, use features directly (CNN)\n",
    "            if features.dim() == 3:\n",
    "                return features[:, 0, :]\n",
    "            else:\n",
    "                return features\n",
    "        \n",
    "        support_feats = get_cls_features(support_imgs)  # shape: [n_way*k_shot, D]\n",
    "        query_feats = get_cls_features(query_imgs)      # shape: [n_way*k_query, D]\n",
    "        \n",
    "        # Compute prototypes: mean feature for each class.\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        prototypes = []\n",
    "        for cls in range(n_way):\n",
    "            cls_indices = (support_labels == cls).nonzero(as_tuple=True)[0]\n",
    "            cls_feats = support_feats[cls_indices]\n",
    "            prototype = cls_feats.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "        prototypes = torch.stack(prototypes)  # shape: [n_way, D]\n",
    "        \n",
    "        # Compute distances between each query feature and prototypes.\n",
    "        dists = torch.cdist(query_feats, prototypes, p=2)  # shape: [n_way*k_query, n_way]\n",
    "        probs = F.softmax(-dists, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        correct = (preds.cpu() == query_labels).sum().item()\n",
    "        total = query_labels.size(0)\n",
    "    \n",
    "    return correct, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71a500",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "242bc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Episode Accuracy: 32.07%\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# Evaluate on a few episodes from the DataLoader\n",
    "# ------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume model_fer is the fine-tuned FER model we defined previously.\n",
    "model_fer = model_fer.to(device)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_episodes = 50  # Evaluate on 50 episodes for instance.\n",
    "\n",
    "for i, ((support_imgs, support_labels), (query_imgs, query_labels)) in enumerate(few_shot_loader):\n",
    "    if i >= num_episodes:\n",
    "        break\n",
    "    correct, total = evaluate_episode(model_fer, support_imgs.squeeze(0), support_labels.squeeze(0),\n",
    "                                      query_imgs.squeeze(0), query_labels.squeeze(0), device)\n",
    "    total_correct += correct\n",
    "    total_samples += total\n",
    "\n",
    "episode_accuracy = 100.0 * total_correct / total_samples\n",
    "print(\"Few-Shot Episode Accuracy: {:.2f}%\".format(episode_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146cd44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4230872c",
   "metadata": {},
   "source": [
    "# Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ada878",
   "metadata": {},
   "source": [
    "### Align Label spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8d4ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"surprise\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41c71d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappedImageFolder(ImageFolder):\n",
    "    def __init__(self, root, label_map, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.samples = [\n",
    "            (path, label_map[self.classes[label]])\n",
    "            for path, label in self.samples\n",
    "            if self.classes[label] in label_map\n",
    "        ]\n",
    "        self.targets = [s[1] for s in self.samples]\n",
    "        \n",
    "        inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "        self.classes = [inverse_label_map[i] for i in sorted(inverse_label_map)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbb7fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485]\n",
    "std=[0.229]\n",
    "\n",
    "manga_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.GaussianBlur(3), T.RandomSolarize(0.5)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.4, p=0.3),\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "\n",
    "manga_faces_train_dir = Path(os.getcwd(), 'datasets', 'manga', 'train')\n",
    "manga_faces_train_images = MappedImageFolder(root=manga_faces_train_dir, label_map=label_map  , transform=manga_transforms)\n",
    "manga_faces_train_images_loader = DataLoader(manga_faces_train_images, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Test Set\n",
    "test_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "manga_faces_test_dir = Path(os.getcwd(), 'datasets', 'manga', 'test')\n",
    "manga_faces_test_images = MappedImageFolder(root=manga_faces_test_dir, label_map=label_map  , transform=test_transforms)\n",
    "manga_faces_test_images_loader = DataLoader(manga_faces_test_images, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d59b68d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "print(set(manga_faces_test_images.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fef35",
   "metadata": {},
   "source": [
    "### Contrastive Loss Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9cd8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):  # Increased temperature\n",
    "        super().__init__()\n",
    "        self.tau = temperature # hyperparameter for scaling the similarity scores\n",
    "        \n",
    "    def forward(self, source_emb, source_labels, target_emb, target_labels):\n",
    "        device = source_emb.device\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        source_emb = F.normalize(source_emb, p=2, dim=1)\n",
    "        target_emb = F.normalize(target_emb, p=2, dim=1)\n",
    "        \n",
    "        embeddings = torch.cat([source_emb, target_emb], dim=0)\n",
    "        labels = torch.cat([source_labels, target_labels], dim=0)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        sim_matrix = torch.mm(target_emb, embeddings.T) / self.tau\n",
    "        \n",
    "        # Masks\n",
    "        pos_mask = torch.zeros_like(sim_matrix, dtype=torch.bool)\n",
    "        for i, label in enumerate(target_labels):\n",
    "            pos_mask[i, :len(source_labels)] = (source_labels == label)\n",
    "            \n",
    "        neg_mask = (labels != target_labels.unsqueeze(1))\n",
    "        neg_mask[:, len(source_labels):] &= ~torch.eye(\n",
    "            len(target_labels), dtype=torch.bool, device=device\n",
    "        )\n",
    "        \n",
    "        # Compute terms with stability\n",
    "        pos_term = (sim_matrix.exp() * pos_mask.float()).sum(dim=1) + 1e-8\n",
    "        neg_term = (sim_matrix.exp() * neg_mask.float()).sum(dim=1) + 1e-8\n",
    "        \n",
    "        loss = -torch.log(pos_term / (pos_term + neg_term))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04975b3",
   "metadata": {},
   "source": [
    "### Few-shot sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db02e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot sampling function\n",
    "def get_few_shot_indices(dataset, shots_per_class=5):\n",
    "    \"\"\"\n",
    "    Returns a balanced list of indices for few-shot learning by randomly selecting\n",
    "    a fixed number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        dataset (ImageFolder): A PyTorch ImageFolder dataset (or any dataset with a `.samples` attribute \n",
    "                              containing (path, label) tuples).\n",
    "        shots_per_class (int, optional): Number of samples to select per class. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of selected indices, ensuring `shots_per_class` samples per class.\n",
    "\n",
    "    Example:\n",
    "        >>> target_set = ImageFolder(root='data/target', transform=transforms.ToTensor())\n",
    "        >>> few_shot_indices = get_few_shot_indices(target_set, shots_per_class=3)\n",
    "        >>> few_shot_loader = DataLoader(Subset(target_set, few_shot_indices), batch_size=3)\n",
    "    \"\"\"\n",
    "    \n",
    "    class_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class_indices.setdefault(label, []).append(idx)\n",
    "    \n",
    "    selected_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        selected_indices.extend(np.random.choice(indices, shots_per_class, replace=False))\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bc0771d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fer = FineTuningModel(num_classes=7, image_size=48, use_pretrained_ssl=False)\n",
    "vit_model_2 = model_fer.to(device)\n",
    "vit_model_2.load_state_dict(torch.load('weights/vit_fine_tuned_with_fer2013_NO_PRETRAIN.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d19a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and losses\n",
    "params = {'num_classes': 7, 'dropout_rate': 0.2}  # Example for FER2013\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "cont_criterion = ContrastiveLoss(temperature=0.2)\n",
    "optimizer = torch.optim.AdamW(vit_model_2.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Prepare few-shot target loader training set\n",
    "few_shot_indices = get_few_shot_indices(manga_faces_train_images, shots_per_class=15)\n",
    "few_shot_loader = DataLoader(\n",
    "    Subset(manga_faces_train_images, few_shot_indices),\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    drop_last=True  # Avoid partial batches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ea98c",
   "metadata": {},
   "source": [
    "### Modified Training Loop for Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e05540",
   "metadata": {},
   "source": [
    "#### Using both CrossEntropyLoss and ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb29e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with domain adaptation\n",
    "def train_epoch(model, source_loader, target_loader, optimizer, epoch, epochs):\n",
    "    model.train()\n",
    "    target_iter = cycle(target_loader)  # Infinite iterator\n",
    "    \n",
    "    # Initialize metrics\n",
    "    total_cls_loss = 0.0\n",
    "    total_cont_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "    source_correct = 0\n",
    "    target_correct = 0\n",
    "    total_source_samples = 0\n",
    "    total_target_samples = 0\n",
    "    \n",
    "    \n",
    "    tk = tqdm(source_loader, desc=\"EPOCH\" + \"[TRAIN]\" + str(epoch) + \"/\" + str(epochs))\n",
    "    \n",
    "    for batch_idx, (source_imgs, source_lbls) in enumerate(tk):\n",
    "        # Get target batch\n",
    "        target_imgs, target_lbls = next(target_iter)\n",
    "        \n",
    "        # Move to device\n",
    "        source_imgs = source_imgs.to(device)\n",
    "        source_lbls = source_lbls.to(device)\n",
    "        target_imgs = target_imgs.to(device)\n",
    "        target_lbls = target_lbls.to(device)\n",
    "        \n",
    "        # Forward pass with embeddings\n",
    "        source_logits, source_emb = model(source_imgs, return_embeddings=True)\n",
    "        target_logits, target_emb = model(target_imgs, return_embeddings=True)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        source_preds = source_logits.argmax(dim=1)\n",
    "        target_preds = target_logits.argmax(dim=1)\n",
    "        \n",
    "        # Update counters\n",
    "        batch_source_correct = (source_preds == source_lbls).sum().item()\n",
    "        batch_target_correct = (target_preds == target_lbls).sum().item()\n",
    "        \n",
    "        source_correct += batch_source_correct\n",
    "        target_correct += batch_target_correct\n",
    "        total_source_samples += source_lbls.size(0)\n",
    "        total_target_samples += target_lbls.size(0)\n",
    "        \n",
    "        # Loss calculation\n",
    "        cls_loss = cls_criterion(source_logits, source_lbls) + \\\n",
    "                cls_criterion(target_logits, target_lbls)\n",
    "        \n",
    "        cont_loss = cont_criterion(source_emb, source_lbls,\n",
    "                                 target_emb, target_lbls)\n",
    "        \n",
    "        current_loss = cls_loss + 0.9 * cont_loss # Adjusted weight\n",
    "        \n",
    "        # Update metrics\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_cont_loss += cont_loss.item() * 0.9 # Adjusted weight\n",
    "        running_total_loss += current_loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate batch-level accuracies\n",
    "        batch_source_acc = 100 * batch_source_correct / source_lbls.size(0)\n",
    "        batch_target_acc = 100 * batch_target_correct / target_lbls.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        tk.set_postfix({\n",
    "            'CLS Loss': f'{total_cls_loss / (batch_idx + 1):.4f}',\n",
    "            'CONT Loss': f'{total_cont_loss / (batch_idx + 1):.4f}',\n",
    "            'Total Loss': f'{running_total_loss / (batch_idx + 1):.4f}',\n",
    "            'Source Acc': f'{batch_source_acc:.2f}%',\n",
    "            'Target Acc': f'{batch_target_acc:.2f}%'\n",
    "        })\n",
    "            \n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_cls_loss = total_cls_loss / len(source_loader)\n",
    "    epoch_cont_loss = total_cont_loss / len(source_loader)\n",
    "    epoch_total_loss = running_total_loss / len(source_loader)\n",
    "    \n",
    "    epoch_source_acc = 100 * source_correct / total_source_samples\n",
    "    epoch_target_acc = 100 * target_correct / total_target_samples\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n",
    "    print(f\"CLS Loss: {epoch_cls_loss:.4f} | CONT Loss: {epoch_cont_loss:.4f} | Total Loss: {epoch_total_loss:.4f}\")\n",
    "    print(f\"Source Acc: {epoch_source_acc:.2f}% | Target Acc: {epoch_target_acc:.2f}%\")\n",
    "    \n",
    "    return epoch_cls_loss, epoch_cont_loss, epoch_total_loss, epoch_source_acc, epoch_target_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a44e57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c6f9115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/50: 100%|██████████| 898/898 [01:55<00:00,  7.79it/s, CLS Loss=2.0659, CONT Loss=1.8462, Total Loss=3.9121, Source Acc=20.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 Summary:\n",
      "CLS Loss: 2.0659 | CONT Loss: 1.8462 | Total Loss: 3.9121\n",
      "Source Acc: 24.26% | Target Acc: 94.90%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:01<00:00,  4.26it/s, loss=1.823862, acc=0.325000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/50: 100%|██████████| 898/898 [01:51<00:00,  8.05it/s, CLS Loss=2.0372, CONT Loss=1.8076, Total Loss=3.8447, Source Acc=0.00%, Target Acc=100.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 Summary:\n",
      "CLS Loss: 2.0372 | CONT Loss: 1.8076 | Total Loss: 3.8447\n",
      "Source Acc: 25.29% | Target Acc: 94.89%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 15.95it/s, loss=1.818009, acc=0.287500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/50: 100%|██████████| 898/898 [01:45<00:00,  8.51it/s, CLS Loss=2.0547, CONT Loss=1.7252, Total Loss=3.7799, Source Acc=0.00%, Target Acc=100.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 Summary:\n",
      "CLS Loss: 2.0547 | CONT Loss: 1.7252 | Total Loss: 3.7799\n",
      "Source Acc: 25.64% | Target Acc: 93.57%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.46it/s, loss=1.925983, acc=0.337500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/50: 100%|██████████| 898/898 [01:46<00:00,  8.41it/s, CLS Loss=2.0035, CONT Loss=1.7638, Total Loss=3.7673, Source Acc=20.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 Summary:\n",
      "CLS Loss: 2.0035 | CONT Loss: 1.7638 | Total Loss: 3.7673\n",
      "Source Acc: 25.84% | Target Acc: 95.50%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.63it/s, loss=2.471120, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/50: 100%|██████████| 898/898 [01:47<00:00,  8.36it/s, CLS Loss=2.0278, CONT Loss=1.7559, Total Loss=3.7837, Source Acc=20.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 Summary:\n",
      "CLS Loss: 2.0278 | CONT Loss: 1.7559 | Total Loss: 3.7837\n",
      "Source Acc: 26.16% | Target Acc: 94.51%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 15.50it/s, loss=2.329355, acc=0.450000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/50: 100%|██████████| 898/898 [01:45<00:00,  8.48it/s, CLS Loss=2.0083, CONT Loss=1.7901, Total Loss=3.7984, Source Acc=40.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 Summary:\n",
      "CLS Loss: 2.0083 | CONT Loss: 1.7901 | Total Loss: 3.7984\n",
      "Source Acc: 26.27% | Target Acc: 94.88%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.64it/s, loss=2.998486, acc=0.343750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/50: 100%|██████████| 898/898 [01:50<00:00,  8.11it/s, CLS Loss=2.1156, CONT Loss=1.6480, Total Loss=3.7636, Source Acc=60.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 Summary:\n",
      "CLS Loss: 2.1156 | CONT Loss: 1.6480 | Total Loss: 3.7636\n",
      "Source Acc: 25.79% | Target Acc: 90.82%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.98it/s, loss=3.205936, acc=0.281250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/50: 100%|██████████| 898/898 [02:03<00:00,  7.26it/s, CLS Loss=1.9773, CONT Loss=1.6692, Total Loss=3.6465, Source Acc=0.00%, Target Acc=100.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 Summary:\n",
      "CLS Loss: 1.9773 | CONT Loss: 1.6692 | Total Loss: 3.6465\n",
      "Source Acc: 26.38% | Target Acc: 96.20%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 18.43it/s, loss=2.615782, acc=0.231250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/50: 100%|██████████| 898/898 [02:02<00:00,  7.31it/s, CLS Loss=2.0217, CONT Loss=1.7233, Total Loss=3.7450, Source Acc=0.00%, Target Acc=100.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 Summary:\n",
      "CLS Loss: 2.0217 | CONT Loss: 1.7233 | Total Loss: 3.7450\n",
      "Source Acc: 25.88% | Target Acc: 95.11%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.76it/s, loss=2.005509, acc=0.287500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 1 epoch(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/50: 100%|██████████| 898/898 [02:02<00:00,  7.33it/s, CLS Loss=1.9849, CONT Loss=1.7393, Total Loss=3.7242, Source Acc=40.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 Summary:\n",
      "CLS Loss: 1.9849 | CONT Loss: 1.7393 | Total Loss: 3.7242\n",
      "Source Acc: 26.63% | Target Acc: 96.24%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.82it/s, loss=1.679839, acc=0.400000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 2 epoch(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]11/50: 100%|██████████| 898/898 [02:02<00:00,  7.36it/s, CLS Loss=2.0353, CONT Loss=1.8179, Total Loss=3.8531, Source Acc=20.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 Summary:\n",
      "CLS Loss: 2.0353 | CONT Loss: 1.8179 | Total Loss: 3.8531\n",
      "Source Acc: 26.42% | Target Acc: 94.71%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 20.73it/s, loss=1.940639, acc=0.287500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 3 epoch(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]12/50: 100%|██████████| 898/898 [02:00<00:00,  7.47it/s, CLS Loss=2.0468, CONT Loss=1.7537, Total Loss=3.8005, Source Acc=20.00%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 Summary:\n",
      "CLS Loss: 2.0468 | CONT Loss: 1.7537 | Total Loss: 3.8005\n",
      "Source Acc: 25.60% | Target Acc: 94.18%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:01<00:00,  3.49it/s, loss=2.031717, acc=0.387500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 4 epoch(s).\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]13/50: 100%|██████████| 898/898 [01:47<00:00,  8.35it/s, CLS Loss=2.0326, CONT Loss=1.8070, Total Loss=3.8396, Source Acc=0.00%, Target Acc=100.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 Summary:\n",
      "CLS Loss: 2.0326 | CONT Loss: 1.8070 | Total Loss: 3.8396\n",
      "Source Acc: 25.99% | Target Acc: 95.31%\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/1: 100%|██████████| 5/5 [00:00<00:00, 15.44it/s, loss=2.056227, acc=0.337500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No improvement in validation loss for 5 epoch(s).\n",
      "Patience exceeded. Early stopping at epoch 14\n",
      "\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contrastive_loss_metrics = {\n",
    "    'cls_loss': [],\n",
    "    'cont_loss': [],\n",
    "    'total_loss': [],\n",
    "    'source_accuracy': [],\n",
    "    'target_accuracy': []\n",
    "}\n",
    "\n",
    "validation_loss_accuracy = {\n",
    "    'validation_loss': [],\n",
    "    'validation_accuracy': []\n",
    "}\n",
    "\n",
    "best_valid_loss = np.inf\n",
    "patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "early_stop = False # Flag to indicate whether to stop training\n",
    "save_weights_patience = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n",
    "        \n",
    "    cls_loss, cont_loss, total_loss, source_acc, target_acc = \\\n",
    "        train_epoch(vit_model_2, train_loader, few_shot_loader, optimizer, epoch, EPOCHS)\n",
    "    \n",
    "    contrastive_loss_metrics['cls_loss'].append(cls_loss)\n",
    "    contrastive_loss_metrics['cont_loss'].append(cont_loss)\n",
    "    contrastive_loss_metrics['total_loss'].append(total_loss)\n",
    "    contrastive_loss_metrics['source_accuracy'].append(source_acc)\n",
    "    contrastive_loss_metrics['target_accuracy'].append(target_acc)\n",
    "    \n",
    "    print()\n",
    "    current_val_loss_accuracy = test_out_of_distribution(vit_model_2, manga_faces_test_images_loader, epochs=1, device=device)\n",
    "    validation_loss_accuracy['validation_loss'].append(float(current_val_loss_accuracy['validation_loss'][0]))\n",
    "    validation_loss_accuracy['validation_accuracy'].append(float(current_val_loss_accuracy['validation_accuracy'][0]))\n",
    "    \n",
    "    \n",
    "    if total_loss < best_valid_loss:\n",
    "        torch.save(vit_model_2.state_dict(), 'weights/vit_model_contrastive_learning_weights.pt')\n",
    "        print(\"SAVED-BEST-WEIGHTS!\")\n",
    "        best_valid_loss = total_loss\n",
    "        patience_counter = 0 # Reset early stopping\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "    if patience_counter >= save_weights_patience:\n",
    "        print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "        early_stop = True\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f81014",
   "metadata": {},
   "source": [
    "#### Store Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b0c56f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Store the metrics from when the model was tested on the out-of-distribution dataset\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(contrastive_loss_metrics['cls_loss']) + 1)),\n",
    "    \"CLS_LOSS\": contrastive_loss_metrics['cls_loss'],\n",
    "    \"CONT_LOSS\": contrastive_loss_metrics['cont_loss'],\n",
    "    \"Total Loss\": contrastive_loss_metrics['total_loss'],\n",
    "    \"Source Accuracy\": contrastive_loss_metrics['source_accuracy'],\n",
    "    \"Target Accuracy\": contrastive_loss_metrics['target_accuracy']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/vit_model_contrastive_learning_stats_training.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97bf1b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Store the metrics from when the model was tested on the out-of-distribution dataset\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(validation_loss_accuracy['validation_loss']) + 1)),\n",
    "    \"Validation Loss\": validation_loss_accuracy['validation_loss'],\n",
    "    \"Validation Accuracy\": validation_loss_accuracy['validation_accuracy']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/vit_model_contrastive_learning_TESTINGSET_stats_training.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dbae0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/10: 100%|██████████| 5/5 [00:10<00:00,  2.10s/it, loss=2.126303, acc=0.293750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/10: 100%|██████████| 5/5 [00:00<00:00,  7.42it/s, loss=2.066677, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/10: 100%|██████████| 5/5 [00:00<00:00,  8.76it/s, loss=1.975406, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/10: 100%|██████████| 5/5 [00:00<00:00,  9.22it/s, loss=1.982801, acc=0.337500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/10: 100%|██████████| 5/5 [00:00<00:00,  8.24it/s, loss=2.290566, acc=0.337500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]6/10: 100%|██████████| 5/5 [00:00<00:00,  8.16it/s, loss=2.092210, acc=0.425000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]7/10: 100%|██████████| 5/5 [00:00<00:00,  9.18it/s, loss=2.093665, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]8/10: 100%|██████████| 5/5 [00:00<00:00,  8.82it/s, loss=2.157536, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]9/10: 100%|██████████| 5/5 [00:00<00:00,  8.35it/s, loss=1.971365, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]10/10: 100%|██████████| 5/5 [00:00<00:00,  9.97it/s, loss=2.018247, acc=0.381250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tEST the model:\n",
    "cont_test_out_of_distribution_metrics = test_out_of_distribution(vit_model_2, manga_faces_test_images_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "949a8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, target=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            imgs = imgs.to(device)\n",
    "            lbls = lbls.to(device)\n",
    "            \n",
    "            logits, _ = model(imgs, return_embeddings=True)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(lbls.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    class_report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "    \n",
    "    print(f\"{'Target' if target else 'Source'} Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e54ae",
   "metadata": {},
   "source": [
    "#### Test on Source Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c91c23e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 225/225 [00:44<00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Test Accuracy: 28.89%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.02      0.04       958\n",
      "           1       0.00      0.00      0.00       111\n",
      "           2       0.20      0.12      0.15      1024\n",
      "           3       0.30      0.73      0.42      1774\n",
      "           4       0.28      0.12      0.17      1233\n",
      "           5       0.25      0.22      0.23      1247\n",
      "           6       0.42      0.25      0.32       831\n",
      "\n",
      "    accuracy                           0.29      7178\n",
      "   macro avg       0.23      0.21      0.19      7178\n",
      "weighted avg       0.27      0.29      0.24      7178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_test_accuracy, source_report = evaluate_model(vit_model_2, val_loader)\n",
    "# Contrastive learning model does not forget the source domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f221708",
   "metadata": {},
   "source": [
    "#### Test on Target Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27bfbfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Test Accuracy: 35.61%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.10      0.12        21\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.49      0.73      0.59        49\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00        22\n",
      "           6       0.45      0.23      0.30        40\n",
      "\n",
      "    accuracy                           0.36       132\n",
      "   macro avg       0.19      0.18      0.17       132\n",
      "weighted avg       0.35      0.36      0.33       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_test_accuracy, target_report = evaluate_model(vit_model_2, manga_faces_test_images_loader, target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a372f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
