{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73db0601",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a374da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69d7f8",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d415bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070573d",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5689ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# train_transforms = T.Compose([\n",
    "#     T.Resize((48, 48)),\n",
    "#     T.ToTensor(),\n",
    "#     T.RandomHorizontalFlip(p=0.5),\n",
    "#     T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     T.Normalize(mean, std),\n",
    "#     T.RandomAffine(degrees=0, shear=0.2, scale=(0.8, 1.2))\n",
    "# ])\n",
    "\n",
    "mean = [0.485]  # Single channel\n",
    "std = [0.229]\n",
    "\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f49a1",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df5c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fer_2013_dir = Path(os.getcwd(), 'datasets', 'fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97db57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = ImageFolder(root=fer_2013_dir / 'train', transform=train_transforms)\n",
    "training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_set = ImageFolder(root=fer_2013_dir / 'test', transform=val_transforms)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40020c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 28709 images\n",
      "Testing set: 7178 images\n",
      "One image batch shape : torch.Size([128, 3, 48, 48])\n",
      "One label batch shape : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Print shape of training and testing images\n",
    "print(f\"Training set: {len(training_set)} images\")\n",
    "print(f\"Testing set: {len(test_set)} images\")\n",
    "\n",
    "for images, labels in training_loader:\n",
    "  break\n",
    "\n",
    "print(f\"One image batch shape : {images.shape}\")\n",
    "print(f\"One label batch shape : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17391128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(training_set.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08b7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', \n",
    "    4: 'neutral', 5: 'sad', 6: 'surprise'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867fc1",
   "metadata": {},
   "source": [
    "#### Show sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc0b98a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch\n",
    "images, labels = next(iter(training_loader))\n",
    "\n",
    "# Unnormalize a few images from the batch\n",
    "def unnormalize(img):\n",
    "    # Use values matching your actual normalization\n",
    "    mean = torch.tensor([0.485, 0.485, 0.485]).to(img.device)\n",
    "    std = torch.tensor([0.229, 0.229, 0.229]).to(img.device)\n",
    "    return img * std.view(1, 3, 1, 1) + mean.view(1, 3, 1, 1)\n",
    "\n",
    "# Display with proper channel handling\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    img = unnormalize(images[i].unsqueeze(0)).squeeze(0)\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    axes[i].imshow(img, cmap='gray')  # Force grayscale display\n",
    "    axes[i].set_title(f\"Label: {label_map[int(labels[i])]}\")\n",
    "    axes[i].axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253eda5",
   "metadata": {},
   "source": [
    "# ResNet50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41261683",
   "metadata": {},
   "source": [
    "### Define Class & Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2f5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={          \n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_classes\": 7,\n",
    "}\n",
    "\n",
    "resnet50_fer_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet50_fer_model.fc = nn.Sequential(\n",
    "    nn.Dropout(params[\"dropout_rate\"]),\n",
    "    nn.Linear(resnet50_fer_model.fc.in_features, params[\"num_classes\"])\n",
    ")\n",
    "resnet50_fer_model = resnet50_fer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21b95d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [128, 7]                  --\n",
      "├─Conv2d: 1-1                            [128, 64, 24, 24]         9,408\n",
      "├─BatchNorm2d: 1-2                       [128, 64, 24, 24]         128\n",
      "├─ReLU: 1-3                              [128, 64, 24, 24]         --\n",
      "├─MaxPool2d: 1-4                         [128, 64, 12, 12]         --\n",
      "├─Sequential: 1-5                        [128, 256, 12, 12]        --\n",
      "│    └─Bottleneck: 2-1                   [128, 256, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 12, 12]         4,096\n",
      "│    │    └─BatchNorm2d: 3-2             [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-3                    [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-4                  [128, 64, 12, 12]         36,864\n",
      "│    │    └─BatchNorm2d: 3-5             [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-6                    [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-7                  [128, 256, 12, 12]        16,384\n",
      "│    │    └─BatchNorm2d: 3-8             [128, 256, 12, 12]        512\n",
      "│    │    └─Sequential: 3-9              [128, 256, 12, 12]        16,896\n",
      "│    │    └─ReLU: 3-10                   [128, 256, 12, 12]        --\n",
      "│    └─Bottleneck: 2-2                   [128, 256, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-11                 [128, 64, 12, 12]         16,384\n",
      "│    │    └─BatchNorm2d: 3-12            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-13                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-14                 [128, 64, 12, 12]         36,864\n",
      "│    │    └─BatchNorm2d: 3-15            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-16                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-17                 [128, 256, 12, 12]        16,384\n",
      "│    │    └─BatchNorm2d: 3-18            [128, 256, 12, 12]        512\n",
      "│    │    └─ReLU: 3-19                   [128, 256, 12, 12]        --\n",
      "│    └─Bottleneck: 2-3                   [128, 256, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-20                 [128, 64, 12, 12]         16,384\n",
      "│    │    └─BatchNorm2d: 3-21            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-22                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-23                 [128, 64, 12, 12]         36,864\n",
      "│    │    └─BatchNorm2d: 3-24            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-25                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-26                 [128, 256, 12, 12]        16,384\n",
      "│    │    └─BatchNorm2d: 3-27            [128, 256, 12, 12]        512\n",
      "│    │    └─ReLU: 3-28                   [128, 256, 12, 12]        --\n",
      "├─Sequential: 1-6                        [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-4                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-29                 [128, 128, 12, 12]        32,768\n",
      "│    │    └─BatchNorm2d: 3-30            [128, 128, 12, 12]        256\n",
      "│    │    └─ReLU: 3-31                   [128, 128, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-32                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-33            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-34                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-35                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-36            [128, 512, 6, 6]          1,024\n",
      "│    │    └─Sequential: 3-37             [128, 512, 6, 6]          132,096\n",
      "│    │    └─ReLU: 3-38                   [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-5                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-39                 [128, 128, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-40            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-41                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-42                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-43            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-44                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-45                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-46            [128, 512, 6, 6]          1,024\n",
      "│    │    └─ReLU: 3-47                   [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-6                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-48                 [128, 128, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-49            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-50                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-51                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-52            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-53                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-54                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-55            [128, 512, 6, 6]          1,024\n",
      "│    │    └─ReLU: 3-56                   [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-7                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-57                 [128, 128, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-58            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-59                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-60                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-61            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-62                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-63                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-64            [128, 512, 6, 6]          1,024\n",
      "│    │    └─ReLU: 3-65                   [128, 512, 6, 6]          --\n",
      "├─Sequential: 1-7                        [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-8                   [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-66                 [128, 256, 6, 6]          131,072\n",
      "│    │    └─BatchNorm2d: 3-67            [128, 256, 6, 6]          512\n",
      "│    │    └─ReLU: 3-68                   [128, 256, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-69                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-70            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-71                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-72                 [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-73            [128, 1024, 3, 3]         2,048\n",
      "│    │    └─Sequential: 3-74             [128, 1024, 3, 3]         526,336\n",
      "│    │    └─ReLU: 3-75                   [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-9                   [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-76                 [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-77            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-78                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-79                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-80            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-81                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-82                 [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-83            [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-84                   [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-10                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-85                 [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-86            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-87                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-88                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-89            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-90                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-91                 [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-92            [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-93                   [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-11                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-94                 [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-95            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-96                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-97                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-98            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-99                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-100                [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-101           [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-102                  [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-12                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-103                [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-104           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-105                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-106                [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-107           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-108                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-109                [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-110           [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-111                  [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-13                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-112                [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-113           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-114                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-115                [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-116           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-117                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-118                [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-119           [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-120                  [128, 1024, 3, 3]         --\n",
      "├─Sequential: 1-8                        [128, 2048, 2, 2]         --\n",
      "│    └─Bottleneck: 2-14                  [128, 2048, 2, 2]         --\n",
      "│    │    └─Conv2d: 3-121                [128, 512, 3, 3]          524,288\n",
      "│    │    └─BatchNorm2d: 3-122           [128, 512, 3, 3]          1,024\n",
      "│    │    └─ReLU: 3-123                  [128, 512, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-124                [128, 512, 2, 2]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-125           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-126                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-127                [128, 2048, 2, 2]         1,048,576\n",
      "│    │    └─BatchNorm2d: 3-128           [128, 2048, 2, 2]         4,096\n",
      "│    │    └─Sequential: 3-129            [128, 2048, 2, 2]         2,101,248\n",
      "│    │    └─ReLU: 3-130                  [128, 2048, 2, 2]         --\n",
      "│    └─Bottleneck: 2-15                  [128, 2048, 2, 2]         --\n",
      "│    │    └─Conv2d: 3-131                [128, 512, 2, 2]          1,048,576\n",
      "│    │    └─BatchNorm2d: 3-132           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-133                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-134                [128, 512, 2, 2]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-135           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-136                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-137                [128, 2048, 2, 2]         1,048,576\n",
      "│    │    └─BatchNorm2d: 3-138           [128, 2048, 2, 2]         4,096\n",
      "│    │    └─ReLU: 3-139                  [128, 2048, 2, 2]         --\n",
      "│    └─Bottleneck: 2-16                  [128, 2048, 2, 2]         --\n",
      "│    │    └─Conv2d: 3-140                [128, 512, 2, 2]          1,048,576\n",
      "│    │    └─BatchNorm2d: 3-141           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-142                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-143                [128, 512, 2, 2]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-144           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-145                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-146                [128, 2048, 2, 2]         1,048,576\n",
      "│    │    └─BatchNorm2d: 3-147           [128, 2048, 2, 2]         4,096\n",
      "│    │    └─ReLU: 3-148                  [128, 2048, 2, 2]         --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [128, 2048, 1, 1]         --\n",
      "├─Sequential: 1-10                       [128, 7]                  --\n",
      "│    └─Dropout: 2-17                     [128, 2048]               --\n",
      "│    └─Linear: 2-18                      [128, 7]                  14,343\n",
      "==========================================================================================\n",
      "Total params: 23,522,375\n",
      "Trainable params: 23,522,375\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 27.26\n",
      "==========================================================================================\n",
      "Input size (MB): 3.54\n",
      "Forward/backward pass size (MB): 1083.71\n",
      "Params size (MB): 94.09\n",
      "Estimated Total Size (MB): 1181.34\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(resnet50_fer_model, input_size=(BATCH_SIZE, 3, 48, 48), device=device.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8a14",
   "metadata": {},
   "source": [
    "# Create Train and Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf63633",
   "metadata": {},
   "source": [
    "### Define Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cee4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred,y_true):\n",
    "    top_p,top_class = y_pred.topk(1, dim = 1)\n",
    "    equals = top_class == y_true.view(*top_class.shape)\n",
    "    return torch.mean(equals.type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Train one epoch of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The  model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities from model\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for logging; Total loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "\n",
    "        # Print/log training loss and accuracy for this epoch\n",
    "        tk.set_postfix({\n",
    "            'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "            'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "        })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7824849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Test one epoch of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "        \n",
    "        running_acc (float): Returns epoch accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[VALID]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for t, data in enumerate(tk):          \n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute log probabilities from model\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)            \n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate loss for logging; Total loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "            \n",
    "\n",
    "            tk.set_postfix({\n",
    "                'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "                'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "            })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, training_dataloader, testing_dataloader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 3\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'training_loss': [],\n",
    "        'training_accuracy': [],\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss, train_accuracy = train_one_epoch(model, training_dataloader, criterion, optimizer, device, epoch, epochs)\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "\n",
    "        history_metrics['training_loss'].append(train_loss)\n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['training_accuracy'].append(train_accuracy)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "\n",
    "        # Update the learning rate based on validation loss and print\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), 'weights/RESNET50_model_with_fer2013_weights.pt')\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "            \n",
    "        \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1b19",
   "metadata": {},
   "source": [
    "### Run train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f103698",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e820802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/100: 100%|██████████| 225/225 [03:40<00:00,  1.02it/s, loss=1.923978, acc=0.188164]\n",
      "EPOCH[VALID]1/100: 100%|██████████| 57/57 [00:19<00:00,  2.92it/s, loss=1.886772, acc=0.220477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 2, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/100: 100%|██████████| 225/225 [01:19<00:00,  2.84it/s, loss=1.868253, acc=0.224323]\n",
      "EPOCH[VALID]2/100: 100%|██████████| 57/57 [00:04<00:00, 11.87it/s, loss=1.832163, acc=0.245422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 3, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/100: 100%|██████████| 225/225 [01:18<00:00,  2.85it/s, loss=1.830810, acc=0.246639]\n",
      "EPOCH[VALID]3/100: 100%|██████████| 57/57 [00:04<00:00, 12.99it/s, loss=1.794433, acc=0.270504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 4, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/100: 100%|██████████| 225/225 [01:19<00:00,  2.84it/s, loss=1.800389, acc=0.258688]\n",
      "EPOCH[VALID]4/100: 100%|██████████| 57/57 [00:04<00:00, 12.64it/s, loss=1.770551, acc=0.284704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 5, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/100: 100%|██████████| 225/225 [01:24<00:00,  2.65it/s, loss=1.776322, acc=0.274756]\n",
      "EPOCH[VALID]5/100: 100%|██████████| 57/57 [00:04<00:00, 12.55it/s, loss=1.745412, acc=0.296135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 6, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/100: 100%|██████████| 225/225 [01:27<00:00,  2.58it/s, loss=1.753474, acc=0.290058]\n",
      "EPOCH[VALID]6/100: 100%|██████████| 57/57 [00:04<00:00, 12.24it/s, loss=1.721222, acc=0.306579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 7, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/100: 100%|██████████| 225/225 [01:14<00:00,  3.00it/s, loss=1.734470, acc=0.302278]\n",
      "EPOCH[VALID]7/100: 100%|██████████| 57/57 [00:04<00:00, 13.52it/s, loss=1.707858, acc=0.311075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 8, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/100: 100%|██████████| 225/225 [01:12<00:00,  3.09it/s, loss=1.717121, acc=0.309305]\n",
      "EPOCH[VALID]8/100: 100%|██████████| 57/57 [00:04<00:00, 13.54it/s, loss=1.675314, acc=0.336321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 9, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/100: 100%|██████████| 225/225 [01:11<00:00,  3.14it/s, loss=1.698920, acc=0.318434]\n",
      "EPOCH[VALID]9/100: 100%|██████████| 57/57 [00:04<00:00, 13.50it/s, loss=1.655551, acc=0.351288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 10, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/100: 100%|██████████| 225/225 [01:13<00:00,  3.05it/s, loss=1.677388, acc=0.337534]\n",
      "EPOCH[VALID]10/100: 100%|██████████| 57/57 [00:04<00:00, 13.55it/s, loss=1.636642, acc=0.363130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 11, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]11/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.655947, acc=0.342619]\n",
      "EPOCH[VALID]11/100: 100%|██████████| 57/57 [00:04<00:00, 13.43it/s, loss=1.607104, acc=0.374342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 12, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]12/100: 100%|██████████| 225/225 [01:12<00:00,  3.11it/s, loss=1.635146, acc=0.357622]\n",
      "EPOCH[VALID]12/100: 100%|██████████| 57/57 [00:04<00:00, 13.37it/s, loss=1.589503, acc=0.381908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 13, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]13/100: 100%|██████████| 225/225 [01:11<00:00,  3.13it/s, loss=1.614221, acc=0.365128]\n",
      "EPOCH[VALID]13/100: 100%|██████████| 57/57 [00:04<00:00, 13.60it/s, loss=1.566680, acc=0.394901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 14, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]14/100: 100%|██████████| 225/225 [01:13<00:00,  3.05it/s, loss=1.596233, acc=0.376543]\n",
      "EPOCH[VALID]14/100: 100%|██████████| 57/57 [00:04<00:00, 13.53it/s, loss=1.557609, acc=0.399452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 15, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]15/100: 100%|██████████| 225/225 [01:11<00:00,  3.13it/s, loss=1.580886, acc=0.381449]\n",
      "EPOCH[VALID]15/100: 100%|██████████| 57/57 [00:04<00:00, 13.61it/s, loss=1.546404, acc=0.401645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 16, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]16/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.559790, acc=0.393162]\n",
      "EPOCH[VALID]16/100: 100%|██████████| 57/57 [00:04<00:00, 13.49it/s, loss=1.515764, acc=0.423602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 17, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]17/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.545215, acc=0.402203]\n",
      "EPOCH[VALID]17/100: 100%|██████████| 57/57 [00:04<00:00, 13.38it/s, loss=1.505586, acc=0.424287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 18, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]18/100: 100%|██████████| 225/225 [01:11<00:00,  3.16it/s, loss=1.529478, acc=0.408405]\n",
      "EPOCH[VALID]18/100: 100%|██████████| 57/57 [00:04<00:00, 13.73it/s, loss=1.489239, acc=0.433279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 19, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]19/100: 100%|██████████| 225/225 [01:11<00:00,  3.14it/s, loss=1.509172, acc=0.418626]\n",
      "EPOCH[VALID]19/100: 100%|██████████| 57/57 [00:04<00:00, 13.61it/s, loss=1.476228, acc=0.439748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 20, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]20/100: 100%|██████████| 225/225 [01:12<00:00,  3.11it/s, loss=1.491714, acc=0.423020]\n",
      "EPOCH[VALID]20/100: 100%|██████████| 57/57 [00:04<00:00, 13.58it/s, loss=1.462211, acc=0.438213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 21, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]21/100: 100%|██████████| 225/225 [01:12<00:00,  3.09it/s, loss=1.486654, acc=0.425328]\n",
      "EPOCH[VALID]21/100: 100%|██████████| 57/57 [00:04<00:00, 13.19it/s, loss=1.454485, acc=0.442078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 22, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]22/100: 100%|██████████| 225/225 [01:13<00:00,  3.07it/s, loss=1.468952, acc=0.434902]\n",
      "EPOCH[VALID]22/100: 100%|██████████| 57/57 [00:04<00:00, 13.64it/s, loss=1.435394, acc=0.452988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 23, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]23/100: 100%|██████████| 225/225 [01:12<00:00,  3.12it/s, loss=1.457264, acc=0.440653]\n",
      "EPOCH[VALID]23/100: 100%|██████████| 57/57 [00:04<00:00, 13.70it/s, loss=1.425801, acc=0.452577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 24, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]24/100: 100%|██████████| 225/225 [01:12<00:00,  3.09it/s, loss=1.448977, acc=0.441265]\n",
      "EPOCH[VALID]24/100: 100%|██████████| 57/57 [00:04<00:00, 13.51it/s, loss=1.416774, acc=0.454934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 25, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]25/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.438874, acc=0.445413]\n",
      "EPOCH[VALID]25/100: 100%|██████████| 57/57 [00:04<00:00, 13.36it/s, loss=1.407709, acc=0.462034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 26, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]26/100: 100%|██████████| 225/225 [01:13<00:00,  3.07it/s, loss=1.428829, acc=0.450239]\n",
      "EPOCH[VALID]26/100: 100%|██████████| 57/57 [00:04<00:00, 13.61it/s, loss=1.397609, acc=0.467626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 27, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]27/100: 100%|██████████| 225/225 [01:11<00:00,  3.13it/s, loss=1.417579, acc=0.455236]\n",
      "EPOCH[VALID]27/100: 100%|██████████| 57/57 [00:04<00:00, 13.45it/s, loss=1.395955, acc=0.465351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 28, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]28/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.404064, acc=0.462784]\n",
      "EPOCH[VALID]28/100: 100%|██████████| 57/57 [00:04<00:00, 13.50it/s, loss=1.379181, acc=0.475055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 29, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]29/100: 100%|██████████| 225/225 [01:12<00:00,  3.11it/s, loss=1.397033, acc=0.462923]\n",
      "EPOCH[VALID]29/100: 100%|██████████| 57/57 [00:04<00:00, 13.65it/s, loss=1.375033, acc=0.479715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 30, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]30/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.392934, acc=0.463661]\n",
      "EPOCH[VALID]30/100: 100%|██████████| 57/57 [00:04<00:00, 13.44it/s, loss=1.375975, acc=0.476371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 31, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]31/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.380970, acc=0.473599]\n",
      "EPOCH[VALID]31/100: 100%|██████████| 57/57 [00:04<00:00, 13.62it/s, loss=1.361915, acc=0.482209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 32, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]32/100: 100%|██████████| 225/225 [01:11<00:00,  3.13it/s, loss=1.370806, acc=0.476171]\n",
      "EPOCH[VALID]32/100: 100%|██████████| 57/57 [00:04<00:00, 13.56it/s, loss=1.350625, acc=0.486568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 33, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]33/100: 100%|██████████| 225/225 [01:12<00:00,  3.09it/s, loss=1.366600, acc=0.473137]\n",
      "EPOCH[VALID]33/100: 100%|██████████| 57/57 [00:04<00:00, 13.45it/s, loss=1.350597, acc=0.486486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 34, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]34/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.355062, acc=0.481262]\n",
      "EPOCH[VALID]34/100: 100%|██████████| 57/57 [00:04<00:00, 13.45it/s, loss=1.337584, acc=0.488925]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 35, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]35/100: 100%|██████████| 225/225 [01:12<00:00,  3.09it/s, loss=1.347265, acc=0.484283]\n",
      "EPOCH[VALID]35/100: 100%|██████████| 57/57 [00:04<00:00, 13.55it/s, loss=1.343343, acc=0.487034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 36, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]36/100: 100%|██████████| 225/225 [01:12<00:00,  3.12it/s, loss=1.342768, acc=0.487752]\n",
      "EPOCH[VALID]36/100: 100%|██████████| 57/57 [00:04<00:00, 13.35it/s, loss=1.325271, acc=0.491118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 37, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]37/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.337728, acc=0.490859]\n",
      "EPOCH[VALID]37/100: 100%|██████████| 57/57 [00:04<00:00, 13.38it/s, loss=1.317374, acc=0.495203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 38, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]38/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.328122, acc=0.488298]\n",
      "EPOCH[VALID]38/100: 100%|██████████| 57/57 [00:04<00:00, 13.44it/s, loss=1.319942, acc=0.498109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 39, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]39/100: 100%|██████████| 225/225 [01:13<00:00,  3.07it/s, loss=1.322432, acc=0.493848]\n",
      "EPOCH[VALID]39/100: 100%|██████████| 57/57 [00:04<00:00, 13.27it/s, loss=1.300742, acc=0.501590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 40, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]40/100: 100%|██████████| 225/225 [01:12<00:00,  3.09it/s, loss=1.314999, acc=0.497841]\n",
      "EPOCH[VALID]40/100: 100%|██████████| 57/57 [00:04<00:00, 13.42it/s, loss=1.295347, acc=0.506414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 41, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]41/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.303019, acc=0.501999]\n",
      "EPOCH[VALID]41/100: 100%|██████████| 57/57 [00:04<00:00, 13.52it/s, loss=1.285952, acc=0.515104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 42, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]42/100: 100%|██████████| 225/225 [01:13<00:00,  3.08it/s, loss=1.305789, acc=0.499993]\n",
      "EPOCH[VALID]42/100: 100%|██████████| 57/57 [00:04<00:00, 13.35it/s, loss=1.289204, acc=0.507812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 43, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]43/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.302999, acc=0.503490]\n",
      "EPOCH[VALID]43/100: 100%|██████████| 57/57 [00:04<00:00, 13.52it/s, loss=1.287339, acc=0.511650]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 44, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]44/100: 100%|██████████| 225/225 [01:12<00:00,  3.11it/s, loss=1.294224, acc=0.503757]\n",
      "EPOCH[VALID]44/100: 100%|██████████| 57/57 [00:04<00:00, 13.52it/s, loss=1.281681, acc=0.512610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 45, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]45/100: 100%|██████████| 225/225 [01:14<00:00,  3.04it/s, loss=1.287481, acc=0.507077]\n",
      "EPOCH[VALID]45/100: 100%|██████████| 57/57 [00:04<00:00, 13.29it/s, loss=1.272552, acc=0.517133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 46, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]46/100: 100%|██████████| 225/225 [01:12<00:00,  3.10it/s, loss=1.281846, acc=0.509916]\n",
      "EPOCH[VALID]46/100: 100%|██████████| 57/57 [00:04<00:00, 13.40it/s, loss=1.272060, acc=0.515899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 47, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]47/100: 100%|██████████| 225/225 [01:13<00:00,  3.06it/s, loss=1.270116, acc=0.514256]\n",
      "EPOCH[VALID]47/100: 100%|██████████| 57/57 [00:04<00:00, 12.42it/s, loss=1.267634, acc=0.513761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 48, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]48/100: 100%|██████████| 225/225 [01:12<00:00,  3.11it/s, loss=1.270972, acc=0.514713]\n",
      "EPOCH[VALID]48/100: 100%|██████████| 57/57 [00:04<00:00, 12.99it/s, loss=1.255251, acc=0.524041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 49, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]49/100: 100%|██████████| 225/225 [01:13<00:00,  3.04it/s, loss=1.266361, acc=0.519309]\n",
      "EPOCH[VALID]49/100: 100%|██████████| 57/57 [00:04<00:00, 13.42it/s, loss=1.256858, acc=0.521382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 50, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]50/100: 100%|██████████| 225/225 [01:13<00:00,  3.07it/s, loss=1.257823, acc=0.515231]\n",
      "EPOCH[VALID]50/100: 100%|██████████| 57/57 [00:04<00:00, 13.51it/s, loss=1.260105, acc=0.517873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 51, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]51/100: 100%|██████████| 225/225 [01:12<00:00,  3.11it/s, loss=1.256725, acc=0.520984]\n",
      "EPOCH[VALID]51/100: 100%|██████████| 57/57 [00:04<00:00, 13.40it/s, loss=1.257031, acc=0.520779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 3 epoch(s).\n",
      "Patience exceeded. Early stopping at epoch 51\n",
      "Early stopping triggered. Stopping training.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "resnet_model_losses = train_and_validate_model(resnet50_fer_model, training_loader, test_loader, epochs=100, learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a7c3b",
   "metadata": {},
   "source": [
    "> TRained for about 50 + 14 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d7e622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  1:09:02.470153\n"
     ]
    }
   ],
   "source": [
    "time2 = datetime.now()\n",
    "print(\"Training time: \", time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6722ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(resnet_model_losses['training_loss']) + 1)),\n",
    "    \"Training Loss\": resnet_model_losses['training_loss'],\n",
    "    \"Validation Loss\": resnet_model_losses['validation_loss'],\n",
    "    \"Training Accuracy\": [acc.cpu().item() for acc in resnet_model_losses['training_accuracy']],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in resnet_model_losses['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/resnet50_model_stats_001_100 epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e318b5b5",
   "metadata": {},
   "source": [
    "# Test Model Accuracy on Out of Distribution Data set (Manga Faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d39f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out_of_distribution(model, testing_dataloader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "        \n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "                \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2450a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_fer_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet50_fer_model.fc = nn.Sequential(\n",
    "    nn.Dropout(params[\"dropout_rate\"]),\n",
    "    nn.Linear(resnet50_fer_model.fc.in_features, params[\"num_classes\"])\n",
    ")\n",
    "resnet50_fer_model = resnet50_fer_model.to(device)\n",
    "resnet50_fer_model.load_state_dict(torch.load('weights/RESNET50_model_with_fer2013_weights.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47282a0",
   "metadata": {},
   "source": [
    "## Import MangaFaces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5acd7e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "manga_faces_train_dir = Path(os.getcwd(), 'datasets', 'manga', 'train')\n",
    "manga_faces_train_images = ImageFolder(root=manga_faces_train_dir, transform=train_transforms)\n",
    "manga_faces_train_images_loader = DataLoader(manga_faces_train_images, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test Set\n",
    "manga_faces_test_dir = Path(os.getcwd(), 'datasets', 'manga', 'test')\n",
    "manga_faces_test_images = ImageFolder(root=manga_faces_test_dir, transform=val_transforms)\n",
    "manga_faces_test_images_loader = DataLoader(manga_faces_test_images, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d105e",
   "metadata": {},
   "source": [
    "**Changes made to the data set folder**\n",
    "> Changed\n",
    "\n",
    " {'angry': 0, 'happy': 3, 'sad': 5,\n",
    " \n",
    " \n",
    " 'disgust': 1, 'fear': 2,  'neutral': 4,  'surprise': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c0ca92",
   "metadata": {},
   "source": [
    "## Run 'test_out_of_distribution' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680836f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/5: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s, loss=2.972042, acc=0.107686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/5: 100%|██████████| 2/2 [00:00<00:00,  3.76it/s, loss=3.046495, acc=0.108742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/5: 100%|██████████| 2/2 [00:00<00:00,  3.79it/s, loss=3.111988, acc=0.073902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/5: 100%|██████████| 2/2 [00:00<00:00,  3.55it/s, loss=3.088979, acc=0.059333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/5: 100%|██████████| 2/2 [00:00<00:00,  3.32it/s, loss=3.011518, acc=0.096284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_metrics = test_out_of_distribution(resnet50_fer_model, manga_faces_train_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e37b5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Store the metrics from when the model was tested on the out-of-distribution dataset\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(test_out_of_distribution_metrics['validation_loss']) + 1)),\n",
    "    \"Validation Loss\": test_out_of_distribution_metrics['validation_loss'],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in test_out_of_distribution_metrics['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/RESNET_test_model_out_of_distribution_stats_001_5_epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddb99760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/5: 100%|██████████| 2/2 [00:00<00:00,  4.12it/s, loss=2.714801, acc=0.050781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/5: 100%|██████████| 2/2 [00:00<00:00, 10.26it/s, loss=2.614193, acc=0.171875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/5: 100%|██████████| 2/2 [00:00<00:00, 10.01it/s, loss=3.192715, acc=0.050781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/5: 100%|██████████| 2/2 [00:00<00:00,  9.94it/s, loss=3.057675, acc=0.050781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/5: 100%|██████████| 2/2 [00:00<00:00, 10.20it/s, loss=3.485487, acc=0.050781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_metrics = test_out_of_distribution(resnet50_fer_model, manga_faces_test_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c183a06",
   "metadata": {},
   "source": [
    "# FSL DA Prototypical Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40840d",
   "metadata": {},
   "source": [
    "### ResNet50 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6559bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50FeatureExtractor(nn.Module):\n",
    "    def __init__(self, base_model, embedding_dim=512):\n",
    "        super(ResNet50FeatureExtractor, self).__init__()\n",
    "        # Use the ResNet50 layers up to average pool.\n",
    "        # list(base_model.children())[:-1] ignores the last fc layer.\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        # Optionally, add a projection layer to reduce dimensionality.\n",
    "        # ResNet50 outputs feature vectors of size 2048 after avgpool.\n",
    "        self.projection = nn.Linear(2048, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)             # shape: [B, 2048, 1, 1]\n",
    "        x = x.flatten(1)                 # shape: [B, 2048]\n",
    "        embeddings = self.projection(x)  # shape: [B, embedding_dim]\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56ef4039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_fer_model_2 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet50_fer_model_2.fc = nn.Sequential(\n",
    "    nn.Dropout(params[\"dropout_rate\"]),\n",
    "    nn.Linear(resnet50_fer_model_2.fc.in_features, params[\"num_classes\"])\n",
    ")\n",
    "resnet50_fer_model_2 = resnet50_fer_model_2.to(device)\n",
    "resnet50_fer_model_2.load_state_dict(torch.load('weights/RESNET50_model_with_fer2013_weights.pt', weights_only=True))\n",
    "#resnet50_fer_model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc2787f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming resnet50_fer_model is already defined and on device:\n",
    "feature_extractor = ResNet50FeatureExtractor(resnet50_fer_model_2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556c60f",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15233c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotFERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for few-shot FER, where images are organized by class in folders.\n",
    "    This dataset generates episodes (tasks) on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, n_way=5, k_shot=1, k_query=5, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Root folder containing one folder per class.\n",
    "        n_way: number of classes per episode.\n",
    "        k_shot: number of support examples per class.\n",
    "        k_query: number of query examples per class.\n",
    "        transform: transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Build a mapping: class -> list of image paths.\n",
    "        self.class_to_imgs = {}\n",
    "        for cls_name in os.listdir(root_dir):\n",
    "            cls_folder = Path.joinpath(root_dir, cls_name)\n",
    "            if Path.is_dir(cls_folder):\n",
    "                self.class_to_imgs[cls_name] = [Path.joinpath(cls_folder, img)                                                 \n",
    "                                                 for img in Path(cls_folder).rglob('*')\n",
    "                                                 if str(img).endswith('.jpg') or str(img).endswith('.png')]        \n",
    "        self.classes = list(self.class_to_imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Define the number of episodes arbitrarily.\n",
    "        return 1000  # or any number representing episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly sample n_way classes for this episode.\n",
    "        sampled_classes = random.sample(self.classes, self.n_way)\n",
    "        support_imgs, support_labels = [], []\n",
    "        query_imgs, query_labels = [], []\n",
    "        \n",
    "        label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "        for cls_name in sampled_classes:\n",
    "            imgs = self.class_to_imgs[cls_name]\n",
    "            # Ensure there are enough examples in this class.\n",
    "            selected_imgs = random.sample(imgs, self.k_shot + self.k_query)\n",
    "            support_paths = selected_imgs[:self.k_shot]\n",
    "            query_paths = selected_imgs[self.k_shot:]\n",
    "            \n",
    "            for sp in support_paths:\n",
    "                img = Image.open(sp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_imgs.append(img)\n",
    "                support_labels.append(label_map[cls_name])\n",
    "            \n",
    "            for qp in query_paths:\n",
    "                img = Image.open(qp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_imgs.append(img)\n",
    "                query_labels.append(label_map[cls_name])\n",
    "        \n",
    "        # Convert lists to tensors.\n",
    "        support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels, dtype=torch.long)\n",
    "        query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels, dtype=torch.long)\n",
    "        \n",
    "        return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adc3d5",
   "metadata": {},
   "source": [
    "### Instantiate Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b699e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485],\n",
    "                std=[0.229])\n",
    "])\n",
    "\n",
    "# Root folder with classes as subfolders.\n",
    "few_shot_dataset = FewShotFERDataset(root_dir=manga_faces_train_dir, n_way=4, k_shot=10, k_query=22, transform=transform)\n",
    "few_shot_loader = DataLoader(few_shot_dataset, batch_size=1, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3e480",
   "metadata": {},
   "source": [
    "### Prototypical Network Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "584fdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(feature_extractor, support_imgs, support_labels, query_imgs, query_labels, device):\n",
    "    \"\"\"\n",
    "    feature_extractor: model that outputs embeddings [B, embedding_dim]\n",
    "    support_imgs: tensor of shape [n_way * k_shot, C, H, W]\n",
    "    support_labels: tensor of shape [n_way * k_shot]\n",
    "    query_imgs: tensor of shape [n_way * k_query, C, H, W]\n",
    "    query_labels: tensor of shape [n_way * k_query]\n",
    "    \"\"\"\n",
    "    feature_extractor.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move data to device.\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        \n",
    "        # Compute the embeddings.\n",
    "        support_embeddings = feature_extractor(support_imgs)  # [n_way*k_shot, D]\n",
    "        query_embeddings = feature_extractor(query_imgs)      # [n_way*k_query, D]\n",
    "    \n",
    "        # Compute prototypes: mean of support embeddings per class.\n",
    "        prototypes = []\n",
    "        unique_labels = torch.unique(support_labels)\n",
    "        for cls in unique_labels:\n",
    "            cls_indices = (support_labels == cls).nonzero(as_tuple=True)[0]\n",
    "            cls_embeddings = support_embeddings[cls_indices]\n",
    "            prototype = cls_embeddings.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "        prototypes = torch.stack(prototypes)  # shape: [n_way, D]\n",
    "        \n",
    "        # Compute distances between query embeddings and prototypes.\n",
    "        # We use Euclidean distance here.\n",
    "        # query_embeddings: [Q, D], prototypes: [n_way, D]\n",
    "        distances = torch.cdist(query_embeddings, prototypes, p=2)  # shape: [Q, n_way]\n",
    "        \n",
    "        # Convert distances to probabilities (smaller distance -> higher probability).\n",
    "        probs = F.softmax(-distances, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        correct = (preds.cpu() == query_labels).sum().item()\n",
    "        total = query_labels.size(0)\n",
    "    \n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71a500",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "242bc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Episode Accuracy: 41.30%\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_episodes = 50  # Evaluate on 50 episodes.\n",
    "\n",
    "for i, episode in enumerate(few_shot_loader):\n",
    "    if i >= num_episodes:\n",
    "        break\n",
    "    # Remove the extra batch dimension since batch_size=1.\n",
    "    (support_imgs, support_labels), (query_imgs, query_labels) = episode\n",
    "    support_imgs = support_imgs.squeeze(0)\n",
    "    support_labels = support_labels.squeeze(0)\n",
    "    query_imgs = query_imgs.squeeze(0)\n",
    "    query_labels = query_labels.squeeze(0)\n",
    "    \n",
    "    correct, total = evaluate_episode(feature_extractor, support_imgs, support_labels, query_imgs, query_labels, device)\n",
    "    total_correct += correct\n",
    "    total_samples += total\n",
    "\n",
    "episode_accuracy = 100.0 * total_correct / total_samples\n",
    "print(\"Few-Shot Episode Accuracy: {:.2f}%\".format(episode_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff10fa",
   "metadata": {},
   "source": [
    "# Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc67cb5",
   "metadata": {},
   "source": [
    "### Align Label spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "455653d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"surprise\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c14473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappedImageFolder(ImageFolder):\n",
    "    def __init__(self, root, label_map, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.samples = [\n",
    "            (path, label_map[self.classes[label]])\n",
    "            for path, label in self.samples\n",
    "            if self.classes[label] in label_map\n",
    "        ]\n",
    "        self.targets = [s[1] for s in self.samples]\n",
    "        \n",
    "        inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "        self.classes = [inverse_label_map[i] for i in sorted(inverse_label_map)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ff5a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485]\n",
    "std=[0.229]\n",
    "\n",
    "manga_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.GaussianBlur(3), T.RandomSolarize(0.5)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.4, p=0.3),\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "\n",
    "manga_faces_train_dir = Path(os.getcwd(), 'datasets', 'manga', 'train')\n",
    "manga_faces_train_images = MappedImageFolder(root=manga_faces_train_dir, label_map=label_map  , transform=manga_transforms)\n",
    "manga_faces_train_images_loader = DataLoader(manga_faces_train_images, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Test Set\n",
    "test_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "manga_faces_test_dir = Path(os.getcwd(), 'datasets', 'manga', 'test')\n",
    "manga_faces_test_images = MappedImageFolder(root=manga_faces_test_dir, label_map=label_map  , transform=test_transforms)\n",
    "manga_faces_test_images_loader = DataLoader(manga_faces_test_images, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2330e4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "print(set(manga_faces_test_images.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9c9cc8",
   "metadata": {},
   "source": [
    "### Contrastive Loss Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0046fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):  # Increased temperature\n",
    "        super().__init__()\n",
    "        self.tau = temperature # hyperparameter for scaling the similarity scores\n",
    "        \n",
    "    def forward(self, source_emb, source_labels, target_emb, target_labels):\n",
    "        device = source_emb.device\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        source_emb = F.normalize(source_emb, p=2, dim=1)\n",
    "        target_emb = F.normalize(target_emb, p=2, dim=1)\n",
    "        \n",
    "        embeddings = torch.cat([source_emb, target_emb], dim=0)\n",
    "        labels = torch.cat([source_labels, target_labels], dim=0)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        sim_matrix = torch.mm(target_emb, embeddings.T) / self.tau\n",
    "        \n",
    "        # Masks\n",
    "        pos_mask = torch.zeros_like(sim_matrix, dtype=torch.bool)\n",
    "        for i, label in enumerate(target_labels):\n",
    "            pos_mask[i, :len(source_labels)] = (source_labels == label)\n",
    "            \n",
    "        neg_mask = (labels != target_labels.unsqueeze(1))\n",
    "        neg_mask[:, len(source_labels):] &= ~torch.eye(\n",
    "            len(target_labels), dtype=torch.bool, device=device\n",
    "        )\n",
    "        \n",
    "        # Compute terms with stability\n",
    "        pos_term = (sim_matrix.exp() * pos_mask.float()).sum(dim=1) + 1e-8\n",
    "        neg_term = (sim_matrix.exp() * neg_mask.float()).sum(dim=1) + 1e-8\n",
    "        \n",
    "        loss = -torch.log(pos_term / (pos_term + neg_term))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2ac32",
   "metadata": {},
   "source": [
    "### Few-shot sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94f9ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot sampling function\n",
    "def get_few_shot_indices(dataset, shots_per_class=5):\n",
    "    \"\"\"\n",
    "    Returns a balanced list of indices for few-shot learning by randomly selecting\n",
    "    a fixed number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        dataset (ImageFolder): A PyTorch ImageFolder dataset (or any dataset with a `.samples` attribute \n",
    "                              containing (path, label) tuples).\n",
    "        shots_per_class (int, optional): Number of samples to select per class. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of selected indices, ensuring `shots_per_class` samples per class.\n",
    "\n",
    "    Example:\n",
    "        >>> target_set = ImageFolder(root='data/target', transform=transforms.ToTensor())\n",
    "        >>> few_shot_indices = get_few_shot_indices(target_set, shots_per_class=3)\n",
    "        >>> few_shot_loader = DataLoader(Subset(target_set, few_shot_indices), batch_size=3)\n",
    "    \"\"\"\n",
    "    \n",
    "    class_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class_indices.setdefault(label, []).append(idx)\n",
    "    \n",
    "    selected_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        selected_indices.extend(np.random.choice(indices, shots_per_class, replace=False))\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3378ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50FeatureExtractor(nn.Module):\n",
    "    def __init__(self, base_model, embedding_dim=512):\n",
    "        super().__init__()\n",
    "        # Keep original layers up to avgpool\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        # Preserve BOTH original fc and your custom fc\n",
    "        self.original_fc = base_model.fc  # This is your modified fc (with dropout + num_classes)\n",
    "        # Add projection for embeddings\n",
    "        self.projection = nn.Linear(2048, embedding_dim)\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        x = self.features(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        logits = self.original_fc(x)\n",
    "        embeddings = self.projection(x)\n",
    "        \n",
    "        return (logits, embeddings) if return_embeddings else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c64fdd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_fer_model_contrastive = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet50_fer_model_contrastive.fc = nn.Sequential(\n",
    "    nn.Dropout(params[\"dropout_rate\"]),\n",
    "    nn.Linear(resnet50_fer_model_contrastive.fc.in_features, params[\"num_classes\"])\n",
    ")\n",
    "resnet50_fer_model_contrastive.load_state_dict(torch.load('weights/RESNET50_model_with_fer2013_weights.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32bcf1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_contrastive = ResNet50FeatureExtractor(resnet50_fer_model_contrastive).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc4b1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and losses\n",
    "params = {'num_classes': 7, 'dropout_rate': 0.2}  # Example for FER2013\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "cont_criterion = ContrastiveLoss(temperature=0.2)\n",
    "optimizer = torch.optim.AdamW(feature_extractor_contrastive.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Prepare few-shot target loader training set\n",
    "few_shot_indices = get_few_shot_indices(manga_faces_train_images, shots_per_class=15)\n",
    "few_shot_loader = DataLoader(\n",
    "    Subset(manga_faces_train_images, few_shot_indices),\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    drop_last=True  # Avoid partial batches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1abeea",
   "metadata": {},
   "source": [
    "### Modified Training Loop for Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62767354",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9f2fc86",
   "metadata": {},
   "source": [
    "#### Using only CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114099c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca04043",
   "metadata": {},
   "source": [
    "#### Using only ContrastiveLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57b8020",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de4d09d0",
   "metadata": {},
   "source": [
    "#### Using both CrossEntropyLoss and ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed09ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with domain adaptation\n",
    "def train_epoch(model, source_loader, target_loader, optimizer, epoch, epochs):\n",
    "    model.train()\n",
    "    target_iter = cycle(target_loader)  # Infinite iterator\n",
    "    \n",
    "    # Initialize metrics\n",
    "    total_cls_loss = 0.0\n",
    "    total_cont_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "    source_correct = 0\n",
    "    target_correct = 0\n",
    "    total_source_samples = 0\n",
    "    total_target_samples = 0\n",
    "    \n",
    "    \n",
    "    tk = tqdm(source_loader, desc=\"EPOCH\" + \"[TRAIN]\" + str(epoch) + \"/\" + str(epochs))\n",
    "    \n",
    "    for batch_idx, (source_imgs, source_lbls) in enumerate(tk):\n",
    "        # Get target batch\n",
    "        target_imgs, target_lbls = next(target_iter)\n",
    "        \n",
    "        # Move to device\n",
    "        source_imgs = source_imgs.to(device)\n",
    "        source_lbls = source_lbls.to(device)\n",
    "        target_imgs = target_imgs.to(device)\n",
    "        target_lbls = target_lbls.to(device)\n",
    "        \n",
    "        # Forward pass with embeddings\n",
    "        source_logits, source_emb = model(source_imgs, return_embeddings=True)\n",
    "        target_logits, target_emb = model(target_imgs, return_embeddings=True)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        source_preds = source_logits.argmax(dim=1)\n",
    "        target_preds = target_logits.argmax(dim=1)\n",
    "        \n",
    "        # Update counters\n",
    "        batch_source_correct = (source_preds == source_lbls).sum().item()\n",
    "        batch_target_correct = (target_preds == target_lbls).sum().item()\n",
    "        \n",
    "        source_correct += batch_source_correct\n",
    "        target_correct += batch_target_correct\n",
    "        total_source_samples += source_lbls.size(0)\n",
    "        total_target_samples += target_lbls.size(0)\n",
    "        \n",
    "        # Loss calculation\n",
    "        cls_loss = cls_criterion(source_logits, source_lbls) + \\\n",
    "                cls_criterion(target_logits, target_lbls)\n",
    "        \n",
    "        cont_loss = cont_criterion(source_emb, source_lbls,\n",
    "                                 target_emb, target_lbls)\n",
    "        \n",
    "        current_loss = cls_loss + 0.9 * cont_loss # Adjusted weight\n",
    "        \n",
    "        # Update metrics\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_cont_loss += cont_loss.item() * 0.5 # Adjusted weight\n",
    "        running_total_loss += current_loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate batch-level accuracies\n",
    "        batch_source_acc = 100 * batch_source_correct / source_lbls.size(0)\n",
    "        batch_target_acc = 100 * batch_target_correct / target_lbls.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        tk.set_postfix({\n",
    "            'CLS Loss': f'{total_cls_loss / (batch_idx + 1):.4f}',\n",
    "            'CONT Loss': f'{total_cont_loss / (batch_idx + 1):.4f}',\n",
    "            'Total Loss': f'{running_total_loss / (batch_idx + 1):.4f}',\n",
    "            'Source Acc': f'{batch_source_acc:.2f}%',\n",
    "            'Target Acc': f'{batch_target_acc:.2f}%'\n",
    "        })\n",
    "            \n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_cls_loss = total_cls_loss / len(source_loader)\n",
    "    epoch_cont_loss = total_cont_loss / len(source_loader)\n",
    "    epoch_total_loss = running_total_loss / len(source_loader)\n",
    "    \n",
    "    epoch_source_acc = 100 * source_correct / total_source_samples\n",
    "    epoch_target_acc = 100 * target_correct / total_target_samples\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n",
    "    print(f\"CLS Loss: {epoch_cls_loss:.4f} | CONT Loss: {epoch_cont_loss:.4f} | Total Loss: {epoch_total_loss:.4f}\")\n",
    "    print(f\"Source Acc: {epoch_source_acc:.2f}% | Target Acc: {epoch_target_acc:.2f}%\")\n",
    "    \n",
    "    return epoch_cls_loss, epoch_cont_loss, epoch_total_loss, epoch_source_acc, epoch_target_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cca20e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e9d6647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/10: 100%|██████████| 225/225 [02:08<00:00,  1.75it/s, CLS Loss=1.5588, CONT Loss=0.4598, Total Loss=2.3864, Source Acc=54.05%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 Summary:\n",
      "CLS Loss: 1.5588 | CONT Loss: 0.4598 | Total Loss: 2.3864\n",
      "Source Acc: 48.32% | Target Acc: 95.78%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/10: 100%|██████████| 225/225 [00:39<00:00,  5.75it/s, CLS Loss=1.4144, CONT Loss=0.3028, Total Loss=1.9594, Source Acc=45.95%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 Summary:\n",
      "CLS Loss: 1.4144 | CONT Loss: 0.3028 | Total Loss: 1.9594\n",
      "Source Acc: 52.38% | Target Acc: 96.18%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/10: 100%|██████████| 225/225 [00:38<00:00,  5.84it/s, CLS Loss=1.3408, CONT Loss=0.2654, Total Loss=1.8185, Source Acc=51.35%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 Summary:\n",
      "CLS Loss: 1.3408 | CONT Loss: 0.2654 | Total Loss: 1.8185\n",
      "Source Acc: 54.83% | Target Acc: 96.44%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/10: 100%|██████████| 225/225 [00:38<00:00,  5.84it/s, CLS Loss=1.2691, CONT Loss=0.2305, Total Loss=1.6840, Source Acc=35.14%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 Summary:\n",
      "CLS Loss: 1.2691 | CONT Loss: 0.2305 | Total Loss: 1.6840\n",
      "Source Acc: 56.07% | Target Acc: 97.60%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/10: 100%|██████████| 225/225 [00:37<00:00,  5.92it/s, CLS Loss=1.1971, CONT Loss=0.2010, Total Loss=1.5589, Source Acc=51.35%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 Summary:\n",
      "CLS Loss: 1.1971 | CONT Loss: 0.2010 | Total Loss: 1.5589\n",
      "Source Acc: 57.49% | Target Acc: 98.62%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/10: 100%|██████████| 225/225 [00:38<00:00,  5.81it/s, CLS Loss=1.1699, CONT Loss=0.1853, Total Loss=1.5035, Source Acc=70.27%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 Summary:\n",
      "CLS Loss: 1.1699 | CONT Loss: 0.1853 | Total Loss: 1.5035\n",
      "Source Acc: 58.12% | Target Acc: 98.62%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/10: 100%|██████████| 225/225 [00:37<00:00,  5.93it/s, CLS Loss=1.1252, CONT Loss=0.2025, Total Loss=1.4896, Source Acc=70.27%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 Summary:\n",
      "CLS Loss: 1.1252 | CONT Loss: 0.2025 | Total Loss: 1.4896\n",
      "Source Acc: 59.40% | Target Acc: 98.53%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/10: 100%|██████████| 225/225 [00:38<00:00,  5.87it/s, CLS Loss=1.0839, CONT Loss=0.1385, Total Loss=1.3332, Source Acc=51.35%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 Summary:\n",
      "CLS Loss: 1.0839 | CONT Loss: 0.1385 | Total Loss: 1.3332\n",
      "Source Acc: 60.21% | Target Acc: 98.98%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/10: 100%|██████████| 225/225 [00:38<00:00,  5.83it/s, CLS Loss=1.0556, CONT Loss=0.1311, Total Loss=1.2915, Source Acc=51.35%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 Summary:\n",
      "CLS Loss: 1.0556 | CONT Loss: 0.1311 | Total Loss: 1.2915\n",
      "Source Acc: 61.70% | Target Acc: 98.93%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/10: 100%|██████████| 225/225 [00:39<00:00,  5.73it/s, CLS Loss=1.0066, CONT Loss=0.1269, Total Loss=1.2351, Source Acc=75.68%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 Summary:\n",
      "CLS Loss: 1.0066 | CONT Loss: 0.1269 | Total Loss: 1.2351\n",
      "Source Acc: 62.85% | Target Acc: 99.56%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contrastive_loss_metrics = {\n",
    "    'cls_loss': [],\n",
    "    'cont_loss': [],\n",
    "    'total_loss': [],\n",
    "    'source_accuracy': [],\n",
    "    'target_accuracy': []\n",
    "}\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    cls_loss, cont_loss, total_loss, source_acc, target_acc = \\\n",
    "        train_epoch(feature_extractor_contrastive, training_loader, few_shot_loader, optimizer, epoch, EPOCHS)\n",
    "    \n",
    "    \n",
    "    contrastive_loss_metrics['cls_loss'].append(cls_loss)\n",
    "    contrastive_loss_metrics['cont_loss'].append(cont_loss)\n",
    "    contrastive_loss_metrics['total_loss'].append(total_loss)\n",
    "    contrastive_loss_metrics['source_accuracy'].append(source_acc)\n",
    "    contrastive_loss_metrics['target_accuracy'].append(target_acc)\n",
    "    \n",
    "    if total_loss < best_valid_loss:\n",
    "        torch.save(feature_extractor_contrastive.state_dict(), 'weights/base_model_contrastive_learning_weights.pt')\n",
    "        print(\"SAVED-BEST-WEIGHTS!\")\n",
    "        best_valid_loss = total_loss\n",
    "        #patience_counter = 0 # Reset early stopping\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc0c8ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Store the metrics from when the model was tested on the out-of-distribution dataset\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(contrastive_loss_metrics['cls_loss']) + 1)),\n",
    "    \"CLS_LOSS\": contrastive_loss_metrics['cls_loss'],\n",
    "    \"CONT_LOSS\": contrastive_loss_metrics['cont_loss'],\n",
    "    \"Total Loss\": contrastive_loss_metrics['total_loss'],\n",
    "    \"Source Accuracy\": contrastive_loss_metrics['source_accuracy'],\n",
    "    \"Target Accuracy\": contrastive_loss_metrics['target_accuracy']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/resnet50_model_contrastive_learning_stats_training.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "587a22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/10: 100%|██████████| 5/5 [00:01<00:00,  3.02it/s, loss=1.478963, acc=0.418750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/10: 100%|██████████| 5/5 [00:00<00:00, 17.86it/s, loss=1.559164, acc=0.418750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/10: 100%|██████████| 5/5 [00:00<00:00, 19.02it/s, loss=1.436139, acc=0.418750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/10: 100%|██████████| 5/5 [00:00<00:00, 19.39it/s, loss=1.414413, acc=0.462500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/10: 100%|██████████| 5/5 [00:00<00:00, 18.01it/s, loss=1.493332, acc=0.375000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]6/10: 100%|██████████| 5/5 [00:00<00:00, 17.34it/s, loss=1.616026, acc=0.418750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]7/10: 100%|██████████| 5/5 [00:00<00:00, 16.74it/s, loss=1.618096, acc=0.418750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]8/10: 100%|██████████| 5/5 [00:00<00:00, 16.98it/s, loss=1.866287, acc=0.331250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]9/10: 100%|██████████| 5/5 [00:00<00:00, 17.93it/s, loss=1.566414, acc=0.375000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]10/10: 100%|██████████| 5/5 [00:00<00:00, 16.57it/s, loss=1.710280, acc=0.331250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tEST the model:\n",
    "cont_test_out_of_distribution_metrics = test_out_of_distribution(feature_extractor_contrastive, manga_faces_test_images_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fdafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_loader, target=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            imgs = imgs.to(device)\n",
    "            lbls = lbls.to(device)\n",
    "            \n",
    "            logits, _ = model(imgs, return_embeddings=True)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(lbls.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    class_report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "    \n",
    "    print(f\"{'Target' if target else 'Source'} Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c1877",
   "metadata": {},
   "source": [
    "#### Test on Source Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d18f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 57/57 [00:58<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Test Accuracy: 57.55%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.45      0.48       958\n",
      "           1       0.81      0.15      0.26       111\n",
      "           2       0.62      0.18      0.28      1024\n",
      "           3       0.79      0.80      0.80      1774\n",
      "           4       0.46      0.67      0.55      1233\n",
      "           5       0.49      0.48      0.49      1247\n",
      "           6       0.53      0.78      0.63       831\n",
      "\n",
      "    accuracy                           0.58      7178\n",
      "   macro avg       0.60      0.50      0.50      7178\n",
      "weighted avg       0.59      0.58      0.56      7178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_test_accuracy, source_report = evaluate_model(feature_extractor_contrastive, test_loader)\n",
    "# Contrastive learning model does not forget the source domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623c9f3",
   "metadata": {},
   "source": [
    "#### Test on Target Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77ca4c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 12.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Test Accuracy: 40.15%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.14      0.19        21\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.60      0.53      0.57        49\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.33      0.59      0.42        22\n",
      "           6       0.65      0.28      0.39        40\n",
      "\n",
      "    accuracy                           0.40       132\n",
      "   macro avg       0.31      0.26      0.26       132\n",
      "weighted avg       0.52      0.40      0.43       132\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_test_accuracy, target_report = evaluate_model(feature_extractor_contrastive, manga_faces_test_images_loader, target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.class_to_idx"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
