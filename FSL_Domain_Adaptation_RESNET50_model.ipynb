{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73db0601",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a374da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69d7f8",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d415bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070573d",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5689ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomAffine(degrees=0, shear=0.2, scale=(0.8, 1.2))\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f49a1",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df5c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fer_2013_dir = Path(os.getcwd(), 'datasets', 'fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97db57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = ImageFolder(root=fer_2013_dir / 'train', transform=train_transforms)\n",
    "training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_set = ImageFolder(root=fer_2013_dir / 'test', transform=val_transforms)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40020c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 28709 images\n",
      "Testing set: 7178 images\n",
      "One image batch shape : torch.Size([128, 3, 48, 48])\n",
      "One label batch shape : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Print shape of training and testing images\n",
    "print(f\"Training set: {len(training_set)} images\")\n",
    "print(f\"Testing set: {len(test_set)} images\")\n",
    "\n",
    "for images, labels in training_loader:\n",
    "  break\n",
    "\n",
    "print(f\"One image batch shape : {images.shape}\")\n",
    "print(f\"One label batch shape : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17391128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(training_set.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08b7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', \n",
    "    4: 'neutral', 5: 'sad', 6: 'surprise'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867fc1",
   "metadata": {},
   "source": [
    "#### Show sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0b98a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch\n",
    "images, labels = next(iter(training_loader))\n",
    "\n",
    "# Unnormalize a few images from the batch\n",
    "def unnormalize(img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return img * std + mean\n",
    "\n",
    "# Display first 4 images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    img = unnormalize(images[i])\n",
    "    img = img.permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Label: {label_map[int(labels[i])]}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253eda5",
   "metadata": {},
   "source": [
    "# ResNet50 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41261683",
   "metadata": {},
   "source": [
    "### Define Class & Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2f5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={          \n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_classes\": 7,\n",
    "}\n",
    "\n",
    "resnet50_fer_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "resnet50_fer_model.fc = nn.Sequential(\n",
    "    nn.Dropout(params[\"dropout_rate\"]),\n",
    "    nn.Linear(resnet50_fer_model.fc.in_features, params[\"num_classes\"])\n",
    ")\n",
    "resnet50_fer_model = resnet50_fer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b95d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [128, 7]                  --\n",
      "├─Conv2d: 1-1                            [128, 64, 24, 24]         9,408\n",
      "├─BatchNorm2d: 1-2                       [128, 64, 24, 24]         128\n",
      "├─ReLU: 1-3                              [128, 64, 24, 24]         --\n",
      "├─MaxPool2d: 1-4                         [128, 64, 12, 12]         --\n",
      "├─Sequential: 1-5                        [128, 256, 12, 12]        --\n",
      "│    └─Bottleneck: 2-1                   [128, 256, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 12, 12]         4,096\n",
      "│    │    └─BatchNorm2d: 3-2             [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-3                    [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-4                  [128, 64, 12, 12]         36,864\n",
      "│    │    └─BatchNorm2d: 3-5             [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-6                    [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-7                  [128, 256, 12, 12]        16,384\n",
      "│    │    └─BatchNorm2d: 3-8             [128, 256, 12, 12]        512\n",
      "│    │    └─Sequential: 3-9              [128, 256, 12, 12]        16,896\n",
      "│    │    └─ReLU: 3-10                   [128, 256, 12, 12]        --\n",
      "│    └─Bottleneck: 2-2                   [128, 256, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-11                 [128, 64, 12, 12]         16,384\n",
      "│    │    └─BatchNorm2d: 3-12            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-13                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-14                 [128, 64, 12, 12]         36,864\n",
      "│    │    └─BatchNorm2d: 3-15            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-16                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-17                 [128, 256, 12, 12]        16,384\n",
      "│    │    └─BatchNorm2d: 3-18            [128, 256, 12, 12]        512\n",
      "│    │    └─ReLU: 3-19                   [128, 256, 12, 12]        --\n",
      "│    └─Bottleneck: 2-3                   [128, 256, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-20                 [128, 64, 12, 12]         16,384\n",
      "│    │    └─BatchNorm2d: 3-21            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-22                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-23                 [128, 64, 12, 12]         36,864\n",
      "│    │    └─BatchNorm2d: 3-24            [128, 64, 12, 12]         128\n",
      "│    │    └─ReLU: 3-25                   [128, 64, 12, 12]         --\n",
      "│    │    └─Conv2d: 3-26                 [128, 256, 12, 12]        16,384\n",
      "│    │    └─BatchNorm2d: 3-27            [128, 256, 12, 12]        512\n",
      "│    │    └─ReLU: 3-28                   [128, 256, 12, 12]        --\n",
      "├─Sequential: 1-6                        [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-4                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-29                 [128, 128, 12, 12]        32,768\n",
      "│    │    └─BatchNorm2d: 3-30            [128, 128, 12, 12]        256\n",
      "│    │    └─ReLU: 3-31                   [128, 128, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-32                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-33            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-34                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-35                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-36            [128, 512, 6, 6]          1,024\n",
      "│    │    └─Sequential: 3-37             [128, 512, 6, 6]          132,096\n",
      "│    │    └─ReLU: 3-38                   [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-5                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-39                 [128, 128, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-40            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-41                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-42                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-43            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-44                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-45                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-46            [128, 512, 6, 6]          1,024\n",
      "│    │    └─ReLU: 3-47                   [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-6                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-48                 [128, 128, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-49            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-50                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-51                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-52            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-53                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-54                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-55            [128, 512, 6, 6]          1,024\n",
      "│    │    └─ReLU: 3-56                   [128, 512, 6, 6]          --\n",
      "│    └─Bottleneck: 2-7                   [128, 512, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-57                 [128, 128, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-58            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-59                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-60                 [128, 128, 6, 6]          147,456\n",
      "│    │    └─BatchNorm2d: 3-61            [128, 128, 6, 6]          256\n",
      "│    │    └─ReLU: 3-62                   [128, 128, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-63                 [128, 512, 6, 6]          65,536\n",
      "│    │    └─BatchNorm2d: 3-64            [128, 512, 6, 6]          1,024\n",
      "│    │    └─ReLU: 3-65                   [128, 512, 6, 6]          --\n",
      "├─Sequential: 1-7                        [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-8                   [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-66                 [128, 256, 6, 6]          131,072\n",
      "│    │    └─BatchNorm2d: 3-67            [128, 256, 6, 6]          512\n",
      "│    │    └─ReLU: 3-68                   [128, 256, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-69                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-70            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-71                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-72                 [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-73            [128, 1024, 3, 3]         2,048\n",
      "│    │    └─Sequential: 3-74             [128, 1024, 3, 3]         526,336\n",
      "│    │    └─ReLU: 3-75                   [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-9                   [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-76                 [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-77            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-78                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-79                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-80            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-81                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-82                 [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-83            [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-84                   [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-10                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-85                 [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-86            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-87                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-88                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-89            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-90                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-91                 [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-92            [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-93                   [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-11                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-94                 [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-95            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-96                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-97                 [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-98            [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-99                   [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-100                [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-101           [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-102                  [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-12                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-103                [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-104           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-105                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-106                [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-107           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-108                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-109                [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-110           [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-111                  [128, 1024, 3, 3]         --\n",
      "│    └─Bottleneck: 2-13                  [128, 1024, 3, 3]         --\n",
      "│    │    └─Conv2d: 3-112                [128, 256, 3, 3]          262,144\n",
      "│    │    └─BatchNorm2d: 3-113           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-114                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-115                [128, 256, 3, 3]          589,824\n",
      "│    │    └─BatchNorm2d: 3-116           [128, 256, 3, 3]          512\n",
      "│    │    └─ReLU: 3-117                  [128, 256, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-118                [128, 1024, 3, 3]         262,144\n",
      "│    │    └─BatchNorm2d: 3-119           [128, 1024, 3, 3]         2,048\n",
      "│    │    └─ReLU: 3-120                  [128, 1024, 3, 3]         --\n",
      "├─Sequential: 1-8                        [128, 2048, 2, 2]         --\n",
      "│    └─Bottleneck: 2-14                  [128, 2048, 2, 2]         --\n",
      "│    │    └─Conv2d: 3-121                [128, 512, 3, 3]          524,288\n",
      "│    │    └─BatchNorm2d: 3-122           [128, 512, 3, 3]          1,024\n",
      "│    │    └─ReLU: 3-123                  [128, 512, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-124                [128, 512, 2, 2]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-125           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-126                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-127                [128, 2048, 2, 2]         1,048,576\n",
      "│    │    └─BatchNorm2d: 3-128           [128, 2048, 2, 2]         4,096\n",
      "│    │    └─Sequential: 3-129            [128, 2048, 2, 2]         2,101,248\n",
      "│    │    └─ReLU: 3-130                  [128, 2048, 2, 2]         --\n",
      "│    └─Bottleneck: 2-15                  [128, 2048, 2, 2]         --\n",
      "│    │    └─Conv2d: 3-131                [128, 512, 2, 2]          1,048,576\n",
      "│    │    └─BatchNorm2d: 3-132           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-133                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-134                [128, 512, 2, 2]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-135           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-136                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-137                [128, 2048, 2, 2]         1,048,576\n",
      "│    │    └─BatchNorm2d: 3-138           [128, 2048, 2, 2]         4,096\n",
      "│    │    └─ReLU: 3-139                  [128, 2048, 2, 2]         --\n",
      "│    └─Bottleneck: 2-16                  [128, 2048, 2, 2]         --\n",
      "│    │    └─Conv2d: 3-140                [128, 512, 2, 2]          1,048,576\n",
      "│    │    └─BatchNorm2d: 3-141           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-142                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-143                [128, 512, 2, 2]          2,359,296\n",
      "│    │    └─BatchNorm2d: 3-144           [128, 512, 2, 2]          1,024\n",
      "│    │    └─ReLU: 3-145                  [128, 512, 2, 2]          --\n",
      "│    │    └─Conv2d: 3-146                [128, 2048, 2, 2]         1,048,576\n",
      "│    │    └─BatchNorm2d: 3-147           [128, 2048, 2, 2]         4,096\n",
      "│    │    └─ReLU: 3-148                  [128, 2048, 2, 2]         --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [128, 2048, 1, 1]         --\n",
      "├─Sequential: 1-10                       [128, 7]                  --\n",
      "│    └─Dropout: 2-17                     [128, 2048]               --\n",
      "│    └─Linear: 2-18                      [128, 7]                  14,343\n",
      "==========================================================================================\n",
      "Total params: 23,522,375\n",
      "Trainable params: 23,522,375\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 27.26\n",
      "==========================================================================================\n",
      "Input size (MB): 3.54\n",
      "Forward/backward pass size (MB): 1083.71\n",
      "Params size (MB): 94.09\n",
      "Estimated Total Size (MB): 1181.34\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(summary(resnet50_fer_model, input_size=(BATCH_SIZE, 3, 48, 48), device=device.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8a14",
   "metadata": {},
   "source": [
    "# Create Train and Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf63633",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cee4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred,y_true):\n",
    "    top_p,top_class = y_pred.topk(1, dim = 1)\n",
    "    equals = top_class == y_true.view(*top_class.shape)\n",
    "    return torch.mean(equals.type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Train one epoch of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The  model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities from model\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for logging; Total loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "\n",
    "        # Print/log training loss and accuracy for this epoch\n",
    "        tk.set_postfix({\n",
    "            'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "            'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "        })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7824849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Test one epoch of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "        \n",
    "        running_acc (float): Returns epoch accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[VALID]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for t, data in enumerate(tk):          \n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute log probabilities from model\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)            \n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate loss for logging; Total loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "            \n",
    "\n",
    "            tk.set_postfix({\n",
    "                'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "                'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "            })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, training_dataloader, testing_dataloader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 3\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'training_loss': [],\n",
    "        'training_accuracy': [],\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss, train_accuracy = train_one_epoch(model, training_dataloader, criterion, optimizer, device, epoch, epochs)\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "\n",
    "        history_metrics['training_loss'].append(train_loss)\n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['training_accuracy'].append(train_accuracy)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "\n",
    "        # Update the learning rate based on validation loss and print\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), 'weights/RESNET50_model_with_fer2013_weights.pt')\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "            \n",
    "        \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1b19",
   "metadata": {},
   "source": [
    "### Run train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f103698",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e820802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/100:   9%|▉         | 20/225 [00:10<01:47,  1.91it/s, loss=1.973859, acc=0.158984]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m resnet_model_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet50_fer_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m, in \u001b[0;36mtrain_and_validate_model\u001b[1;34m(model, training_dataloader, testing_dataloader, epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheduler\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Testing step\u001b[39;00m\n\u001b[0;32m     46\u001b[0m valid_loss, valid_accuracy \u001b[38;5;241m=\u001b[39m test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device, current_epoch, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     16\u001b[0m tk \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[TRAIN]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(current_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epochs))\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tk):\n\u001b[0;32m     19\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     21\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torchvision\\datasets\\folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torchvision\\datasets\\folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torchvision\\datasets\\folder.py:262\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    263\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "resnet_model_losses = train_and_validate_model(resnet50_fer_model, training_loader, test_loader, epochs=100, learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a7c3b",
   "metadata": {},
   "source": [
    "> TRained for about 50 + 14 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time2 = datetime.now()\n",
    "print(\"Training time: \", time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6722ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(resnet_model_losses['training_loss']) + 1)),\n",
    "    \"Training Loss\": resnet_model_losses['training_loss'],\n",
    "    \"Validation Loss\": resnet_model_losses['validation_loss'],\n",
    "    \"Training Accuracy\": resnet_model_losses['training_accuracy'],\n",
    "    \"Validation Accuracy\": resnet_model_losses['validation_accuracy']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"base_model_stats_5_layers.csv\", index=False)\n",
    "print(\"Losses and accuracy saved to losses_and_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39f44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680836f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c183a06",
   "metadata": {},
   "source": [
    "# FSL DA Prototypical Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556c60f",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15233c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotFERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for few-shot FER, where images are organized by class in folders.\n",
    "    This dataset generates episodes (tasks) on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, n_way=5, k_shot=1, k_query=5, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Root folder containing one folder per class.\n",
    "        n_way: number of classes per episode.\n",
    "        k_shot: number of support examples per class.\n",
    "        k_query: number of query examples per class.\n",
    "        transform: transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Build a mapping: class -> list of image paths.\n",
    "        self.class_to_imgs = {}\n",
    "        for cls_name in os.listdir(root_dir):\n",
    "            cls_folder = Path.joinpath(root_dir, cls_name)\n",
    "            if Path.is_dir(cls_folder):\n",
    "                self.class_to_imgs[cls_name] = [Path.joinpath(cls_folder, img)                                                 \n",
    "                                                 for img in Path(cls_folder).rglob('*')\n",
    "                                                 if img.endswith('.jpg') or img.endswith('.png')]        \n",
    "        self.classes = list(self.class_to_imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Define the number of episodes arbitrarily.\n",
    "        return 1000  # or any number representing episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly sample n_way classes for this episode.\n",
    "        sampled_classes = random.sample(self.classes, self.n_way)\n",
    "        support_imgs, support_labels = [], []\n",
    "        query_imgs, query_labels = [], []\n",
    "        \n",
    "        label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "        for cls_name in sampled_classes:\n",
    "            imgs = self.class_to_imgs[cls_name]\n",
    "            # Ensure there are enough examples in this class.\n",
    "            selected_imgs = random.sample(imgs, self.k_shot + self.k_query)\n",
    "            support_paths = selected_imgs[:self.k_shot]\n",
    "            query_paths = selected_imgs[self.k_shot:]\n",
    "            \n",
    "            for sp in support_paths:\n",
    "                img = Image.open(sp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_imgs.append(img)\n",
    "                support_labels.append(label_map[cls_name])\n",
    "            \n",
    "            for qp in query_paths:\n",
    "                img = Image.open(qp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_imgs.append(img)\n",
    "                query_labels.append(label_map[cls_name])\n",
    "        \n",
    "        # Convert lists to tensors.\n",
    "        support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels, dtype=torch.long)\n",
    "        query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels, dtype=torch.long)\n",
    "        \n",
    "        return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adc3d5",
   "metadata": {},
   "source": [
    "### Constructing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b699e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (should match what the encoder expects)\n",
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Root folder with classes as subfolders.\n",
    "few_shot_dataset = FewShotFERDataset(root_dir=\"path/to/FER_few_shot\", n_way=5, k_shot=1, k_query=5, transform=transform)\n",
    "few_shot_loader = DataLoader(few_shot_dataset, batch_size=1, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3e480",
   "metadata": {},
   "source": [
    "### Prototypical Network Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, support_imgs, support_labels, query_imgs, query_labels, device):\n",
    "    \"\"\"\n",
    "    model: the fine-tuned FER model, used as feature extractor.\n",
    "    support_imgs: [n_way*k_shot, C, H, W]\n",
    "    query_imgs: [n_way*k_query, C, H, W]\n",
    "    support_labels: [n_way*k_shot]\n",
    "    query_labels: [n_way*k_query]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        \n",
    "        # Extract features using the pretrained encoder.\n",
    "        # We assume the encoder returns a [B, 1 + num_patches, D] tensor and we use the CLS token.\n",
    "        def get_cls_features(x):\n",
    "            tokens = model.encoder(x)  # shape: [B, 1 + num_patches, D]\n",
    "            return tokens[:, 0, :]     # extract CLS token\n",
    "        \n",
    "        support_feats = get_cls_features(support_imgs)  # shape: [n_way*k_shot, D]\n",
    "        query_feats = get_cls_features(query_imgs)      # shape: [n_way*k_query, D]\n",
    "        \n",
    "        # Compute prototypes: mean feature for each class.\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        prototypes = []\n",
    "        for cls in range(n_way):\n",
    "            cls_indices = (support_labels == cls).nonzero(as_tuple=True)[0]\n",
    "            cls_feats = support_feats[cls_indices]\n",
    "            prototype = cls_feats.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "        prototypes = torch.stack(prototypes)  # shape: [n_way, D]\n",
    "        \n",
    "        # Compute distances between each query feature and prototypes.\n",
    "        # We'll use Euclidean distance.\n",
    "        # Expand dimensions: query_feats: [Q, 1, D] and prototypes: [1, n_way, D]\n",
    "        dists = torch.cdist(query_feats, prototypes, p=2)  # shape: [n_way*k_query, n_way]\n",
    "        \n",
    "        # Convert distances to probabilities with a softmax on negative distances.\n",
    "        probs = F.softmax(-dists, dim=1)  # lower distance = higher probability\n",
    "        \n",
    "        # Predictions: class with the highest probability.\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        correct = (preds.cpu() == query_labels).sum().item()\n",
    "        total = query_labels.size(0)\n",
    "    \n",
    "    return correct, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71a500",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Evaluate on a few episodes from the DataLoader\n",
    "# ------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume model_fer is the fine-tuned FER model we defined previously.\n",
    "model_fer = model_fer.to(device)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_episodes = 50  # Evaluate on 50 episodes for instance.\n",
    "\n",
    "for i, ((support_imgs, support_labels), (query_imgs, query_labels)) in enumerate(few_shot_loader):\n",
    "    if i >= num_episodes:\n",
    "        break\n",
    "    correct, total = evaluate_episode(model_fer, support_imgs.squeeze(0), support_labels.squeeze(0),\n",
    "                                      query_imgs.squeeze(0), query_labels.squeeze(0), device)\n",
    "    total_correct += correct\n",
    "    total_samples += total\n",
    "\n",
    "episode_accuracy = 100.0 * total_correct / total_samples\n",
    "print(\"Few-Shot Episode Accuracy: {:.2f}%\".format(episode_accuracy))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
