{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73db0601",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a374da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69d7f8",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d415bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070573d",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5689ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean = [0.485, 0.456, 0.406]\n",
    "#std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# train_transforms = T.Compose([\n",
    "#     T.Resize((48, 48)),\n",
    "#     T.ToTensor(),\n",
    "#     T.RandomHorizontalFlip(p=0.5),\n",
    "#     T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     T.Normalize(mean, std),\n",
    "#     T.RandomAffine(degrees=0, shear=0.2, scale=(0.8, 1.2))\n",
    "# ])\n",
    "\n",
    "mean = [0.485]  # Single channel\n",
    "std = [0.229]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f49a1",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df5c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fer_2013_dir = Path(os.getcwd(), 'datasets', 'fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97db57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = ImageFolder(root=fer_2013_dir / 'train', transform=train_transforms)\n",
    "training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_set = ImageFolder(root=fer_2013_dir / 'test', transform=val_transforms)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40020c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 28709 images\n",
      "Testing set: 7178 images\n",
      "One image batch shape : torch.Size([128, 3, 48, 48])\n",
      "One label batch shape : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Print shape of training and testing images\n",
    "print(f\"Training set: {len(training_set)} images\")\n",
    "print(f\"Testing set: {len(test_set)} images\")\n",
    "\n",
    "for images, labels in training_loader:\n",
    "  break\n",
    "\n",
    "print(f\"One image batch shape : {images.shape}\")\n",
    "print(f\"One label batch shape : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17391128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(training_set.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08b7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', \n",
    "    4: 'neutral', 5: 'sad', 6: 'surprise'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867fc1",
   "metadata": {},
   "source": [
    "#### Show sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0b98a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch\n",
    "images, labels = next(iter(training_loader))\n",
    "\n",
    "# Unnormalize a few images from the batch\n",
    "def unnormalize(img):\n",
    "    # Use values matching your actual normalization\n",
    "    mean = torch.tensor([0.485, 0.485, 0.485]).to(img.device)\n",
    "    std = torch.tensor([0.229, 0.229, 0.229]).to(img.device)\n",
    "    return img * std.view(1, 3, 1, 1) + mean.view(1, 3, 1, 1)\n",
    "\n",
    "# Display with proper channel handling\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    img = unnormalize(images[i].unsqueeze(0)).squeeze(0)\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    axes[i].imshow(img, cmap='gray')  # Force grayscale display\n",
    "    axes[i].set_title(f\"Label: {label_map[int(labels[i])]}\")\n",
    "    axes[i].axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253eda5",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da6d19",
   "metadata": {},
   "source": [
    "### Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e811ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFERModel(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(BaseFERModel, self).__init__()\n",
    "        num_classes = params['num_classes']\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # Output: 32x24x24\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 48, 48)\n",
    "            out = self.layer1(dummy_input)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = self.layer4(out)\n",
    "            out = self.layer5(out)\n",
    "            out = self.layer6(out)\n",
    "            flattened_size = out.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "        # Fully connected (dense) layers\n",
    "        #self.fc1 = nn.Linear(in_features=1024*1*1, out_features=1024)\n",
    "        self.fc1 = nn.Linear(in_features=flattened_size, out_features=1024)\n",
    "        self.dropout = nn.Dropout(params['dropout_rate'])\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=num_classes)\n",
    "       \n",
    "        #self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        output = self.layer1(x)  #  first layer\n",
    "        output = self.layer2(output)  # second layer\n",
    "        output = self.layer3(output)  # third layer\n",
    "        output = self.layer4(output)  # fourth layer\n",
    "        output = self.layer5(output)  # fifth layer\n",
    "        output = self.layer6(output)  # sixth layer        \n",
    "        output = output.view(output.size(0), -1)  # Flatten for the fully connected layers\n",
    "        \n",
    "        # Embedding extraction (no dropout)\n",
    "        embeddings = F.relu(self.fc1(output))\n",
    "        embeddings = self.fc2(embeddings)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Classification head (with dropout)\n",
    "        #logits = self.dropout(F.relu(embeddings))  # Reintroduce dropout here\n",
    "        logits = self.dropout(embeddings)\n",
    "        logits = self.fc3(logits)\n",
    "        \n",
    "        return (logits, embeddings) if return_embeddings else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf315ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFERModel_V2(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Stem\n",
    "            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # FER-Specialized Blocks\n",
    "            DepthwiseSeparableConv(64, 128),\n",
    "            DepthwiseSeparableConv(128, 256),\n",
    "            DepthwiseSeparableConv(256, 512),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((1,1))  # GAP instead of flattening\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, params['num_classes'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        # Feature extraction\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            # Return embeddings before final classifier\n",
    "            embeddings = self.classifier[:-1](x)\n",
    "            logits = self.classifier[-1:](embeddings)\n",
    "            return logits, embeddings\n",
    "        \n",
    "        # Full classification\n",
    "        return self.classifier(x) # logits\n",
    "\n",
    "def DepthwiseSeparableConv(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        # Depthwise\n",
    "        nn.Conv2d(in_c, in_c, 3, padding=1, groups=in_c, bias=False),\n",
    "        nn.BatchNorm2d(in_c),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        # Pointwise\n",
    "        nn.Conv2d(in_c, out_c, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.LeakyReLU(0.1),\n",
    "        nn.MaxPool2d(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41261683",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2f5287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "BaseFERModel_V2                          [128, 7]                  --\n",
      "├─Sequential: 1-1                        [128, 512, 1, 1]          --\n",
      "│    └─Conv2d: 2-1                       [128, 64, 48, 48]         1,728\n",
      "│    └─BatchNorm2d: 2-2                  [128, 64, 48, 48]         128\n",
      "│    └─LeakyReLU: 2-3                    [128, 64, 48, 48]         --\n",
      "│    └─MaxPool2d: 2-4                    [128, 64, 24, 24]         --\n",
      "│    └─Sequential: 2-5                   [128, 128, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-1                  [128, 64, 24, 24]         576\n",
      "│    │    └─BatchNorm2d: 3-2             [128, 64, 24, 24]         128\n",
      "│    │    └─LeakyReLU: 3-3               [128, 64, 24, 24]         --\n",
      "│    │    └─Conv2d: 3-4                  [128, 128, 24, 24]        8,192\n",
      "│    │    └─BatchNorm2d: 3-5             [128, 128, 24, 24]        256\n",
      "│    │    └─LeakyReLU: 3-6               [128, 128, 24, 24]        --\n",
      "│    │    └─MaxPool2d: 3-7               [128, 128, 12, 12]        --\n",
      "│    └─Sequential: 2-6                   [128, 256, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-8                  [128, 128, 12, 12]        1,152\n",
      "│    │    └─BatchNorm2d: 3-9             [128, 128, 12, 12]        256\n",
      "│    │    └─LeakyReLU: 3-10              [128, 128, 12, 12]        --\n",
      "│    │    └─Conv2d: 3-11                 [128, 256, 12, 12]        32,768\n",
      "│    │    └─BatchNorm2d: 3-12            [128, 256, 12, 12]        512\n",
      "│    │    └─LeakyReLU: 3-13              [128, 256, 12, 12]        --\n",
      "│    │    └─MaxPool2d: 3-14              [128, 256, 6, 6]          --\n",
      "│    └─Sequential: 2-7                   [128, 512, 3, 3]          --\n",
      "│    │    └─Conv2d: 3-15                 [128, 256, 6, 6]          2,304\n",
      "│    │    └─BatchNorm2d: 3-16            [128, 256, 6, 6]          512\n",
      "│    │    └─LeakyReLU: 3-17              [128, 256, 6, 6]          --\n",
      "│    │    └─Conv2d: 3-18                 [128, 512, 6, 6]          131,072\n",
      "│    │    └─BatchNorm2d: 3-19            [128, 512, 6, 6]          1,024\n",
      "│    │    └─LeakyReLU: 3-20              [128, 512, 6, 6]          --\n",
      "│    │    └─MaxPool2d: 3-21              [128, 512, 3, 3]          --\n",
      "│    └─AdaptiveAvgPool2d: 2-8            [128, 512, 1, 1]          --\n",
      "├─Sequential: 1-2                        [128, 7]                  --\n",
      "│    └─Linear: 2-9                       [128, 256]                131,328\n",
      "│    └─BatchNorm1d: 2-10                 [128, 256]                512\n",
      "│    └─LeakyReLU: 2-11                   [128, 256]                --\n",
      "│    └─Dropout: 2-12                     [128, 256]                --\n",
      "│    └─Linear: 2-13                      [128, 7]                  1,799\n",
      "==========================================================================================\n",
      "Total params: 314,247\n",
      "Trainable params: 314,247\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 2.41\n",
      "==========================================================================================\n",
      "Input size (MB): 3.54\n",
      "Forward/backward pass size (MB): 698.88\n",
      "Params size (MB): 1.26\n",
      "Estimated Total Size (MB): 703.68\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "        \"initial_filters\": 8,    \n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_classes\": 7}\n",
    "\n",
    "#base_fer_model = BaseFERModel(params).to(device)\n",
    "base_fer_model = BaseFERModel_V2(params).to(device)\n",
    "print(summary(base_fer_model, input_size=(BATCH_SIZE, 3, 48, 48), device=device.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8a14",
   "metadata": {},
   "source": [
    "# Create Train and Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf63633",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cee4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred,y_true):\n",
    "    top_p,top_class = y_pred.topk(1, dim = 1)\n",
    "    equals = top_class == y_true.view(*top_class.shape)\n",
    "    return torch.mean(equals.type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Train one epoch of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The  model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities from model\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for logging; Total loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "\n",
    "        # Print/log training loss and accuracy for this epoch\n",
    "        tk.set_postfix({\n",
    "            'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "            'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "        })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7824849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Test one epoch of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "        \n",
    "        running_acc (float): Returns epoch accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[VALID]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for t, data in enumerate(tk):          \n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute log probabilities from model\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)            \n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate loss for logging; Total loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "            \n",
    "\n",
    "            tk.set_postfix({\n",
    "                'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "                'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "            })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, training_dataloader, testing_dataloader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 5\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'training_loss': [],\n",
    "        'training_accuracy': [],\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss, train_accuracy = train_one_epoch(model, training_dataloader, criterion, optimizer, device, epoch, epochs)\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "\n",
    "        history_metrics['training_loss'].append(train_loss)\n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['training_accuracy'].append(train_accuracy)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "\n",
    "        # Update the learning rate based on validation loss and print\n",
    "        #scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), 'weights/base_model_with_fer2013_weights.pt')\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "            \n",
    "        \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1b19",
   "metadata": {},
   "source": [
    "### Run train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e53dfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e820802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/50: 100%|██████████| 225/225 [03:27<00:00,  1.09it/s, loss=1.719857, acc=0.312969]\n",
      "EPOCH[VALID]1/50: 100%|██████████| 57/57 [00:20<00:00,  2.84it/s, loss=1.531349, acc=0.407237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 2, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/50: 100%|██████████| 225/225 [00:31<00:00,  7.24it/s, loss=1.512696, acc=0.410191]\n",
      "EPOCH[VALID]2/50: 100%|██████████| 57/57 [00:03<00:00, 16.56it/s, loss=1.385648, acc=0.476261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 3, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/50: 100%|██████████| 225/225 [00:31<00:00,  7.10it/s, loss=1.425704, acc=0.450536]\n",
      "EPOCH[VALID]3/50: 100%|██████████| 57/57 [00:03<00:00, 16.37it/s, loss=1.378405, acc=0.476206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 4, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/50: 100%|██████████| 225/225 [00:31<00:00,  7.13it/s, loss=1.367784, acc=0.478639]\n",
      "EPOCH[VALID]4/50: 100%|██████████| 57/57 [00:03<00:00, 16.53it/s, loss=1.331536, acc=0.478262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 5, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/50: 100%|██████████| 225/225 [00:31<00:00,  7.05it/s, loss=1.324140, acc=0.492825]\n",
      "EPOCH[VALID]5/50: 100%|██████████| 57/57 [00:03<00:00, 16.46it/s, loss=1.326619, acc=0.483553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 6, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/50: 100%|██████████| 225/225 [00:31<00:00,  7.12it/s, loss=1.294140, acc=0.503447]\n",
      "EPOCH[VALID]6/50: 100%|██████████| 57/57 [00:03<00:00, 15.75it/s, loss=1.311136, acc=0.509814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 7, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/50: 100%|██████████| 225/225 [00:31<00:00,  7.04it/s, loss=1.266059, acc=0.516350]\n",
      "EPOCH[VALID]7/50: 100%|██████████| 57/57 [00:04<00:00, 13.20it/s, loss=1.276729, acc=0.534814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 8, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/50: 100%|██████████| 225/225 [03:00<00:00,  1.25it/s, loss=1.255420, acc=0.522109]\n",
      "EPOCH[VALID]8/50: 100%|██████████| 57/57 [00:13<00:00,  4.08it/s, loss=1.191492, acc=0.538569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 9, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/50: 100%|██████████| 225/225 [00:33<00:00,  6.63it/s, loss=1.235479, acc=0.528836]\n",
      "EPOCH[VALID]9/50: 100%|██████████| 57/57 [00:04<00:00, 13.79it/s, loss=1.186137, acc=0.539693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 10, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/50: 100%|██████████| 225/225 [00:31<00:00,  7.11it/s, loss=1.221586, acc=0.534170]\n",
      "EPOCH[VALID]10/50: 100%|██████████| 57/57 [00:03<00:00, 16.67it/s, loss=1.184348, acc=0.554879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 11, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]11/50: 100%|██████████| 225/225 [00:30<00:00,  7.32it/s, loss=1.201565, acc=0.537974]\n",
      "EPOCH[VALID]11/50: 100%|██████████| 57/57 [00:03<00:00, 16.74it/s, loss=1.163231, acc=0.564501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 12, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]12/50: 100%|██████████| 225/225 [00:30<00:00,  7.29it/s, loss=1.193982, acc=0.543421]\n",
      "EPOCH[VALID]12/50: 100%|██████████| 57/57 [00:03<00:00, 16.79it/s, loss=1.259271, acc=0.534759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 13, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]13/50: 100%|██████████| 225/225 [00:30<00:00,  7.31it/s, loss=1.184729, acc=0.549726]\n",
      "EPOCH[VALID]13/50: 100%|██████████| 57/57 [00:03<00:00, 16.90it/s, loss=1.227506, acc=0.535636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 14, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]14/50: 100%|██████████| 225/225 [00:30<00:00,  7.27it/s, loss=1.178353, acc=0.550747]\n",
      "EPOCH[VALID]14/50: 100%|██████████| 57/57 [00:03<00:00, 16.84it/s, loss=1.128949, acc=0.581908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 15, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]15/50: 100%|██████████| 225/225 [00:30<00:00,  7.28it/s, loss=1.162799, acc=0.555698]\n",
      "EPOCH[VALID]15/50: 100%|██████████| 57/57 [00:03<00:00, 16.75it/s, loss=1.099521, acc=0.584923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 16, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]16/50: 100%|██████████| 225/225 [00:31<00:00,  7.25it/s, loss=1.146796, acc=0.565177]\n",
      "EPOCH[VALID]16/50: 100%|██████████| 57/57 [00:03<00:00, 16.45it/s, loss=1.129549, acc=0.570285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 17, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]17/50: 100%|██████████| 225/225 [00:30<00:00,  7.30it/s, loss=1.143695, acc=0.567731]\n",
      "EPOCH[VALID]17/50: 100%|██████████| 57/57 [00:03<00:00, 16.80it/s, loss=1.138717, acc=0.571382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 18, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]18/50: 100%|██████████| 225/225 [00:30<00:00,  7.31it/s, loss=1.133258, acc=0.566663]\n",
      "EPOCH[VALID]18/50: 100%|██████████| 57/57 [00:03<00:00, 16.86it/s, loss=1.098197, acc=0.594764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 19, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]19/50: 100%|██████████| 225/225 [00:30<00:00,  7.30it/s, loss=1.122501, acc=0.573823]\n",
      "EPOCH[VALID]19/50: 100%|██████████| 57/57 [00:03<00:00, 17.02it/s, loss=1.068383, acc=0.595669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 20, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]20/50: 100%|██████████| 225/225 [00:30<00:00,  7.31it/s, loss=1.118500, acc=0.575322]\n",
      "EPOCH[VALID]20/50: 100%|██████████| 57/57 [00:03<00:00, 16.73it/s, loss=1.073513, acc=0.594846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 21, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]21/50: 100%|██████████| 225/225 [00:30<00:00,  7.29it/s, loss=1.118624, acc=0.575423]\n",
      "EPOCH[VALID]21/50: 100%|██████████| 57/57 [00:03<00:00, 16.93it/s, loss=1.065407, acc=0.600603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 22, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]22/50: 100%|██████████| 225/225 [00:30<00:00,  7.31it/s, loss=1.107053, acc=0.580354]\n",
      "EPOCH[VALID]22/50: 100%|██████████| 57/57 [00:03<00:00, 16.51it/s, loss=1.045485, acc=0.609540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 23, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]23/50: 100%|██████████| 225/225 [00:30<00:00,  7.26it/s, loss=1.102030, acc=0.583987]\n",
      "EPOCH[VALID]23/50: 100%|██████████| 57/57 [00:03<00:00, 16.86it/s, loss=1.099391, acc=0.581689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 24, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]24/50: 100%|██████████| 225/225 [00:30<00:00,  7.29it/s, loss=1.101820, acc=0.583757]\n",
      "EPOCH[VALID]24/50: 100%|██████████| 57/57 [00:03<00:00, 16.66it/s, loss=1.098343, acc=0.588816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 25, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]25/50: 100%|██████████| 225/225 [00:30<00:00,  7.36it/s, loss=1.094220, acc=0.586636]\n",
      "EPOCH[VALID]25/50: 100%|██████████| 57/57 [00:03<00:00, 16.87it/s, loss=1.056245, acc=0.605428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 26, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]26/50: 100%|██████████| 225/225 [00:30<00:00,  7.32it/s, loss=1.088748, acc=0.587766]\n",
      "EPOCH[VALID]26/50: 100%|██████████| 57/57 [00:03<00:00, 16.65it/s, loss=1.024856, acc=0.611815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 27, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]27/50: 100%|██████████| 225/225 [00:30<00:00,  7.33it/s, loss=1.083257, acc=0.590322]\n",
      "EPOCH[VALID]27/50: 100%|██████████| 57/57 [00:03<00:00, 16.30it/s, loss=1.041537, acc=0.611349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 28, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]28/50: 100%|██████████| 225/225 [00:31<00:00,  7.08it/s, loss=1.078721, acc=0.590941]\n",
      "EPOCH[VALID]28/50: 100%|██████████| 57/57 [00:03<00:00, 16.41it/s, loss=1.085880, acc=0.592489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 29, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]29/50: 100%|██████████| 225/225 [00:31<00:00,  7.22it/s, loss=1.069691, acc=0.593236]\n",
      "EPOCH[VALID]29/50: 100%|██████████| 57/57 [00:03<00:00, 16.37it/s, loss=1.032211, acc=0.611952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 30, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]30/50: 100%|██████████| 225/225 [00:33<00:00,  6.75it/s, loss=1.067263, acc=0.595023]\n",
      "EPOCH[VALID]30/50: 100%|██████████| 57/57 [00:04<00:00, 12.42it/s, loss=1.023720, acc=0.621656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 31, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]31/50: 100%|██████████| 225/225 [00:33<00:00,  6.68it/s, loss=1.059680, acc=0.596235]\n",
      "EPOCH[VALID]31/50: 100%|██████████| 57/57 [00:03<00:00, 16.07it/s, loss=1.025796, acc=0.619956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 32, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]32/50: 100%|██████████| 225/225 [00:32<00:00,  6.90it/s, loss=1.061905, acc=0.599221]\n",
      "EPOCH[VALID]32/50: 100%|██████████| 57/57 [00:03<00:00, 16.44it/s, loss=1.066914, acc=0.605784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 33, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]33/50: 100%|██████████| 225/225 [00:31<00:00,  7.22it/s, loss=1.047255, acc=0.601716]\n",
      "EPOCH[VALID]33/50: 100%|██████████| 57/57 [00:03<00:00, 16.57it/s, loss=1.102563, acc=0.601179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 34, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]34/50: 100%|██████████| 225/225 [00:31<00:00,  7.22it/s, loss=1.047151, acc=0.603284]\n",
      "EPOCH[VALID]34/50: 100%|██████████| 57/57 [00:03<00:00, 16.53it/s, loss=1.017571, acc=0.622368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 35, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]35/50: 100%|██████████| 225/225 [00:31<00:00,  7.21it/s, loss=1.053003, acc=0.599082]\n",
      "EPOCH[VALID]35/50: 100%|██████████| 57/57 [00:03<00:00, 16.61it/s, loss=1.015704, acc=0.619079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 36, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]36/50: 100%|██████████| 225/225 [00:31<00:00,  7.16it/s, loss=1.042825, acc=0.605404]\n",
      "EPOCH[VALID]36/50: 100%|██████████| 57/57 [00:03<00:00, 16.14it/s, loss=1.011261, acc=0.628810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 37, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]37/50: 100%|██████████| 225/225 [00:31<00:00,  7.21it/s, loss=1.034214, acc=0.610487]\n",
      "EPOCH[VALID]37/50: 100%|██████████| 57/57 [00:03<00:00, 16.82it/s, loss=1.016836, acc=0.625822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 38, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]38/50: 100%|██████████| 225/225 [00:31<00:00,  7.25it/s, loss=1.036051, acc=0.608297]\n",
      "EPOCH[VALID]38/50: 100%|██████████| 57/57 [00:03<00:00, 16.28it/s, loss=1.010894, acc=0.622780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 39, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]39/50: 100%|██████████| 225/225 [00:31<00:00,  7.20it/s, loss=1.031930, acc=0.610834]\n",
      "EPOCH[VALID]39/50: 100%|██████████| 57/57 [00:03<00:00, 16.70it/s, loss=1.005712, acc=0.624534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 40, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]40/50: 100%|██████████| 225/225 [00:31<00:00,  7.22it/s, loss=1.030999, acc=0.609659]\n",
      "EPOCH[VALID]40/50: 100%|██████████| 57/57 [00:03<00:00, 16.72it/s, loss=0.993722, acc=0.628481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 41, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]41/50: 100%|██████████| 225/225 [00:31<00:00,  7.23it/s, loss=1.025725, acc=0.613011]\n",
      "EPOCH[VALID]41/50: 100%|██████████| 57/57 [00:03<00:00, 16.49it/s, loss=1.041648, acc=0.608690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 42, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]42/50: 100%|██████████| 225/225 [00:31<00:00,  7.20it/s, loss=1.015697, acc=0.614240]\n",
      "EPOCH[VALID]42/50: 100%|██████████| 57/57 [00:03<00:00, 16.41it/s, loss=1.019615, acc=0.622259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 43, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]43/50: 100%|██████████| 225/225 [00:31<00:00,  7.21it/s, loss=1.015720, acc=0.613436]\n",
      "EPOCH[VALID]43/50: 100%|██████████| 57/57 [00:03<00:00, 16.43it/s, loss=1.033311, acc=0.622423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 3 epoch(s).\n",
      "Epoch 44, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]44/50: 100%|██████████| 225/225 [00:31<00:00,  7.20it/s, loss=1.012528, acc=0.615688]\n",
      "EPOCH[VALID]44/50: 100%|██████████| 57/57 [00:03<00:00, 16.50it/s, loss=1.060455, acc=0.608443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 4 epoch(s).\n",
      "Epoch 45, LR: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]45/50: 100%|██████████| 225/225 [00:30<00:00,  7.28it/s, loss=1.009415, acc=0.617122]\n",
      "EPOCH[VALID]45/50: 100%|██████████| 57/57 [00:03<00:00, 16.34it/s, loss=1.049177, acc=0.615488]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 5 epoch(s).\n",
      "Patience exceeded. Early stopping at epoch 45\n",
      "Early stopping triggered. Stopping training.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "base_model_losses = train_and_validate_model(base_fer_model, training_loader, test_loader, epochs=50, learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "971550d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time:  0:32:00.443535\n"
     ]
    }
   ],
   "source": [
    "time2 = datetime.now()\n",
    "print(\"Total training time: \", time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6722ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(base_model_losses['training_loss']) + 1)),\n",
    "    \"Training Loss\": base_model_losses['training_loss'],\n",
    "    \"Validation Loss\": base_model_losses['validation_loss'],\n",
    "    \"Training Accuracy\": [acc.cpu().item() for acc in base_model_losses['training_accuracy']],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in base_model_losses['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/base_model_stats_001_50_epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143882d",
   "metadata": {},
   "source": [
    "# Test Model Accuracy on Out of Distribution Data set (Manga Faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "846e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out_of_distribution(model, testing_dataloader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "        \n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "                \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0084c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_fer_model_2 = BaseFERModel_V2(params).to(device)\n",
    "base_fer_model_2 = base_fer_model_2.to(device)\n",
    "base_fer_model_2.load_state_dict(torch.load('weights/base_model_with_fer2013_weights.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e436afd",
   "metadata": {},
   "source": [
    "## Import MangaFaces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2c985db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "manga_faces_train_dir = Path(os.getcwd(), 'datasets', 'manga', 'train')\n",
    "manga_faces_train_images = ImageFolder(root=manga_faces_train_dir, transform=train_transforms)\n",
    "manga_faces_train_images_loader = DataLoader(manga_faces_train_images, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Test Set\n",
    "manga_faces_test_dir = Path(os.getcwd(), 'datasets', 'manga', 'test')\n",
    "manga_faces_test_images = ImageFolder(root=manga_faces_test_dir, transform=val_transforms)\n",
    "manga_faces_test_images_loader = DataLoader(manga_faces_test_images, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735db254",
   "metadata": {},
   "source": [
    "## Run 'test_out_of_distribution' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ce41c5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/5: 100%|██████████| 7/7 [00:00<00:00, 10.75it/s, loss=2.583786, acc=0.216071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/5: 100%|██████████| 7/7 [00:00<00:00, 12.12it/s, loss=2.531525, acc=0.193750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/5: 100%|██████████| 7/7 [00:00<00:00, 14.98it/s, loss=2.458348, acc=0.272321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/5: 100%|██████████| 7/7 [00:00<00:00, 14.17it/s, loss=2.452406, acc=0.199107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/5: 100%|██████████| 7/7 [00:00<00:00, 14.96it/s, loss=2.546078, acc=0.234821]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_metrics = test_out_of_distribution(base_fer_model_2, manga_faces_train_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9e9fc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses and accuracy saved\n"
     ]
    }
   ],
   "source": [
    "# Store the metrics from when the model was tested on the out-of-distribution dataset\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(test_out_of_distribution_metrics['validation_loss']) + 1)),\n",
    "    \"Validation Loss\": test_out_of_distribution_metrics['validation_loss'],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in test_out_of_distribution_metrics['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/test_model_out_of_distribution_stats_001_5_epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "82398d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/5: 100%|██████████| 5/5 [00:00<00:00, 10.98it/s, loss=3.377855, acc=0.156250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/5: 100%|██████████| 5/5 [00:00<00:00, 23.95it/s, loss=2.565618, acc=0.156250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/5: 100%|██████████| 5/5 [00:00<00:00, 27.50it/s, loss=2.638279, acc=0.200000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/5: 100%|██████████| 5/5 [00:00<00:00, 27.55it/s, loss=2.533443, acc=0.287500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/5: 100%|██████████| 5/5 [00:00<00:00, 28.39it/s, loss=2.684730, acc=0.156250]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_metrics = test_out_of_distribution(base_fer_model_2, manga_faces_test_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c183a06",
   "metadata": {},
   "source": [
    "# FSL DA Prototypical Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556c60f",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "15233c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotFERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for few-shot FER, where images are organized by class in folders.\n",
    "    This dataset generates episodes (tasks) on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, n_way=5, k_shot=1, k_query=5, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Root folder containing one folder per class.\n",
    "        n_way: number of classes per episode.\n",
    "        k_shot: number of support examples per class.\n",
    "        k_query: number of query examples per class.\n",
    "        transform: transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Build a mapping: class -> list of image paths.\n",
    "        self.class_to_imgs = {}\n",
    "        for cls_name in os.listdir(root_dir):\n",
    "            cls_folder = Path.joinpath(root_dir, cls_name)\n",
    "            if Path.is_dir(cls_folder):\n",
    "                self.class_to_imgs[cls_name] = [Path.joinpath(cls_folder, img)                                                 \n",
    "                                                 for img in Path(cls_folder).rglob('*')\n",
    "                                                 if str(img).endswith('.jpg') or str(img).endswith('.png')]        \n",
    "        self.classes = list(self.class_to_imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Define the number of episodes arbitrarily.\n",
    "        return 1000  # or any number representing episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly sample n_way classes for this episode.\n",
    "        sampled_classes = random.sample(self.classes, self.n_way)\n",
    "        support_imgs, support_labels = [], []\n",
    "        query_imgs, query_labels = [], []\n",
    "        \n",
    "        label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "        for cls_name in sampled_classes:\n",
    "            imgs = self.class_to_imgs[cls_name]\n",
    "            # Ensure there are enough examples in this class.\n",
    "            selected_imgs = random.sample(imgs, self.k_shot + self.k_query)\n",
    "            support_paths = selected_imgs[:self.k_shot]\n",
    "            query_paths = selected_imgs[self.k_shot:]\n",
    "            \n",
    "            for sp in support_paths:\n",
    "                img = Image.open(sp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_imgs.append(img)\n",
    "                support_labels.append(label_map[cls_name])\n",
    "            \n",
    "            for qp in query_paths:\n",
    "                img = Image.open(qp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_imgs.append(img)\n",
    "                query_labels.append(label_map[cls_name])\n",
    "        \n",
    "        # Convert lists to tensors.\n",
    "        support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels, dtype=torch.long)\n",
    "        query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels, dtype=torch.long)\n",
    "        \n",
    "        return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adc3d5",
   "metadata": {},
   "source": [
    "### Instantiate DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6b699e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (should match what the encoder expects)\n",
    "#mean = [0.485]  # Single channel\n",
    "#std = [0.229]\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485],\n",
    "                std=[0.229])\n",
    "])\n",
    "\n",
    "# Root folder with classes as subfolders.\n",
    "few_shot_dataset = FewShotFERDataset(root_dir=manga_faces_train_dir, n_way=4, k_shot=10, k_query=22, transform=transform)\n",
    "few_shot_loader = DataLoader(few_shot_dataset, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3e480",
   "metadata": {},
   "source": [
    "### Prototypical Network Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "584fdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(feature_extractor, support_imgs, support_labels, query_imgs, query_labels, device):\n",
    "    \"\"\"\n",
    "    feature_extractor: model that outputs embeddings [B, embedding_dim]\n",
    "    support_imgs: tensor of shape [n_way * k_shot, C, H, W]\n",
    "    support_labels: tensor of shape [n_way * k_shot]\n",
    "    query_imgs: tensor of shape [n_way * k_query, C, H, W]\n",
    "    query_labels: tensor of shape [n_way * k_query]\n",
    "    \"\"\"\n",
    "    feature_extractor.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move data to device.\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        \n",
    "        # Compute the embeddings.\n",
    "        support_embeddings = feature_extractor(support_imgs)  # [n_way*k_shot, D]\n",
    "        query_embeddings = feature_extractor(query_imgs)      # [n_way*k_query, D]\n",
    "    \n",
    "        # Compute prototypes: mean of support embeddings per class.\n",
    "        prototypes = []\n",
    "        unique_labels = torch.unique(support_labels)\n",
    "        for cls in unique_labels:\n",
    "            cls_indices = (support_labels == cls).nonzero(as_tuple=True)[0]\n",
    "            cls_embeddings = support_embeddings[cls_indices]\n",
    "            prototype = cls_embeddings.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "        prototypes = torch.stack(prototypes)  # shape: [n_way, D]\n",
    "        \n",
    "        # Compute distances between query embeddings and prototypes.\n",
    "        # We use Euclidean distance here.\n",
    "        # query_embeddings: [Q, D], prototypes: [n_way, D]\n",
    "        distances = torch.cdist(query_embeddings, prototypes, p=2)  # shape: [Q, n_way]\n",
    "        \n",
    "        # Convert distances to probabilities (smaller distance -> higher probability).\n",
    "        probs = F.softmax(-distances, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        correct = (preds.cpu() == query_labels).sum().item()\n",
    "        total = query_labels.size(0)\n",
    "    \n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71a500",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "242bc81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Episode Accuracy: 36.14%\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_episodes = 50  # Evaluate on 50 episodes.\n",
    "\n",
    "for i, episode in enumerate(few_shot_loader):\n",
    "    if i >= num_episodes:\n",
    "        break\n",
    "    # Remove the extra batch dimension since batch_size=1.\n",
    "    (support_imgs, support_labels), (query_imgs, query_labels) = episode\n",
    "    support_imgs = support_imgs.squeeze(0)\n",
    "    support_labels = support_labels.squeeze(0)\n",
    "    query_imgs = query_imgs.squeeze(0)\n",
    "    query_labels = query_labels.squeeze(0)\n",
    "    \n",
    "    correct, total = evaluate_episode(base_fer_model_2, support_imgs, support_labels, query_imgs, query_labels, device)\n",
    "    total_correct += correct\n",
    "    total_samples += total\n",
    "\n",
    "episode_accuracy = 100.0 * total_correct / total_samples\n",
    "print(\"Few-Shot Episode Accuracy: {:.2f}%\".format(episode_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bad0b2",
   "metadata": {},
   "source": [
    "# Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38835e02",
   "metadata": {},
   "source": [
    "### Align Label spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "1286eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"angry\": 0,\n",
    "    \"disgust\": 1,\n",
    "    \"fear\": 2,\n",
    "    \"happy\": 3,\n",
    "    \"neutral\": 4,\n",
    "    \"sad\": 5,\n",
    "    \"surprise\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "017fa156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappedImageFolder(ImageFolder):\n",
    "    def __init__(self, root, label_map, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.samples = [\n",
    "            (path, label_map[self.classes[label]])\n",
    "            for path, label in self.samples\n",
    "            if self.classes[label] in label_map\n",
    "        ]\n",
    "        self.targets = [s[1] for s in self.samples]\n",
    "        \n",
    "        inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "        self.classes = [inverse_label_map[i] for i in sorted(inverse_label_map)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4c8e7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485]\n",
    "std=[0.229]\n",
    "\n",
    "manga_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),  # Keep 3 channels but use grayscale\n",
    "    T.RandomApply([T.GaussianBlur(3), T.RandomSolarize(0.5)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.4, p=0.3),\n",
    "    T.RandomApply([T.RandomRotation(15)], p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    T.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomErasing(p=0.2)  # Helps with occlusion\n",
    "])\n",
    "\n",
    "\n",
    "manga_faces_train_dir = Path(os.getcwd(), 'datasets', 'manga', 'train')\n",
    "manga_faces_train_images = MappedImageFolder(root=manga_faces_train_dir, label_map=label_map  , transform=manga_transforms)\n",
    "manga_faces_train_images_loader = DataLoader(manga_faces_train_images, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "# Test Set\n",
    "test_transforms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485], std=[0.229])\n",
    "])\n",
    "manga_faces_test_dir = Path(os.getcwd(), 'datasets', 'manga', 'test')\n",
    "manga_faces_test_images = MappedImageFolder(root=manga_faces_test_dir, label_map=label_map  , transform=test_transforms)\n",
    "manga_faces_test_images_loader = DataLoader(manga_faces_test_images, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c5504b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "print(set(manga_faces_test_images.targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cf8ef",
   "metadata": {},
   "source": [
    "### Contrastive Loss Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):  # Increased temperature\n",
    "        super().__init__()\n",
    "        self.tau = temperature # hyperparameter for scaling the similarity scores\n",
    "        \n",
    "    def forward(self, source_emb, source_labels, target_emb, target_labels):\n",
    "        device = source_emb.device\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        source_emb = F.normalize(source_emb, p=2, dim=1)\n",
    "        target_emb = F.normalize(target_emb, p=2, dim=1)\n",
    "        \n",
    "        embeddings = torch.cat([source_emb, target_emb], dim=0)\n",
    "        labels = torch.cat([source_labels, target_labels], dim=0)\n",
    "        \n",
    "        # Similarity matrix\n",
    "        sim_matrix = torch.mm(target_emb, embeddings.T) / self.tau\n",
    "        \n",
    "        # Masks\n",
    "        pos_mask = torch.zeros_like(sim_matrix, dtype=torch.bool)\n",
    "        for i, label in enumerate(target_labels):\n",
    "            pos_mask[i, :len(source_labels)] = (source_labels == label)\n",
    "            \n",
    "        neg_mask = (labels != target_labels.unsqueeze(1))\n",
    "        neg_mask[:, len(source_labels):] &= ~torch.eye(\n",
    "            len(target_labels), dtype=torch.bool, device=device\n",
    "        )\n",
    "        \n",
    "        # Compute terms with stability\n",
    "        pos_term = (sim_matrix.exp() * pos_mask.float()).sum(dim=1) + 1e-8\n",
    "        neg_term = (sim_matrix.exp() * neg_mask.float()).sum(dim=1) + 1e-8\n",
    "        \n",
    "        loss = -torch.log(pos_term / (pos_term + neg_term))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8dcecc",
   "metadata": {},
   "source": [
    "### Few-shot sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c38636a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot sampling function\n",
    "def get_few_shot_indices(dataset, shots_per_class=5):\n",
    "    \"\"\"\n",
    "    Returns a balanced list of indices for few-shot learning by randomly selecting\n",
    "    a fixed number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        dataset (ImageFolder): A PyTorch ImageFolder dataset (or any dataset with a `.samples` attribute \n",
    "                              containing (path, label) tuples).\n",
    "        shots_per_class (int, optional): Number of samples to select per class. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of selected indices, ensuring `shots_per_class` samples per class.\n",
    "\n",
    "    Example:\n",
    "        >>> target_set = ImageFolder(root='data/target', transform=transforms.ToTensor())\n",
    "        >>> few_shot_indices = get_few_shot_indices(target_set, shots_per_class=3)\n",
    "        >>> few_shot_loader = DataLoader(Subset(target_set, few_shot_indices), batch_size=3)\n",
    "    \"\"\"\n",
    "    \n",
    "    class_indices = {}\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class_indices.setdefault(label, []).append(idx)\n",
    "    \n",
    "    selected_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        selected_indices.extend(np.random.choice(indices, shots_per_class, replace=False))\n",
    "    return selected_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c296ef10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_contrastive = BaseFERModel_V2(params).to(device)\n",
    "model_contrastive = base_fer_model_2.to(device)\n",
    "model_contrastive.load_state_dict(torch.load('weights/base_model_with_fer2013_weights.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e7ff5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and losses\n",
    "params = {'num_classes': 7, 'dropout_rate': 0.2}  # Example for FER2013\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "cont_criterion = ContrastiveLoss(temperature=0.2)\n",
    "optimizer = torch.optim.AdamW(model_contrastive.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Prepare few-shot target loader training set\n",
    "few_shot_indices = get_few_shot_indices(manga_faces_train_images, shots_per_class=15)\n",
    "few_shot_loader = DataLoader(\n",
    "    Subset(manga_faces_train_images, few_shot_indices),\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    drop_last=True  # Avoid partial batches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfa3a3",
   "metadata": {},
   "source": [
    "### Modified Training Loop for Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699f9524",
   "metadata": {},
   "source": [
    "#### Using only CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5beff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38d4661",
   "metadata": {},
   "source": [
    "#### Using only ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ddf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4135265c",
   "metadata": {},
   "source": [
    "#### Using both CrossEntropyLoss and ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with domain adaptation\n",
    "def train_epoch(model, source_loader, target_loader, optimizer, epoch, epochs):\n",
    "    model.train()\n",
    "    target_iter = cycle(target_loader)  # Infinite iterator\n",
    "    \n",
    "    # Initialize metrics\n",
    "    total_cls_loss = 0.0\n",
    "    total_cont_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "    source_correct = 0\n",
    "    target_correct = 0\n",
    "    total_source_samples = 0\n",
    "    total_target_samples = 0\n",
    "    \n",
    "    \n",
    "    tk = tqdm(source_loader, desc=\"EPOCH\" + \"[TRAIN]\" + str(epoch) + \"/\" + str(epochs))\n",
    "    \n",
    "    for batch_idx, (source_imgs, source_lbls) in enumerate(tk):\n",
    "        # Get target batch\n",
    "        target_imgs, target_lbls = next(target_iter)\n",
    "        \n",
    "        # Move to device\n",
    "        source_imgs = source_imgs.to(device)\n",
    "        source_lbls = source_lbls.to(device)\n",
    "        target_imgs = target_imgs.to(device)\n",
    "        target_lbls = target_lbls.to(device)\n",
    "        \n",
    "        # Forward pass with embeddings\n",
    "        source_logits, source_emb = model(source_imgs, return_embeddings=True)\n",
    "        target_logits, target_emb = model(target_imgs, return_embeddings=True)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        source_preds = source_logits.argmax(dim=1)\n",
    "        target_preds = target_logits.argmax(dim=1)\n",
    "        \n",
    "        # Update counters\n",
    "        batch_source_correct = (source_preds == source_lbls).sum().item()\n",
    "        batch_target_correct = (target_preds == target_lbls).sum().item()\n",
    "        \n",
    "        source_correct += batch_source_correct\n",
    "        target_correct += batch_target_correct\n",
    "        total_source_samples += source_lbls.size(0)\n",
    "        total_target_samples += target_lbls.size(0)\n",
    "        \n",
    "        # Loss calculation\n",
    "        cls_loss = cls_criterion(source_logits, source_lbls) + \\\n",
    "                cls_criterion(target_logits, target_lbls)\n",
    "        \n",
    "        cont_loss = cont_criterion(source_emb, source_lbls,\n",
    "                                 target_emb, target_lbls)\n",
    "        \n",
    "        current_loss = cls_loss + 0.9 * cont_loss # Adjusted weight\n",
    "        \n",
    "        # Update metrics\n",
    "        total_cls_loss += cls_loss.item()\n",
    "        total_cont_loss += cont_loss.item() * 0.5 # Adjusted weight\n",
    "        running_total_loss += current_loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        current_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate batch-level accuracies\n",
    "        batch_source_acc = 100 * batch_source_correct / source_lbls.size(0)\n",
    "        batch_target_acc = 100 * batch_target_correct / target_lbls.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        tk.set_postfix({\n",
    "            'CLS Loss': f'{total_cls_loss / (batch_idx + 1):.4f}',\n",
    "            'CONT Loss': f'{total_cont_loss / (batch_idx + 1):.4f}',\n",
    "            'Total Loss': f'{running_total_loss / (batch_idx + 1):.4f}',\n",
    "            'Source Acc': f'{batch_source_acc:.2f}%',\n",
    "            'Target Acc': f'{batch_target_acc:.2f}%'\n",
    "        })\n",
    "            \n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_cls_loss = total_cls_loss / len(source_loader)\n",
    "    epoch_cont_loss = total_cont_loss / len(source_loader)\n",
    "    epoch_total_loss = running_total_loss / len(source_loader)\n",
    "    \n",
    "    epoch_source_acc = 100 * source_correct / total_source_samples\n",
    "    epoch_target_acc = 100 * target_correct / total_target_samples\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n",
    "    print(f\"CLS Loss: {epoch_cls_loss:.4f} | CONT Loss: {epoch_cont_loss:.4f} | Total Loss: {epoch_total_loss:.4f}\")\n",
    "    print(f\"Source Acc: {epoch_source_acc:.2f}% | Target Acc: {epoch_target_acc:.2f}%\")\n",
    "    \n",
    "    return epoch_cls_loss, epoch_cont_loss, epoch_total_loss, epoch_source_acc, epoch_target_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "99023746",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5135f",
   "metadata": {},
   "source": [
    "### Run Contrastive Loss Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2bb61f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/50: 100%|██████████| 225/225 [00:27<00:00,  8.04it/s, CLS Loss=2.8959, CONT Loss=0.8832, Total Loss=4.4857, Source Acc=62.16%, Target Acc=20.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 Summary:\n",
      "CLS Loss: 2.8959 | CONT Loss: 0.8832 | Total Loss: 4.4857\n",
      "Source Acc: 61.65% | Target Acc: 36.49%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/50: 100%|██████████| 225/225 [00:27<00:00,  8.21it/s, CLS Loss=2.7307, CONT Loss=0.8939, Total Loss=4.3397, Source Acc=64.86%, Target Acc=30.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 Summary:\n",
      "CLS Loss: 2.7307 | CONT Loss: 0.8939 | Total Loss: 4.3397\n",
      "Source Acc: 61.27% | Target Acc: 44.04%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/50: 100%|██████████| 225/225 [00:27<00:00,  8.23it/s, CLS Loss=2.5846, CONT Loss=0.8843, Total Loss=4.1763, Source Acc=51.35%, Target Acc=70.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 Summary:\n",
      "CLS Loss: 2.5846 | CONT Loss: 0.8843 | Total Loss: 4.1763\n",
      "Source Acc: 60.95% | Target Acc: 48.18%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/50: 100%|██████████| 225/225 [00:28<00:00,  7.84it/s, CLS Loss=2.5170, CONT Loss=0.8954, Total Loss=4.1287, Source Acc=54.05%, Target Acc=40.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 Summary:\n",
      "CLS Loss: 2.5170 | CONT Loss: 0.8954 | Total Loss: 4.1287\n",
      "Source Acc: 60.11% | Target Acc: 49.20%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/50: 100%|██████████| 225/225 [00:30<00:00,  7.49it/s, CLS Loss=2.4642, CONT Loss=0.8727, Total Loss=4.0352, Source Acc=62.16%, Target Acc=40.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 Summary:\n",
      "CLS Loss: 2.4642 | CONT Loss: 0.8727 | Total Loss: 4.0352\n",
      "Source Acc: 59.96% | Target Acc: 52.04%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/50: 100%|██████████| 225/225 [00:32<00:00,  6.87it/s, CLS Loss=2.4111, CONT Loss=0.8855, Total Loss=4.0049, Source Acc=64.86%, Target Acc=70.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 Summary:\n",
      "CLS Loss: 2.4111 | CONT Loss: 0.8855 | Total Loss: 4.0049\n",
      "Source Acc: 59.99% | Target Acc: 53.47%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/50: 100%|██████████| 225/225 [00:31<00:00,  7.23it/s, CLS Loss=2.4073, CONT Loss=0.8804, Total Loss=3.9920, Source Acc=56.76%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 Summary:\n",
      "CLS Loss: 2.4073 | CONT Loss: 0.8804 | Total Loss: 3.9920\n",
      "Source Acc: 60.00% | Target Acc: 54.27%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/50: 100%|██████████| 225/225 [00:32<00:00,  6.92it/s, CLS Loss=2.3836, CONT Loss=0.8597, Total Loss=3.9310, Source Acc=59.46%, Target Acc=40.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 Summary:\n",
      "CLS Loss: 2.3836 | CONT Loss: 0.8597 | Total Loss: 3.9310\n",
      "Source Acc: 59.85% | Target Acc: 52.31%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/50: 100%|██████████| 225/225 [00:32<00:00,  6.95it/s, CLS Loss=2.4524, CONT Loss=0.8732, Total Loss=4.0242, Source Acc=70.27%, Target Acc=50.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 Summary:\n",
      "CLS Loss: 2.4524 | CONT Loss: 0.8732 | Total Loss: 4.0242\n",
      "Source Acc: 59.79% | Target Acc: 53.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/50: 100%|██████████| 225/225 [00:33<00:00,  6.79it/s, CLS Loss=2.2866, CONT Loss=0.8651, Total Loss=3.8437, Source Acc=56.76%, Target Acc=40.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 Summary:\n",
      "CLS Loss: 2.2866 | CONT Loss: 0.8651 | Total Loss: 3.8437\n",
      "Source Acc: 59.67% | Target Acc: 57.60%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]11/50: 100%|██████████| 225/225 [00:32<00:00,  6.89it/s, CLS Loss=2.2942, CONT Loss=0.8754, Total Loss=3.8700, Source Acc=64.86%, Target Acc=30.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 Summary:\n",
      "CLS Loss: 2.2942 | CONT Loss: 0.8754 | Total Loss: 3.8700\n",
      "Source Acc: 59.97% | Target Acc: 60.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]12/50: 100%|██████████| 225/225 [00:31<00:00,  7.16it/s, CLS Loss=2.2747, CONT Loss=0.8644, Total Loss=3.8305, Source Acc=54.05%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 Summary:\n",
      "CLS Loss: 2.2747 | CONT Loss: 0.8644 | Total Loss: 3.8305\n",
      "Source Acc: 60.28% | Target Acc: 55.42%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]13/50: 100%|██████████| 225/225 [00:29<00:00,  7.76it/s, CLS Loss=2.3246, CONT Loss=0.8564, Total Loss=3.8661, Source Acc=78.38%, Target Acc=50.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 Summary:\n",
      "CLS Loss: 2.3246 | CONT Loss: 0.8564 | Total Loss: 3.8661\n",
      "Source Acc: 60.01% | Target Acc: 55.56%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]14/50: 100%|██████████| 225/225 [00:31<00:00,  7.09it/s, CLS Loss=2.2420, CONT Loss=0.8586, Total Loss=3.7875, Source Acc=45.95%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 Summary:\n",
      "CLS Loss: 2.2420 | CONT Loss: 0.8586 | Total Loss: 3.7875\n",
      "Source Acc: 60.05% | Target Acc: 59.29%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]15/50: 100%|██████████| 225/225 [00:33<00:00,  6.67it/s, CLS Loss=2.2296, CONT Loss=0.8542, Total Loss=3.7671, Source Acc=64.86%, Target Acc=50.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 Summary:\n",
      "CLS Loss: 2.2296 | CONT Loss: 0.8542 | Total Loss: 3.7671\n",
      "Source Acc: 60.40% | Target Acc: 56.31%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]16/50: 100%|██████████| 225/225 [00:32<00:00,  6.85it/s, CLS Loss=2.2178, CONT Loss=0.8633, Total Loss=3.7717, Source Acc=62.16%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 Summary:\n",
      "CLS Loss: 2.2178 | CONT Loss: 0.8633 | Total Loss: 3.7717\n",
      "Source Acc: 60.20% | Target Acc: 58.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]17/50: 100%|██████████| 225/225 [00:29<00:00,  7.76it/s, CLS Loss=2.1714, CONT Loss=0.8541, Total Loss=3.7089, Source Acc=62.16%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 Summary:\n",
      "CLS Loss: 2.1714 | CONT Loss: 0.8541 | Total Loss: 3.7089\n",
      "Source Acc: 60.69% | Target Acc: 57.64%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]18/50: 100%|██████████| 225/225 [00:29<00:00,  7.67it/s, CLS Loss=2.1758, CONT Loss=0.8462, Total Loss=3.6990, Source Acc=64.86%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 Summary:\n",
      "CLS Loss: 2.1758 | CONT Loss: 0.8462 | Total Loss: 3.6990\n",
      "Source Acc: 60.33% | Target Acc: 62.36%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]19/50: 100%|██████████| 225/225 [00:32<00:00,  7.03it/s, CLS Loss=2.1588, CONT Loss=0.8525, Total Loss=3.6933, Source Acc=56.76%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 Summary:\n",
      "CLS Loss: 2.1588 | CONT Loss: 0.8525 | Total Loss: 3.6933\n",
      "Source Acc: 60.43% | Target Acc: 63.11%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]20/50: 100%|██████████| 225/225 [00:34<00:00,  6.58it/s, CLS Loss=2.1123, CONT Loss=0.8401, Total Loss=3.6245, Source Acc=59.46%, Target Acc=70.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 Summary:\n",
      "CLS Loss: 2.1123 | CONT Loss: 0.8401 | Total Loss: 3.6245\n",
      "Source Acc: 60.54% | Target Acc: 61.96%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]21/50: 100%|██████████| 225/225 [00:32<00:00,  7.00it/s, CLS Loss=2.0671, CONT Loss=0.8440, Total Loss=3.5862, Source Acc=67.57%, Target Acc=90.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 Summary:\n",
      "CLS Loss: 2.0671 | CONT Loss: 0.8440 | Total Loss: 3.5862\n",
      "Source Acc: 60.51% | Target Acc: 68.18%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]22/50: 100%|██████████| 225/225 [00:31<00:00,  7.15it/s, CLS Loss=2.1468, CONT Loss=0.8416, Total Loss=3.6617, Source Acc=54.05%, Target Acc=40.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 Summary:\n",
      "CLS Loss: 2.1468 | CONT Loss: 0.8416 | Total Loss: 3.6617\n",
      "Source Acc: 60.90% | Target Acc: 60.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]23/50: 100%|██████████| 225/225 [00:31<00:00,  7.16it/s, CLS Loss=2.0855, CONT Loss=0.8560, Total Loss=3.6264, Source Acc=59.46%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 Summary:\n",
      "CLS Loss: 2.0855 | CONT Loss: 0.8560 | Total Loss: 3.6264\n",
      "Source Acc: 60.56% | Target Acc: 65.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]24/50: 100%|██████████| 225/225 [00:33<00:00,  6.81it/s, CLS Loss=2.2038, CONT Loss=0.8373, Total Loss=3.7109, Source Acc=56.76%, Target Acc=40.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 Summary:\n",
      "CLS Loss: 2.2038 | CONT Loss: 0.8373 | Total Loss: 3.7109\n",
      "Source Acc: 61.08% | Target Acc: 56.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]25/50: 100%|██████████| 225/225 [00:31<00:00,  7.18it/s, CLS Loss=2.1028, CONT Loss=0.8473, Total Loss=3.6280, Source Acc=56.76%, Target Acc=90.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 Summary:\n",
      "CLS Loss: 2.1028 | CONT Loss: 0.8473 | Total Loss: 3.6280\n",
      "Source Acc: 60.89% | Target Acc: 63.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]26/50: 100%|██████████| 225/225 [00:31<00:00,  7.17it/s, CLS Loss=2.0949, CONT Loss=0.8364, Total Loss=3.6005, Source Acc=64.86%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 Summary:\n",
      "CLS Loss: 2.0949 | CONT Loss: 0.8364 | Total Loss: 3.6005\n",
      "Source Acc: 60.71% | Target Acc: 66.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]27/50: 100%|██████████| 225/225 [00:31<00:00,  7.21it/s, CLS Loss=2.0417, CONT Loss=0.8411, Total Loss=3.5556, Source Acc=43.24%, Target Acc=90.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 Summary:\n",
      "CLS Loss: 2.0417 | CONT Loss: 0.8411 | Total Loss: 3.5556\n",
      "Source Acc: 61.24% | Target Acc: 65.51%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]28/50: 100%|██████████| 225/225 [00:31<00:00,  7.20it/s, CLS Loss=2.1430, CONT Loss=0.8357, Total Loss=3.6472, Source Acc=62.16%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 Summary:\n",
      "CLS Loss: 2.1430 | CONT Loss: 0.8357 | Total Loss: 3.6472\n",
      "Source Acc: 60.79% | Target Acc: 61.56%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]29/50: 100%|██████████| 225/225 [00:34<00:00,  6.55it/s, CLS Loss=2.1150, CONT Loss=0.8316, Total Loss=3.6118, Source Acc=54.05%, Target Acc=60.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 Summary:\n",
      "CLS Loss: 2.1150 | CONT Loss: 0.8316 | Total Loss: 3.6118\n",
      "Source Acc: 61.18% | Target Acc: 64.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]30/50: 100%|██████████| 225/225 [00:33<00:00,  6.65it/s, CLS Loss=2.0582, CONT Loss=0.8463, Total Loss=3.5816, Source Acc=48.65%, Target Acc=90.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 Summary:\n",
      "CLS Loss: 2.0582 | CONT Loss: 0.8463 | Total Loss: 3.5816\n",
      "Source Acc: 60.81% | Target Acc: 64.04%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]31/50: 100%|██████████| 225/225 [00:32<00:00,  6.96it/s, CLS Loss=2.0258, CONT Loss=0.8346, Total Loss=3.5280, Source Acc=59.46%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 Summary:\n",
      "CLS Loss: 2.0258 | CONT Loss: 0.8346 | Total Loss: 3.5280\n",
      "Source Acc: 60.44% | Target Acc: 68.27%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]32/50: 100%|██████████| 225/225 [00:29<00:00,  7.68it/s, CLS Loss=2.1134, CONT Loss=0.8368, Total Loss=3.6196, Source Acc=51.35%, Target Acc=70.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 Summary:\n",
      "CLS Loss: 2.1134 | CONT Loss: 0.8368 | Total Loss: 3.6196\n",
      "Source Acc: 61.22% | Target Acc: 63.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]33/50: 100%|██████████| 225/225 [00:29<00:00,  7.59it/s, CLS Loss=2.1221, CONT Loss=0.8327, Total Loss=3.6210, Source Acc=62.16%, Target Acc=50.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 Summary:\n",
      "CLS Loss: 2.1221 | CONT Loss: 0.8327 | Total Loss: 3.6210\n",
      "Source Acc: 60.95% | Target Acc: 62.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]34/50: 100%|██████████| 225/225 [00:31<00:00,  7.21it/s, CLS Loss=2.0285, CONT Loss=0.8178, Total Loss=3.5005, Source Acc=62.16%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 Summary:\n",
      "CLS Loss: 2.0285 | CONT Loss: 0.8178 | Total Loss: 3.5005\n",
      "Source Acc: 60.97% | Target Acc: 65.96%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]35/50: 100%|██████████| 225/225 [00:32<00:00,  6.92it/s, CLS Loss=2.0218, CONT Loss=0.8245, Total Loss=3.5058, Source Acc=72.97%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 Summary:\n",
      "CLS Loss: 2.0218 | CONT Loss: 0.8245 | Total Loss: 3.5058\n",
      "Source Acc: 60.95% | Target Acc: 68.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]36/50: 100%|██████████| 225/225 [00:30<00:00,  7.40it/s, CLS Loss=1.9484, CONT Loss=0.8276, Total Loss=3.4381, Source Acc=70.27%, Target Acc=50.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 Summary:\n",
      "CLS Loss: 1.9484 | CONT Loss: 0.8276 | Total Loss: 3.4381\n",
      "Source Acc: 60.74% | Target Acc: 69.73%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]37/50: 100%|██████████| 225/225 [00:28<00:00,  7.82it/s, CLS Loss=2.0182, CONT Loss=0.8300, Total Loss=3.5122, Source Acc=51.35%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 Summary:\n",
      "CLS Loss: 2.0182 | CONT Loss: 0.8300 | Total Loss: 3.5122\n",
      "Source Acc: 60.68% | Target Acc: 66.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]38/50: 100%|██████████| 225/225 [00:31<00:00,  7.25it/s, CLS Loss=2.0012, CONT Loss=0.8254, Total Loss=3.4869, Source Acc=56.76%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 Summary:\n",
      "CLS Loss: 2.0012 | CONT Loss: 0.8254 | Total Loss: 3.4869\n",
      "Source Acc: 61.00% | Target Acc: 66.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]39/50: 100%|██████████| 225/225 [00:31<00:00,  7.20it/s, CLS Loss=2.0120, CONT Loss=0.8330, Total Loss=3.5114, Source Acc=59.46%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 Summary:\n",
      "CLS Loss: 2.0120 | CONT Loss: 0.8330 | Total Loss: 3.5114\n",
      "Source Acc: 61.09% | Target Acc: 67.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]40/50: 100%|██████████| 225/225 [00:30<00:00,  7.39it/s, CLS Loss=1.9577, CONT Loss=0.8169, Total Loss=3.4281, Source Acc=56.76%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 Summary:\n",
      "CLS Loss: 1.9577 | CONT Loss: 0.8169 | Total Loss: 3.4281\n",
      "Source Acc: 60.64% | Target Acc: 68.93%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]41/50: 100%|██████████| 225/225 [00:32<00:00,  6.85it/s, CLS Loss=2.0432, CONT Loss=0.8287, Total Loss=3.5348, Source Acc=64.86%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 Summary:\n",
      "CLS Loss: 2.0432 | CONT Loss: 0.8287 | Total Loss: 3.5348\n",
      "Source Acc: 60.75% | Target Acc: 65.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]42/50: 100%|██████████| 225/225 [00:32<00:00,  7.01it/s, CLS Loss=2.0161, CONT Loss=0.8274, Total Loss=3.5054, Source Acc=48.65%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 Summary:\n",
      "CLS Loss: 2.0161 | CONT Loss: 0.8274 | Total Loss: 3.5054\n",
      "Source Acc: 60.79% | Target Acc: 66.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]43/50: 100%|██████████| 225/225 [00:31<00:00,  7.23it/s, CLS Loss=1.9392, CONT Loss=0.8235, Total Loss=3.4216, Source Acc=56.76%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 Summary:\n",
      "CLS Loss: 1.9392 | CONT Loss: 0.8235 | Total Loss: 3.4216\n",
      "Source Acc: 60.98% | Target Acc: 70.04%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]44/50: 100%|██████████| 225/225 [00:31<00:00,  7.17it/s, CLS Loss=2.0085, CONT Loss=0.8269, Total Loss=3.4969, Source Acc=67.57%, Target Acc=90.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 Summary:\n",
      "CLS Loss: 2.0085 | CONT Loss: 0.8269 | Total Loss: 3.4969\n",
      "Source Acc: 61.26% | Target Acc: 68.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]45/50: 100%|██████████| 225/225 [00:30<00:00,  7.37it/s, CLS Loss=1.9066, CONT Loss=0.8172, Total Loss=3.3775, Source Acc=51.35%, Target Acc=90.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 Summary:\n",
      "CLS Loss: 1.9066 | CONT Loss: 0.8172 | Total Loss: 3.3775\n",
      "Source Acc: 60.97% | Target Acc: 71.02%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]46/50: 100%|██████████| 225/225 [00:29<00:00,  7.71it/s, CLS Loss=2.0162, CONT Loss=0.8160, Total Loss=3.4850, Source Acc=62.16%, Target Acc=80.00%] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 Summary:\n",
      "CLS Loss: 2.0162 | CONT Loss: 0.8160 | Total Loss: 3.4850\n",
      "Source Acc: 61.09% | Target Acc: 66.67%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]47/50: 100%|██████████| 225/225 [00:28<00:00,  7.85it/s, CLS Loss=1.8901, CONT Loss=0.8264, Total Loss=3.3776, Source Acc=62.16%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 Summary:\n",
      "CLS Loss: 1.8901 | CONT Loss: 0.8264 | Total Loss: 3.3776\n",
      "Source Acc: 61.32% | Target Acc: 71.73%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]48/50: 100%|██████████| 225/225 [00:27<00:00,  8.09it/s, CLS Loss=1.8586, CONT Loss=0.8165, Total Loss=3.3284, Source Acc=56.76%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 Summary:\n",
      "CLS Loss: 1.8586 | CONT Loss: 0.8165 | Total Loss: 3.3284\n",
      "Source Acc: 60.87% | Target Acc: 74.27%\n",
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]49/50: 100%|██████████| 225/225 [00:29<00:00,  7.67it/s, CLS Loss=1.9041, CONT Loss=0.8141, Total Loss=3.3694, Source Acc=72.97%, Target Acc=100.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 Summary:\n",
      "CLS Loss: 1.9041 | CONT Loss: 0.8141 | Total Loss: 3.3694\n",
      "Source Acc: 61.30% | Target Acc: 71.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]50/50: 100%|██████████| 225/225 [00:27<00:00,  8.06it/s, CLS Loss=1.9370, CONT Loss=0.8253, Total Loss=3.4225, Source Acc=67.57%, Target Acc=100.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 Summary:\n",
      "CLS Loss: 1.9370 | CONT Loss: 0.8253 | Total Loss: 3.4225\n",
      "Source Acc: 60.98% | Target Acc: 71.02%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contrastive_loss_metrics = {\n",
    "    'cls_loss': [],\n",
    "    'cont_loss': [],\n",
    "    'total_loss': [],\n",
    "    'source_accuracy': [],\n",
    "    'target_accuracy': []\n",
    "}\n",
    "best_valid_loss = np.inf\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    cls_loss, cont_loss, total_loss, source_acc, target_acc = \\\n",
    "        train_epoch(model_contrastive, training_loader, few_shot_loader, optimizer, epoch, EPOCHS)\n",
    "    \n",
    "    \n",
    "    contrastive_loss_metrics['cls_loss'].append(cls_loss)\n",
    "    contrastive_loss_metrics['cont_loss'].append(cont_loss)\n",
    "    contrastive_loss_metrics['total_loss'].append(total_loss)\n",
    "    contrastive_loss_metrics['source_accuracy'].append(source_acc)\n",
    "    contrastive_loss_metrics['target_accuracy'].append(target_acc)\n",
    "    \n",
    "    if total_loss < best_valid_loss:\n",
    "        torch.save(model_contrastive.state_dict(), 'weights/base_model_contrastive_learning_weights.pt')\n",
    "        print(\"SAVED-BEST-WEIGHTS!\")\n",
    "        best_valid_loss = total_loss\n",
    "        #patience_counter = 0 # Reset early stopping\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65bcda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "95fd4686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]1/10: 100%|██████████| 5/5 [00:00<00:00, 11.29it/s, loss=1.444562, acc=0.556250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]2/10: 100%|██████████| 5/5 [00:00<00:00, 14.09it/s, loss=1.575016, acc=0.425000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]3/10: 100%|██████████| 5/5 [00:00<00:00, 14.91it/s, loss=1.677868, acc=0.468750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]4/10: 100%|██████████| 5/5 [00:00<00:00, 18.03it/s, loss=1.636436, acc=0.512500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]5/10: 100%|██████████| 5/5 [00:00<00:00, 25.48it/s, loss=1.701019, acc=0.425000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]6/10: 100%|██████████| 5/5 [00:00<00:00, 26.61it/s, loss=1.702232, acc=0.425000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]7/10: 100%|██████████| 5/5 [00:00<00:00, 27.08it/s, loss=1.713848, acc=0.425000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]8/10: 100%|██████████| 5/5 [00:00<00:00, 22.94it/s, loss=1.858013, acc=0.381250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]9/10: 100%|██████████| 5/5 [00:00<00:00, 24.50it/s, loss=1.714138, acc=0.468750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[VALID]10/10: 100%|██████████| 5/5 [00:00<00:00, 25.56it/s, loss=1.519675, acc=0.512500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tEST the model:\n",
    "cont_test_out_of_distribution_metrics = test_out_of_distribution(model_contrastive, manga_faces_test_images_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3eea2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, target=False):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            imgs = imgs.to(device)\n",
    "            lbls = lbls.to(device)\n",
    "            \n",
    "            logits, _ = model(imgs, return_embeddings=True)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(lbls.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    class_report = classification_report(all_labels, all_preds, zero_division=0)\n",
    "    \n",
    "    print(f\"{'Target' if target else 'Source'} Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    return accuracy, class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90fce5",
   "metadata": {},
   "source": [
    "#### Test on Source Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7714becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 57/57 [00:05<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Test Accuracy: 61.42%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.66      0.53       958\n",
      "           1       0.55      0.35      0.43       111\n",
      "           2       0.54      0.26      0.35      1024\n",
      "           3       0.79      0.86      0.82      1774\n",
      "           4       0.57      0.63      0.59      1233\n",
      "           5       0.55      0.41      0.47      1247\n",
      "           6       0.69      0.80      0.74       831\n",
      "\n",
      "    accuracy                           0.61      7178\n",
      "   macro avg       0.59      0.57      0.56      7178\n",
      "weighted avg       0.61      0.61      0.60      7178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "source_test_accuracy, source_report = evaluate_model(model_contrastive, test_loader)\n",
    "# Contrastive learning model does not forget the source domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d965a2",
   "metadata": {},
   "source": [
    "#### Test on Target Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "46a78bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5/5 [00:00<00:00, 15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Test Accuracy: 46.21%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.43      0.44        21\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.70      0.80      0.74        49\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.40      0.36      0.38        22\n",
      "           6       0.62      0.12      0.21        40\n",
      "\n",
      "    accuracy                           0.46       132\n",
      "   macro avg       0.36      0.29      0.30       132\n",
      "weighted avg       0.59      0.46      0.47       132\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_test_accuracy, target_report = evaluate_model(model_contrastive, manga_faces_test_images_loader, target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c5fba147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'angry': 0,\n",
       " 'disgust': 1,\n",
       " 'fear': 2,\n",
       " 'happy': 3,\n",
       " 'neutral': 4,\n",
       " 'sad': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513c430",
   "metadata": {},
   "source": [
    "- Align labels in source and target (limitation)\n",
    "- Split Manga dataset into train and test set  (limitation of small data set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe02458",
   "metadata": {},
   "source": [
    "The initial labels that were similar across both data sets include Angry, Happy, and Sad.\n",
    "\n",
    "The remaining labels in the Manga faces data set include Crying, Embarrassd, Pleased, and Shock\n",
    "The remaining labels in the FER 2013 data set include Disgust, Fear, Neutral, and Surprise\n",
    "\n",
    "\n",
    "I will rename the 'Shock' label in Manga to Surprise. \\\n",
    "I will combine the Pleased label with the Happy label of the Manga faces dataset \\\n",
    "Also, I will rename the Embarrassed label in Manga faces to \n",
    "\n",
    "After this, the new labels in the Manga faces become: Angry, Happy (+Pleased), Sad, and Surprise (Shock)\n",
    "\n",
    "The new data set will then be split into 60% for training and 40% for validation\n",
    "\n",
    "Small testing Manga faces data set is a limitation\n",
    "\n",
    "FUTURE WORK: More experiments with temperature and contrastive loss weighting.\n",
    "experiment on loss functions and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4359449",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
