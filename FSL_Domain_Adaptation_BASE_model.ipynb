{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73db0601",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a374da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69d7f8",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d415bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070573d",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5689ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.Normalize(mean, std),\n",
    "    T.RandomAffine(degrees=0, shear=0.2, scale=(0.8, 1.2))\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f49a1",
   "metadata": {},
   "source": [
    "## DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df5c89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fer_2013_dir = Path(os.getcwd(), 'datasets', 'fer2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97db57d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = ImageFolder(root=fer_2013_dir / 'train', transform=train_transforms)\n",
    "training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_set = ImageFolder(root=fer_2013_dir / 'test', transform=val_transforms)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40020c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 28709 images\n",
      "Testing set: 7178 images\n",
      "One image batch shape : torch.Size([128, 3, 48, 48])\n",
      "One label batch shape : torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Print shape of training and testing images\n",
    "print(f\"Training set: {len(training_set)} images\")\n",
    "print(f\"Testing set: {len(test_set)} images\")\n",
    "\n",
    "for images, labels in training_loader:\n",
    "  break\n",
    "\n",
    "print(f\"One image batch shape : {images.shape}\")\n",
    "print(f\"One label batch shape : {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17391128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "print(training_set.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08b7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', \n",
    "    4: 'neutral', 5: 'sad', 6: 'surprise'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3867fc1",
   "metadata": {},
   "source": [
    "#### Show sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0b98a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch\n",
    "images, labels = next(iter(training_loader))\n",
    "\n",
    "# Unnormalize a few images from the batch\n",
    "def unnormalize(img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return img * std + mean\n",
    "\n",
    "# Display first 4 images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    img = unnormalize(images[i])\n",
    "    img = img.permute(1, 2, 0)  # (C, H, W) -> (H, W, C)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Label: {label_map[int(labels[i])]}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253eda5",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da6d19",
   "metadata": {},
   "source": [
    "### Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e811ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFERModel(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(BaseFERModel, self).__init__()\n",
    "        num_classes = params['num_classes']\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) # Output: 32x24x24\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 48, 48)\n",
    "            out = self.layer1(dummy_input)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = self.layer4(out)\n",
    "            out = self.layer5(out)\n",
    "            out = self.layer6(out)\n",
    "            flattened_size = out.view(1, -1).size(1)\n",
    "\n",
    "\n",
    "        # Fully connected (dense) layers\n",
    "        #self.fc1 = nn.Linear(in_features=1024*1*1, out_features=1024)\n",
    "        self.fc1 = nn.Linear(in_features=flattened_size, out_features=1024)\n",
    "        self.dropout = nn.Dropout(params['dropout_rate'])\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=num_classes)\n",
    "       \n",
    "        #self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer1(x)  #  first layer\n",
    "        output = self.layer2(output)  # second layer\n",
    "        output = self.layer3(output)  # third layer\n",
    "        output = self.layer4(output)  # fourth layer\n",
    "        output = self.layer5(output)  # fifth layer\n",
    "        output = self.layer6(output)  # sixth layer        \n",
    "        output = output.view(output.size(0), -1)  # Flatten for the fully connected layers\n",
    "        output = self.fc1(output)  # 1st dense layer\n",
    "        output = self.dropout(output)  # Apply dropout\n",
    "        output = self.fc2(output)  # Second dense layer\n",
    "        output = self.fc3(output)  # Output layer\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41261683",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2f5287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "BaseFERModel                             [128, 7]                  --\n",
      "├─Sequential: 1-1                        [128, 32, 24, 24]         --\n",
      "│    └─Conv2d: 2-1                       [128, 32, 48, 48]         896\n",
      "│    └─BatchNorm2d: 2-2                  [128, 32, 48, 48]         64\n",
      "│    └─ReLU: 2-3                         [128, 32, 48, 48]         --\n",
      "│    └─MaxPool2d: 2-4                    [128, 32, 24, 24]         --\n",
      "├─Sequential: 1-2                        [128, 64, 12, 12]         --\n",
      "│    └─Conv2d: 2-5                       [128, 64, 24, 24]         18,496\n",
      "│    └─BatchNorm2d: 2-6                  [128, 64, 24, 24]         128\n",
      "│    └─ReLU: 2-7                         [128, 64, 24, 24]         --\n",
      "│    └─MaxPool2d: 2-8                    [128, 64, 12, 12]         --\n",
      "├─Sequential: 1-3                        [128, 128, 6, 6]          --\n",
      "│    └─Conv2d: 2-9                       [128, 128, 12, 12]        73,856\n",
      "│    └─BatchNorm2d: 2-10                 [128, 128, 12, 12]        256\n",
      "│    └─ReLU: 2-11                        [128, 128, 12, 12]        --\n",
      "│    └─MaxPool2d: 2-12                   [128, 128, 6, 6]          --\n",
      "├─Sequential: 1-4                        [128, 256, 6, 6]          --\n",
      "│    └─Conv2d: 2-13                      [128, 256, 6, 6]          295,168\n",
      "│    └─BatchNorm2d: 2-14                 [128, 256, 6, 6]          512\n",
      "│    └─ReLU: 2-15                        [128, 256, 6, 6]          --\n",
      "├─Sequential: 1-5                        [128, 512, 6, 6]          --\n",
      "│    └─Conv2d: 2-16                      [128, 512, 6, 6]          1,180,160\n",
      "│    └─BatchNorm2d: 2-17                 [128, 512, 6, 6]          1,024\n",
      "│    └─ReLU: 2-18                        [128, 512, 6, 6]          --\n",
      "├─Sequential: 1-6                        [128, 1024, 6, 6]         --\n",
      "│    └─Conv2d: 2-19                      [128, 1024, 6, 6]         4,719,616\n",
      "│    └─BatchNorm2d: 2-20                 [128, 1024, 6, 6]         2,048\n",
      "│    └─ReLU: 2-21                        [128, 1024, 6, 6]         --\n",
      "├─Linear: 1-7                            [128, 1024]               37,749,760\n",
      "├─Dropout: 1-8                           [128, 1024]               --\n",
      "├─Linear: 1-9                            [128, 256]                262,400\n",
      "├─Linear: 1-10                           [128, 7]                  1,799\n",
      "==========================================================================================\n",
      "Total params: 44,306,183\n",
      "Trainable params: 44,306,183\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 36.40\n",
      "==========================================================================================\n",
      "Input size (MB): 3.54\n",
      "Forward/backward pass size (MB): 397.68\n",
      "Params size (MB): 177.22\n",
      "Estimated Total Size (MB): 578.44\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "        \"initial_filters\": 8,    \n",
    "        \"dropout_rate\": 0.2,\n",
    "        \"num_classes\": 7}\n",
    "\n",
    "base_fer_model = BaseFERModel(params).to(device)\n",
    "print(summary(base_fer_model, input_size=(BATCH_SIZE, 3, 48, 48), device=device.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dc8a14",
   "metadata": {},
   "source": [
    "# Create Train and Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf63633",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cee4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred,y_true):\n",
    "    top_p,top_class = y_pred.topk(1, dim = 1)\n",
    "    equals = top_class == y_true.view(*top_class.shape)\n",
    "    return torch.mean(equals.type(torch.cuda.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3b04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Train one epoch of the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The  model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[TRAIN]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    for t, data in enumerate(tk):\n",
    "        images, labels = data\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute log probabilities from model\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for logging; Total loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "\n",
    "        # Print/log training loss and accuracy for this epoch\n",
    "        tk.set_postfix({\n",
    "            'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "            'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "        })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7824849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion, device, current_epoch, epochs):\n",
    "    \"\"\"\n",
    "    Test one epoch of the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        training_loss (float): Returns epoch_loss / len(dataloader)\n",
    "        \n",
    "        running_acc (float): Returns epoch accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    tk = tqdm(dataloader, desc=\"EPOCH\" + \"[VALID]\" + str(current_epoch + 1) + \"/\" + str(epochs))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation for testing\n",
    "        for t, data in enumerate(tk):          \n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Compute log probabilities from model\n",
    "            logits = model(images)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)            \n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate loss for logging; Total loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            epoch_accuracy += calculate_accuracy(logits, labels)\n",
    "            \n",
    "\n",
    "            tk.set_postfix({\n",
    "                'loss': '%6f' % float(epoch_loss / (t + 1)), \n",
    "                'acc': '%6f' % float(epoch_accuracy / (t + 1))\n",
    "            })\n",
    "\n",
    "    return epoch_loss / len(dataloader), epoch_accuracy / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b39d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate_model(model, training_dataloader, testing_dataloader, epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0   # Tracks the number of epochs without improvement\n",
    "    early_stop = False # Flag to indicate whether to stop training\n",
    "    save_weights_patience = 3\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'training_loss': [],\n",
    "        'training_accuracy': [],\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, LR: {scheduler.optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        # Training step\n",
    "        train_loss, train_accuracy = train_one_epoch(model, training_dataloader, criterion, optimizer, device, epoch, epochs)\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "\n",
    "        history_metrics['training_loss'].append(train_loss)\n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['training_accuracy'].append(train_accuracy)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "\n",
    "        # Update the learning rate based on validation loss and print\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model.state_dict(), 'weights/base_model_with_fer2013_weights.pt')\n",
    "            print(\"SAVED-BEST-WEIGHTS!\")\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0 # Reset early stopping\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss for {patience_counter} epoch(s).\")\n",
    "\n",
    "        if patience_counter >= save_weights_patience:\n",
    "            print(\"Patience exceeded. Early stopping at epoch \" +str(epoch + 1))\n",
    "            early_stop = True\n",
    "            \n",
    "        \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf1b19",
   "metadata": {},
   "source": [
    "### Run train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53dfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e820802d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]1/20: 100%|██████████| 225/225 [03:07<00:00,  1.20it/s, loss=1.716999, acc=0.307104]\n",
      "EPOCH[VALID]1/20: 100%|██████████| 57/57 [00:48<00:00,  1.17it/s, loss=1.684154, acc=32.056283%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 2, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]2/20: 100%|██████████| 225/225 [01:08<00:00,  3.29it/s, loss=1.688028, acc=0.328071]\n",
      "EPOCH[VALID]2/20: 100%|██████████| 57/57 [00:03<00:00, 16.63it/s, loss=1.651268, acc=34.786849%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 3, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]3/20: 100%|██████████| 225/225 [01:08<00:00,  3.31it/s, loss=1.649106, acc=0.350827]\n",
      "EPOCH[VALID]3/20: 100%|██████████| 57/57 [00:03<00:00, 17.45it/s, loss=1.614435, acc=37.210922%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 4, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]4/20: 100%|██████████| 225/225 [01:06<00:00,  3.37it/s, loss=1.613972, acc=0.369062]\n",
      "EPOCH[VALID]4/20: 100%|██████████| 57/57 [00:03<00:00, 17.40it/s, loss=1.571299, acc=39.997214%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 5, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]5/20: 100%|██████████| 225/225 [01:06<00:00,  3.37it/s, loss=1.580284, acc=0.387843]\n",
      "EPOCH[VALID]5/20: 100%|██████████| 57/57 [00:03<00:00, 16.78it/s, loss=1.533269, acc=41.097799%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 6, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]6/20: 100%|██████████| 225/225 [01:08<00:00,  3.30it/s, loss=1.547030, acc=0.403514]\n",
      "EPOCH[VALID]6/20: 100%|██████████| 57/57 [00:03<00:00, 16.19it/s, loss=1.508516, acc=42.184452%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 7, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]7/20: 100%|██████████| 225/225 [01:09<00:00,  3.26it/s, loss=1.525042, acc=0.414353]\n",
      "EPOCH[VALID]7/20: 100%|██████████| 57/57 [00:02<00:00, 19.01it/s, loss=1.530009, acc=40.944553%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 8, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]8/20: 100%|██████████| 225/225 [01:09<00:00,  3.23it/s, loss=1.499660, acc=0.426612]\n",
      "EPOCH[VALID]8/20: 100%|██████████| 57/57 [00:03<00:00, 14.73it/s, loss=1.462099, acc=44.552800%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 9, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]9/20: 100%|██████████| 225/225 [01:08<00:00,  3.27it/s, loss=1.475381, acc=0.438438]\n",
      "EPOCH[VALID]9/20: 100%|██████████| 57/57 [00:03<00:00, 14.48it/s, loss=1.454913, acc=44.594595%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 10, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]10/20: 100%|██████████| 225/225 [01:08<00:00,  3.28it/s, loss=1.462060, acc=0.437699]\n",
      "EPOCH[VALID]10/20: 100%|██████████| 57/57 [00:03<00:00, 18.79it/s, loss=1.415088, acc=46.085261%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 11, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]11/20: 100%|██████████| 225/225 [01:08<00:00,  3.29it/s, loss=1.445405, acc=0.447534]\n",
      "EPOCH[VALID]11/20: 100%|██████████| 57/57 [00:03<00:00, 15.62it/s, loss=1.414881, acc=46.099192%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 12, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]12/20: 100%|██████████| 225/225 [01:08<00:00,  3.27it/s, loss=1.426542, acc=0.453503]\n",
      "EPOCH[VALID]12/20: 100%|██████████| 57/57 [00:04<00:00, 13.34it/s, loss=1.399403, acc=46.990805%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 13, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]13/20: 100%|██████████| 225/225 [01:08<00:00,  3.28it/s, loss=1.414235, acc=0.462434]\n",
      "EPOCH[VALID]13/20: 100%|██████████| 57/57 [00:03<00:00, 18.74it/s, loss=1.379496, acc=48.161048%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 14, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]14/20: 100%|██████████| 225/225 [01:08<00:00,  3.28it/s, loss=1.397333, acc=0.469395]\n",
      "EPOCH[VALID]14/20: 100%|██████████| 57/57 [00:03<00:00, 14.97it/s, loss=1.379730, acc=48.021733%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 15, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]15/20: 100%|██████████| 225/225 [01:08<00:00,  3.30it/s, loss=1.381862, acc=0.473557]\n",
      "EPOCH[VALID]15/20: 100%|██████████| 57/57 [00:03<00:00, 17.61it/s, loss=1.352393, acc=48.397882%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 16, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]16/20: 100%|██████████| 225/225 [01:09<00:00,  3.24it/s, loss=1.370329, acc=0.478233]\n",
      "EPOCH[VALID]16/20: 100%|██████████| 57/57 [00:03<00:00, 18.06it/s, loss=1.356044, acc=49.233770%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 17, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]17/20: 100%|██████████| 225/225 [01:08<00:00,  3.28it/s, loss=1.357632, acc=0.484988]\n",
      "EPOCH[VALID]17/20: 100%|██████████| 57/57 [00:03<00:00, 17.36it/s, loss=1.362917, acc=48.801895%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 2 epoch(s).\n",
      "Epoch 18, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]18/20: 100%|██████████| 225/225 [01:08<00:00,  3.30it/s, loss=1.345226, acc=0.486115]\n",
      "EPOCH[VALID]18/20: 100%|██████████| 57/57 [00:04<00:00, 13.50it/s, loss=1.316862, acc=50.097520%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "Epoch 19, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]19/20: 100%|██████████| 225/225 [01:08<00:00,  3.30it/s, loss=1.337795, acc=0.488591]\n",
      "EPOCH[VALID]19/20: 100%|██████████| 57/57 [00:03<00:00, 17.93it/s, loss=1.330099, acc=49.484536%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in validation loss for 1 epoch(s).\n",
      "Epoch 20, LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH[TRAIN]20/20: 100%|██████████| 225/225 [01:08<00:00,  3.29it/s, loss=1.319093, acc=0.498706]\n",
      "EPOCH[VALID]20/20: 100%|██████████| 57/57 [00:03<00:00, 18.01it/s, loss=1.303772, acc=50.431875%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED-BEST-WEIGHTS!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model:\n",
    "base_model_losses = train_and_validate_model(base_fer_model, training_loader, test_loader, epochs=20, learning_rate=0.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971550d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time2 = datetime.now()\n",
    "print(\"Total training time: \", time2 - time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6722ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = {\n",
    "    \"Epoch\": list(range(1, len(base_model_losses['training_loss']) + 1)),\n",
    "    \"Training Loss\": base_model_losses['training_loss'],\n",
    "    \"Validation Loss\": base_model_losses['validation_loss'],\n",
    "    \"Training Accuracy\": [acc.cpu().item() for acc in base_model_losses['training_accuracy']],\n",
    "    \"Validation Accuracy\": [acc.cpu().item() for acc in base_model_losses['validation_accuracy']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"stats/resnet50_model_stats_001_100 epochs.csv\", index=False)\n",
    "print(\"Losses and accuracy saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143882d",
   "metadata": {},
   "source": [
    "# Test Model Accuracy on Out of Distribution Data set (Manga Faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "846e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out_of_distribution(model, testing_dataloader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train and Test the speech recognition model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model.\n",
    "        training_dataloader (DataLoader): DataLoader for training data.\n",
    "        testing_dataloader (DataLoader): DataLoader for testing data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        device (torch.device): Device to train the model on (CPU/GPU).\n",
    "    \"\"\"\n",
    "    # Define Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Dictionary to store loss and accuracy values over epochs\n",
    "    history_metrics = {\n",
    "        'validation_loss': [],\n",
    "        'validation_accuracy': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "        # Testing step\n",
    "        valid_loss, valid_accuracy = test_one_epoch(model, testing_dataloader, criterion, device, epoch, epochs) \n",
    "        \n",
    "        history_metrics['validation_loss'].append(valid_loss)\n",
    "        history_metrics['validation_accuracy'].append(valid_accuracy)\n",
    "                \n",
    "    print(\"\")\n",
    "    return history_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0084c040",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BaseFERModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 36864]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m base_fer_model_2 \u001b[38;5;241m=\u001b[39m BaseFERModel(params)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m base_fer_model_2 \u001b[38;5;241m=\u001b[39m base_fer_model_2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mbase_fer_model_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights/base_model_with_fer2013_weights_6_LAYERS.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oreda\\.conda\\envs\\cpe520\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BaseFERModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 36864])."
     ]
    }
   ],
   "source": [
    "base_fer_model_2 = BaseFERModel(params).to(device)\n",
    "base_fer_model_2 = base_fer_model_2.to(device)\n",
    "base_fer_model_2.load_state_dict(torch.load('weights/base_model_with_fer2013_weights_6_LAYERS.pt', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c985db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_faces_dir = Path(os.getcwd(), 'datasets', 'manga')\n",
    "manga_faces_images = ImageFolder(root=manga_faces_dir, transform=val_transforms)\n",
    "manga_faces_images_loader = DataLoader(manga_faces_images, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "test_out_of_distribution_losses = test_out_of_distribution(base_fer_model_2, manga_faces_images_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e436afd",
   "metadata": {},
   "source": [
    "## Import MangaFaces Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14f8b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_faces_dir = Path(os.getcwd(), 'datasets', 'manga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d39f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_faces_test_set = ImageFolder(root=fer_2013_dir / 'test', transform=val_transforms)\n",
    "test_loader = DataLoader(manga_faces_test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680836f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c183a06",
   "metadata": {},
   "source": [
    "# FSL DA Prototypical Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556c60f",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15233c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotFERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for few-shot FER, where images are organized by class in folders.\n",
    "    This dataset generates episodes (tasks) on-the-fly.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, n_way=5, k_shot=1, k_query=5, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Root folder containing one folder per class.\n",
    "        n_way: number of classes per episode.\n",
    "        k_shot: number of support examples per class.\n",
    "        k_query: number of query examples per class.\n",
    "        transform: transformation to apply to images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.k_query = k_query\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Build a mapping: class -> list of image paths.\n",
    "        self.class_to_imgs = {}\n",
    "        for cls_name in os.listdir(root_dir):\n",
    "            cls_folder = Path.joinpath(root_dir, cls_name)\n",
    "            if Path.is_dir(cls_folder):\n",
    "                self.class_to_imgs[cls_name] = [Path.joinpath(cls_folder, img)                                                 \n",
    "                                                 for img in Path(cls_folder).rglob('*')\n",
    "                                                 if img.endswith('.jpg') or img.endswith('.png')]        \n",
    "        self.classes = list(self.class_to_imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Define the number of episodes arbitrarily.\n",
    "        return 1000  # or any number representing episodes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly sample n_way classes for this episode.\n",
    "        sampled_classes = random.sample(self.classes, self.n_way)\n",
    "        support_imgs, support_labels = [], []\n",
    "        query_imgs, query_labels = [], []\n",
    "        \n",
    "        label_map = {cls_name: i for i, cls_name in enumerate(sampled_classes)}\n",
    "        \n",
    "        for cls_name in sampled_classes:\n",
    "            imgs = self.class_to_imgs[cls_name]\n",
    "            # Ensure there are enough examples in this class.\n",
    "            selected_imgs = random.sample(imgs, self.k_shot + self.k_query)\n",
    "            support_paths = selected_imgs[:self.k_shot]\n",
    "            query_paths = selected_imgs[self.k_shot:]\n",
    "            \n",
    "            for sp in support_paths:\n",
    "                img = Image.open(sp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                support_imgs.append(img)\n",
    "                support_labels.append(label_map[cls_name])\n",
    "            \n",
    "            for qp in query_paths:\n",
    "                img = Image.open(qp).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                query_imgs.append(img)\n",
    "                query_labels.append(label_map[cls_name])\n",
    "        \n",
    "        # Convert lists to tensors.\n",
    "        support_imgs = torch.stack(support_imgs)  # shape: [n_way*k_shot, C, H, W]\n",
    "        support_labels = torch.tensor(support_labels, dtype=torch.long)\n",
    "        query_imgs = torch.stack(query_imgs)      # shape: [n_way*k_query, C, H, W]\n",
    "        query_labels = torch.tensor(query_labels, dtype=torch.long)\n",
    "        \n",
    "        return (support_imgs, support_labels), (query_imgs, query_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58adc3d5",
   "metadata": {},
   "source": [
    "### Constructing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b699e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms (should match what the encoder expects)\n",
    "transform = T.Compose([\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Root folder with classes as subfolders.\n",
    "few_shot_dataset = FewShotFERDataset(root_dir=\"path/to/FER_few_shot\", n_way=5, k_shot=1, k_query=5, transform=transform)\n",
    "few_shot_loader = DataLoader(few_shot_dataset, batch_size=1, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3e480",
   "metadata": {},
   "source": [
    "### Prototypical Network Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fdf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(model, support_imgs, support_labels, query_imgs, query_labels, device):\n",
    "    \"\"\"\n",
    "    model: the fine-tuned FER model, used as feature extractor.\n",
    "    support_imgs: [n_way*k_shot, C, H, W]\n",
    "    query_imgs: [n_way*k_query, C, H, W]\n",
    "    support_labels: [n_way*k_shot]\n",
    "    query_labels: [n_way*k_query]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        support_imgs = support_imgs.to(device)\n",
    "        query_imgs = query_imgs.to(device)\n",
    "        \n",
    "        # Extract features using the pretrained encoder.\n",
    "        # We assume the encoder returns a [B, 1 + num_patches, D] tensor and we use the CLS token.\n",
    "        def get_cls_features(x):\n",
    "            tokens = model.encoder(x)  # shape: [B, 1 + num_patches, D]\n",
    "            return tokens[:, 0, :]     # extract CLS token\n",
    "        \n",
    "        support_feats = get_cls_features(support_imgs)  # shape: [n_way*k_shot, D]\n",
    "        query_feats = get_cls_features(query_imgs)      # shape: [n_way*k_query, D]\n",
    "        \n",
    "        # Compute prototypes: mean feature for each class.\n",
    "        n_way = len(torch.unique(support_labels))\n",
    "        prototypes = []\n",
    "        for cls in range(n_way):\n",
    "            cls_indices = (support_labels == cls).nonzero(as_tuple=True)[0]\n",
    "            cls_feats = support_feats[cls_indices]\n",
    "            prototype = cls_feats.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "        prototypes = torch.stack(prototypes)  # shape: [n_way, D]\n",
    "        \n",
    "        # Compute distances between each query feature and prototypes.\n",
    "        # We'll use Euclidean distance.\n",
    "        # Expand dimensions: query_feats: [Q, 1, D] and prototypes: [1, n_way, D]\n",
    "        dists = torch.cdist(query_feats, prototypes, p=2)  # shape: [n_way*k_query, n_way]\n",
    "        \n",
    "        # Convert distances to probabilities with a softmax on negative distances.\n",
    "        probs = F.softmax(-dists, dim=1)  # lower distance = higher probability\n",
    "        \n",
    "        # Predictions: class with the highest probability.\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        correct = (preds.cpu() == query_labels).sum().item()\n",
    "        total = query_labels.size(0)\n",
    "    \n",
    "    return correct, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71a500",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Evaluate on a few episodes from the DataLoader\n",
    "# ------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume model_fer is the fine-tuned FER model we defined previously.\n",
    "model_fer = model_fer.to(device)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "num_episodes = 50  # Evaluate on 50 episodes for instance.\n",
    "\n",
    "for i, ((support_imgs, support_labels), (query_imgs, query_labels)) in enumerate(few_shot_loader):\n",
    "    if i >= num_episodes:\n",
    "        break\n",
    "    correct, total = evaluate_episode(model_fer, support_imgs.squeeze(0), support_labels.squeeze(0),\n",
    "                                      query_imgs.squeeze(0), query_labels.squeeze(0), device)\n",
    "    total_correct += correct\n",
    "    total_samples += total\n",
    "\n",
    "episode_accuracy = 100.0 * total_correct / total_samples\n",
    "print(\"Few-Shot Episode Accuracy: {:.2f}%\".format(episode_accuracy))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
